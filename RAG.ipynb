{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1812059d6a8c4d2e96f7a442b3ac37b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1631141e4c940de95e8aeec2a92338e",
              "IPY_MODEL_0863183d690f4d7db6e1f9e084914e41",
              "IPY_MODEL_fa0036e03db549458e129d09f9a9e7b7"
            ],
            "layout": "IPY_MODEL_04a5be3210a44b2fa3ff4ca645d68a85"
          }
        },
        "e1631141e4c940de95e8aeec2a92338e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09d7a600b4174834b88aac05ca133092",
            "placeholder": "​",
            "style": "IPY_MODEL_5140400a1423465fbc712e05af7d2c9f",
            "value": "config.json: 100%"
          }
        },
        "0863183d690f4d7db6e1f9e084914e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88bd31896e4f44c8894a4d010c972cb0",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f55649bdb5cb496ca12cfbd7db4eb538",
            "value": 614
          }
        },
        "fa0036e03db549458e129d09f9a9e7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a687fd92fb1474f9c0244b342d7eb6a",
            "placeholder": "​",
            "style": "IPY_MODEL_ea2603c6b56b45de8b50326428049c33",
            "value": " 614/614 [00:00&lt;00:00, 22.1kB/s]"
          }
        },
        "04a5be3210a44b2fa3ff4ca645d68a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d7a600b4174834b88aac05ca133092": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5140400a1423465fbc712e05af7d2c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88bd31896e4f44c8894a4d010c972cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f55649bdb5cb496ca12cfbd7db4eb538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a687fd92fb1474f9c0244b342d7eb6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2603c6b56b45de8b50326428049c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9c417a6a4aa4c219d28eca72355d7d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a96a329811e4b26abf171a95fe455a3",
              "IPY_MODEL_5d1c842d91504a57939124346cebe7a5",
              "IPY_MODEL_442385095cb54f3e9b20f6dd1151585d"
            ],
            "layout": "IPY_MODEL_11750f5f543f4138867ed9acf45d3964"
          }
        },
        "8a96a329811e4b26abf171a95fe455a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acde734e3fed43d4a2bf8042d2193731",
            "placeholder": "​",
            "style": "IPY_MODEL_0e0580f93ff343a9b867ea0210c55ac6",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "5d1c842d91504a57939124346cebe7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58a8d3d02ff0494393169159ee4b153f",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acf2e5ae1e7f44e2bbe66755598ad747",
            "value": 26788
          }
        },
        "442385095cb54f3e9b20f6dd1151585d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc5e5f2922e24116b377366e9e02bab9",
            "placeholder": "​",
            "style": "IPY_MODEL_02ab807eb70b4505a8229ed175d03d9e",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 1.80MB/s]"
          }
        },
        "11750f5f543f4138867ed9acf45d3964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acde734e3fed43d4a2bf8042d2193731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e0580f93ff343a9b867ea0210c55ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58a8d3d02ff0494393169159ee4b153f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acf2e5ae1e7f44e2bbe66755598ad747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc5e5f2922e24116b377366e9e02bab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ab807eb70b4505a8229ed175d03d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27567b319a0e4be3aba5e3cba09f27de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4287d96aa1194f99b5b9c85a72c4367b",
              "IPY_MODEL_b6b2cb8c4a084dcebbe7a2b13056895b",
              "IPY_MODEL_c612dfc43029469285c949d62e827b0d"
            ],
            "layout": "IPY_MODEL_5eaa22c0856f4cf8a45aecd870dabe18"
          }
        },
        "4287d96aa1194f99b5b9c85a72c4367b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec49054f4404d09bb65b97c33284977",
            "placeholder": "​",
            "style": "IPY_MODEL_fcb6b8f67cae4e74923a0f64d5adb8be",
            "value": "Downloading shards: 100%"
          }
        },
        "b6b2cb8c4a084dcebbe7a2b13056895b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c115f8d9297845998e56612425285300",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4ca365949d24f75a4477c23a1d2c657",
            "value": 2
          }
        },
        "c612dfc43029469285c949d62e827b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1032b8c0464f41458a7a3b8be67fe7f7",
            "placeholder": "​",
            "style": "IPY_MODEL_3a448ce7326e49e8be635398d1d41786",
            "value": " 2/2 [02:10&lt;00:00, 60.14s/it]"
          }
        },
        "5eaa22c0856f4cf8a45aecd870dabe18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec49054f4404d09bb65b97c33284977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcb6b8f67cae4e74923a0f64d5adb8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c115f8d9297845998e56612425285300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ca365949d24f75a4477c23a1d2c657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1032b8c0464f41458a7a3b8be67fe7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a448ce7326e49e8be635398d1d41786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e73d2437ee2744d3890afb3a5e0b9c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31fb0be88b2d4b7fbc9411c0605b9061",
              "IPY_MODEL_8a4bce78a9404b3d8eec6802e63c18bd",
              "IPY_MODEL_e01502d6f4b041f788cf266a435889b5"
            ],
            "layout": "IPY_MODEL_7b351308dd5d4a7aaac1436c0bfe8410"
          }
        },
        "31fb0be88b2d4b7fbc9411c0605b9061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dec32d4e488431b9644fc6bbfe2b636",
            "placeholder": "​",
            "style": "IPY_MODEL_7ac8de79941e417bb05e8381057fe95c",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "8a4bce78a9404b3d8eec6802e63c18bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e2bef0025474f51be1ded54132cca28",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_157b0390d176467aa429d6b35fcad173",
            "value": 9976576152
          }
        },
        "e01502d6f4b041f788cf266a435889b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbd42838a78840d8a8541bf4bf0a93e3",
            "placeholder": "​",
            "style": "IPY_MODEL_de6d51973a9d4f33a8e265b58de69f2b",
            "value": " 9.98G/9.98G [01:32&lt;00:00, 119MB/s]"
          }
        },
        "7b351308dd5d4a7aaac1436c0bfe8410": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dec32d4e488431b9644fc6bbfe2b636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac8de79941e417bb05e8381057fe95c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e2bef0025474f51be1ded54132cca28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "157b0390d176467aa429d6b35fcad173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbd42838a78840d8a8541bf4bf0a93e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6d51973a9d4f33a8e265b58de69f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba1601a614ab4f4bbb03a0d4722d0339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13d62979963b498888109adff7d4c396",
              "IPY_MODEL_f571dc383bb345c59904dd4b77783ea3",
              "IPY_MODEL_23b29036c6944bcbaafbe86752413ba0"
            ],
            "layout": "IPY_MODEL_2506d773e71344da902d67031b258ebb"
          }
        },
        "13d62979963b498888109adff7d4c396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47abc2942a3047b3a39d98a1c570285f",
            "placeholder": "​",
            "style": "IPY_MODEL_8cc067c6c912401aab719a8eff66941d",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "f571dc383bb345c59904dd4b77783ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d395be5a66c474eb5e0423ebd40d964",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fce530a08dbb494fb90c570e3fc68de3",
            "value": 3500296424
          }
        },
        "23b29036c6944bcbaafbe86752413ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c08cfb5ce3a5493b8ff31308a30b60db",
            "placeholder": "​",
            "style": "IPY_MODEL_848e7eac576f4ede86a6ceb2cfdc4901",
            "value": " 3.50G/3.50G [00:36&lt;00:00, 168MB/s]"
          }
        },
        "2506d773e71344da902d67031b258ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47abc2942a3047b3a39d98a1c570285f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cc067c6c912401aab719a8eff66941d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d395be5a66c474eb5e0423ebd40d964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fce530a08dbb494fb90c570e3fc68de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c08cfb5ce3a5493b8ff31308a30b60db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848e7eac576f4ede86a6ceb2cfdc4901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd72d39b3a0d42098a6506b03d124457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_629e7ef76d024491804b52e9d6a5ff3f",
              "IPY_MODEL_2e5026180412470bb7b1567115d4f2df",
              "IPY_MODEL_060001a792aa4caf992c275f33f3d1d3"
            ],
            "layout": "IPY_MODEL_40a6f6861d58440cb160f7cdb0dd7e75"
          }
        },
        "629e7ef76d024491804b52e9d6a5ff3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d625e33aee1a4fea87af66f0bc2f9f52",
            "placeholder": "​",
            "style": "IPY_MODEL_f7658e589b614709b2f3069fdd70e052",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2e5026180412470bb7b1567115d4f2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b803e9b65f10485790ab1f29899cc6b8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7279a5ee08b4066adfa6b8a09895c5c",
            "value": 2
          }
        },
        "060001a792aa4caf992c275f33f3d1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62bc6d58670f45388fc31d5ad3b9ac37",
            "placeholder": "​",
            "style": "IPY_MODEL_36c2eaab19bd4f4b91141f90fb8d9f96",
            "value": " 2/2 [00:56&lt;00:00, 25.75s/it]"
          }
        },
        "40a6f6861d58440cb160f7cdb0dd7e75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d625e33aee1a4fea87af66f0bc2f9f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7658e589b614709b2f3069fdd70e052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b803e9b65f10485790ab1f29899cc6b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7279a5ee08b4066adfa6b8a09895c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62bc6d58670f45388fc31d5ad3b9ac37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c2eaab19bd4f4b91141f90fb8d9f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03a325b0b08d47a0b0bcc2c9d2306f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_154e02ed7702410aa06b8dd29095b22b",
              "IPY_MODEL_1d2faa03bd6b42218f436ebbb2ad573c",
              "IPY_MODEL_aea8c793c2fc4b06aecc95ffd8192770"
            ],
            "layout": "IPY_MODEL_47d1b1f816da48be853254c321a70748"
          }
        },
        "154e02ed7702410aa06b8dd29095b22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d23fdc7e0aa54e678abf6cbed8a48d9a",
            "placeholder": "​",
            "style": "IPY_MODEL_4eb32378ac8f46b5b9c65110a463dd2c",
            "value": "generation_config.json: 100%"
          }
        },
        "1d2faa03bd6b42218f436ebbb2ad573c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_262e2f65c07648d28f1c24ef8e10cf4e",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_279dd0ccbc5a4f6cb1d7cdfea0d22c8c",
            "value": 188
          }
        },
        "aea8c793c2fc4b06aecc95ffd8192770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3542af1c7a604ce1b81dc2d04cb5d77f",
            "placeholder": "​",
            "style": "IPY_MODEL_930c2a9061d245f29b408807f6d19807",
            "value": " 188/188 [00:00&lt;00:00, 13.4kB/s]"
          }
        },
        "47d1b1f816da48be853254c321a70748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d23fdc7e0aa54e678abf6cbed8a48d9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eb32378ac8f46b5b9c65110a463dd2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "262e2f65c07648d28f1c24ef8e10cf4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "279dd0ccbc5a4f6cb1d7cdfea0d22c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3542af1c7a604ce1b81dc2d04cb5d77f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930c2a9061d245f29b408807f6d19807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0e941d161954201a6acdf3da8af650b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75c7643477154ac096cf2998de43b96b",
              "IPY_MODEL_47b1afc6fd934ef69893c044740083ca",
              "IPY_MODEL_70b87c12bb824fef87e13fa0c7b9ba8a"
            ],
            "layout": "IPY_MODEL_fa41e1745a1b4747986b91f4bf5c42e3"
          }
        },
        "75c7643477154ac096cf2998de43b96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5f57929183f4d74a1490ddde6f60f43",
            "placeholder": "​",
            "style": "IPY_MODEL_66e205b5baf346e0b34f2426be16a715",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "47b1afc6fd934ef69893c044740083ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3661a09281bd42afa9537b496577d53f",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_755b02eea42d4d32954b4c47173e5756",
            "value": 1618
          }
        },
        "70b87c12bb824fef87e13fa0c7b9ba8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f3b44e805a44c2db87e30014dc60ac4",
            "placeholder": "​",
            "style": "IPY_MODEL_f8abe8ac460a4e63a514de30024de9c2",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 82.5kB/s]"
          }
        },
        "fa41e1745a1b4747986b91f4bf5c42e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f57929183f4d74a1490ddde6f60f43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66e205b5baf346e0b34f2426be16a715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3661a09281bd42afa9537b496577d53f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "755b02eea42d4d32954b4c47173e5756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f3b44e805a44c2db87e30014dc60ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8abe8ac460a4e63a514de30024de9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a74b77758984c5f946dc548cbbc0e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f177f76a27e34243821fdede0e94fa6a",
              "IPY_MODEL_4df3b08219834eac88444136602e31fe",
              "IPY_MODEL_54c14bed2bf640e69078c385e96f7399"
            ],
            "layout": "IPY_MODEL_57656b324892419d84414fca9a65290d"
          }
        },
        "f177f76a27e34243821fdede0e94fa6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4642f27ad47e430b85b28065a558b218",
            "placeholder": "​",
            "style": "IPY_MODEL_0a2504f8142c49d0a89482d4d277f22a",
            "value": "tokenizer.model: 100%"
          }
        },
        "4df3b08219834eac88444136602e31fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c78b47241be0454c9782a83f03f11e96",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b5109c3e28741caa5014c10d7867e29",
            "value": 499723
          }
        },
        "54c14bed2bf640e69078c385e96f7399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79b6008e19f049aea0651da85353ade9",
            "placeholder": "​",
            "style": "IPY_MODEL_31ff17ab504741938ef20b1841b28541",
            "value": " 500k/500k [00:00&lt;00:00, 18.1MB/s]"
          }
        },
        "57656b324892419d84414fca9a65290d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4642f27ad47e430b85b28065a558b218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a2504f8142c49d0a89482d4d277f22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c78b47241be0454c9782a83f03f11e96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b5109c3e28741caa5014c10d7867e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79b6008e19f049aea0651da85353ade9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31ff17ab504741938ef20b1841b28541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d0d602ad8184a8aa6d7ca5069962002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5411c98f054d435bb4cbb5c15b6c6fdc",
              "IPY_MODEL_9f45f079d9f842ba9831ffc5f76aa59c",
              "IPY_MODEL_a8b8e119abc24143a516ada2159dbd5d"
            ],
            "layout": "IPY_MODEL_4caf2f7b5f8e4ce88bef225810528ee5"
          }
        },
        "5411c98f054d435bb4cbb5c15b6c6fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a81566090e3f436f84b6d7d0f940ee03",
            "placeholder": "​",
            "style": "IPY_MODEL_5aa3dc87beb046fd960f4ce71d8021a9",
            "value": "tokenizer.json: 100%"
          }
        },
        "9f45f079d9f842ba9831ffc5f76aa59c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02ad3d1204534ffb9f0e209020b38661",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b4561866f524e82b31eb6a61afa2a63",
            "value": 1842767
          }
        },
        "a8b8e119abc24143a516ada2159dbd5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6452041bd4c4edeba16ea36aba92de0",
            "placeholder": "​",
            "style": "IPY_MODEL_30f76833ae474e179eb33a106e7f545f",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 5.01MB/s]"
          }
        },
        "4caf2f7b5f8e4ce88bef225810528ee5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81566090e3f436f84b6d7d0f940ee03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aa3dc87beb046fd960f4ce71d8021a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02ad3d1204534ffb9f0e209020b38661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b4561866f524e82b31eb6a61afa2a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6452041bd4c4edeba16ea36aba92de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30f76833ae474e179eb33a106e7f545f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35336582df9e4adb9183f02e14a4937f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bd0935d6aa5436bb262f1eabe8bfbc8",
              "IPY_MODEL_5985ed3476684abda9cc4ccde1c088d2",
              "IPY_MODEL_9e6760cd62e74d4aac00f034605b1021"
            ],
            "layout": "IPY_MODEL_e9afa380314845bd9cec850122f596b9"
          }
        },
        "3bd0935d6aa5436bb262f1eabe8bfbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2290190323c24610a4504c44c9ba953c",
            "placeholder": "​",
            "style": "IPY_MODEL_a9cd52ce28074c2eb82c2e8e69cdb1af",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5985ed3476684abda9cc4ccde1c088d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a381b9d86a64fcdbbcefda0935718a0",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b17e7d5698748cc936fa3e567267028",
            "value": 414
          }
        },
        "9e6760cd62e74d4aac00f034605b1021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dbbe5bd6118484b85f1be2fa64226cf",
            "placeholder": "​",
            "style": "IPY_MODEL_cf679ff089f24d5b8d87a9ca3f7f3fea",
            "value": " 414/414 [00:00&lt;00:00, 22.5kB/s]"
          }
        },
        "e9afa380314845bd9cec850122f596b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2290190323c24610a4504c44c9ba953c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9cd52ce28074c2eb82c2e8e69cdb1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a381b9d86a64fcdbbcefda0935718a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b17e7d5698748cc936fa3e567267028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0dbbe5bd6118484b85f1be2fa64226cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf679ff089f24d5b8d87a9ca3f7f3fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3940e60d84604f81b0a3ff08f7fd27eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74b3b3ecc3ed4bfda6863e10af796e84",
              "IPY_MODEL_5387bb6e809c4dd197aa965bcd462d03",
              "IPY_MODEL_4ca2fcb48fd344e993d45864be2fe020"
            ],
            "layout": "IPY_MODEL_41f1a9a06a1940df83219fe194b356b0"
          }
        },
        "74b3b3ecc3ed4bfda6863e10af796e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70eb7afe93bc4fdda4ff366699a5ddd6",
            "placeholder": "​",
            "style": "IPY_MODEL_ede9cc898a794c2f92638c5ade954a9e",
            "value": ".gitattributes: 100%"
          }
        },
        "5387bb6e809c4dd197aa965bcd462d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea1d1288ed4e4490bf8034c788d763df",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c1e114173cb4d7091d53f57ca11212c",
            "value": 1175
          }
        },
        "4ca2fcb48fd344e993d45864be2fe020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39edd6c1c8e749a7825e95b281af744b",
            "placeholder": "​",
            "style": "IPY_MODEL_4ef9cd961cff4723baae26a2917883ed",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 78.8kB/s]"
          }
        },
        "41f1a9a06a1940df83219fe194b356b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70eb7afe93bc4fdda4ff366699a5ddd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede9cc898a794c2f92638c5ade954a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea1d1288ed4e4490bf8034c788d763df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c1e114173cb4d7091d53f57ca11212c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39edd6c1c8e749a7825e95b281af744b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ef9cd961cff4723baae26a2917883ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc97e281032c48de8bb820cee493d58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de5bd1d68188407299271352d49286ea",
              "IPY_MODEL_ab8205b0cb444eb584bfca29c34a1295",
              "IPY_MODEL_3710b33d408e4d8e8649617eadffe78e"
            ],
            "layout": "IPY_MODEL_e6a48933067b43f99f22d92645028383"
          }
        },
        "de5bd1d68188407299271352d49286ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44750abc65f14e8e87319c4f32f1f937",
            "placeholder": "​",
            "style": "IPY_MODEL_29fadb181bc54b798143ed4203025d4f",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "ab8205b0cb444eb584bfca29c34a1295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e42dd3936d3c478fbb08c17c66f447ac",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3023f1860ffd49ebbc4577367ffa28c4",
            "value": 190
          }
        },
        "3710b33d408e4d8e8649617eadffe78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fd408c74740452297e0a47bff2e5c8c",
            "placeholder": "​",
            "style": "IPY_MODEL_c7d68f997cc943cb80f8eb299f71488e",
            "value": " 190/190 [00:00&lt;00:00, 14.4kB/s]"
          }
        },
        "e6a48933067b43f99f22d92645028383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44750abc65f14e8e87319c4f32f1f937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29fadb181bc54b798143ed4203025d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e42dd3936d3c478fbb08c17c66f447ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3023f1860ffd49ebbc4577367ffa28c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fd408c74740452297e0a47bff2e5c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7d68f997cc943cb80f8eb299f71488e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b988304270d248348a6538499ef672fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c56e3c5129d4afabff11fb65a6da9c2",
              "IPY_MODEL_88ac9a71e8bc4fcd9b63ec6c163fb943",
              "IPY_MODEL_10be0c0cc9094f7490a97a5258e698e7"
            ],
            "layout": "IPY_MODEL_0ecc522377634d71bad238a7194dfbba"
          }
        },
        "1c56e3c5129d4afabff11fb65a6da9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_471ed1877060453281c78720730581b2",
            "placeholder": "​",
            "style": "IPY_MODEL_835b2891f21e4db0b4593c367182c798",
            "value": "README.md: 100%"
          }
        },
        "88ac9a71e8bc4fcd9b63ec6c163fb943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8f6a6a104f044d6831af5aa662ac8b8",
            "max": 10571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_929dd3f4fa534170b42f7a193168a81e",
            "value": 10571
          }
        },
        "10be0c0cc9094f7490a97a5258e698e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a1ed4af3fad4574885fd86bd2e2afba",
            "placeholder": "​",
            "style": "IPY_MODEL_707565c3baf644c7a8204a1e1ecf3022",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 806kB/s]"
          }
        },
        "0ecc522377634d71bad238a7194dfbba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471ed1877060453281c78720730581b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835b2891f21e4db0b4593c367182c798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f6a6a104f044d6831af5aa662ac8b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "929dd3f4fa534170b42f7a193168a81e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a1ed4af3fad4574885fd86bd2e2afba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707565c3baf644c7a8204a1e1ecf3022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44968b681d804ae69ada7ed0bf7dc08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ac19c54b521497aac2ea67798f5ae0f",
              "IPY_MODEL_90ee7182fbb347dda8cf73a3078944a4",
              "IPY_MODEL_7cb83642c37a4c36a31ee149d14ded08"
            ],
            "layout": "IPY_MODEL_2f685916ddf04a56b9d8a65a7fc50267"
          }
        },
        "1ac19c54b521497aac2ea67798f5ae0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c656a269ecc40bda945fd96b9c117c9",
            "placeholder": "​",
            "style": "IPY_MODEL_f2c7df60dfa54034939e272184bf909d",
            "value": "config.json: 100%"
          }
        },
        "90ee7182fbb347dda8cf73a3078944a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02f2956fb0e146218439db5f8633278d",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06f0e0b411964ff1a3d2d32a0b5841b5",
            "value": 571
          }
        },
        "7cb83642c37a4c36a31ee149d14ded08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b93c08ffeb24cc693d6c9f35e64ba79",
            "placeholder": "​",
            "style": "IPY_MODEL_f29f31d77a0347cc9d06f390e8a3b5ec",
            "value": " 571/571 [00:00&lt;00:00, 25.3kB/s]"
          }
        },
        "2f685916ddf04a56b9d8a65a7fc50267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c656a269ecc40bda945fd96b9c117c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c7df60dfa54034939e272184bf909d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02f2956fb0e146218439db5f8633278d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06f0e0b411964ff1a3d2d32a0b5841b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b93c08ffeb24cc693d6c9f35e64ba79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29f31d77a0347cc9d06f390e8a3b5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0f9adfac683408097a9a029d8c51463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9724bfc63f29423eb3df74dcbbc4ae95",
              "IPY_MODEL_c4c82b6527264cc29b2b705d3419eabe",
              "IPY_MODEL_cc15b55b5c5c47e69a5ab4a439f2633a"
            ],
            "layout": "IPY_MODEL_6414f1acf09a4cbd80cfb84c4a457f22"
          }
        },
        "9724bfc63f29423eb3df74dcbbc4ae95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23588760bc8b4aaa84d66c86c1fb2884",
            "placeholder": "​",
            "style": "IPY_MODEL_654ab5b7a378460d9d65256b0b102138",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "c4c82b6527264cc29b2b705d3419eabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98046f6755ab42ba80aa38e042ae540f",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bf0aca459e743eeb5ec61c46802fc5b",
            "value": 116
          }
        },
        "cc15b55b5c5c47e69a5ab4a439f2633a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb15bea4696e4ceba186d901cb5582e7",
            "placeholder": "​",
            "style": "IPY_MODEL_4ec96a8f14dc4e3bb97772cf5a9cdc28",
            "value": " 116/116 [00:00&lt;00:00, 4.79kB/s]"
          }
        },
        "6414f1acf09a4cbd80cfb84c4a457f22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23588760bc8b4aaa84d66c86c1fb2884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "654ab5b7a378460d9d65256b0b102138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98046f6755ab42ba80aa38e042ae540f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bf0aca459e743eeb5ec61c46802fc5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb15bea4696e4ceba186d901cb5582e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec96a8f14dc4e3bb97772cf5a9cdc28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e19b707f4ddd44e4b2199ada2e4b059f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_534d2b5878bf4d9ab3d697d537565261",
              "IPY_MODEL_f94b318cea3e4103b538131f40b4ca57",
              "IPY_MODEL_970f23e1cae5423e8aec0ea40750fd7b"
            ],
            "layout": "IPY_MODEL_bb0ae8cc6b314178aa715f564d44c435"
          }
        },
        "534d2b5878bf4d9ab3d697d537565261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f56561007e3b42e38f847046c187f0ed",
            "placeholder": "​",
            "style": "IPY_MODEL_ac1901be8e674f84a037bc1afa6336af",
            "value": "data_config.json: 100%"
          }
        },
        "f94b318cea3e4103b538131f40b4ca57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68573a32b70147dd998965664fb3d158",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9da9846627f4a39ab4f9bd94e8c4ade",
            "value": 39265
          }
        },
        "970f23e1cae5423e8aec0ea40750fd7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee3e2b867dc449b196a41bf16209afbc",
            "placeholder": "​",
            "style": "IPY_MODEL_526facf324bf4c0989c76cb92b974d85",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 1.70MB/s]"
          }
        },
        "bb0ae8cc6b314178aa715f564d44c435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f56561007e3b42e38f847046c187f0ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac1901be8e674f84a037bc1afa6336af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68573a32b70147dd998965664fb3d158": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9da9846627f4a39ab4f9bd94e8c4ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee3e2b867dc449b196a41bf16209afbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "526facf324bf4c0989c76cb92b974d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d08eaffb93ff497facb64a48062cdf84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e21f63df376479793a8d13de228a090",
              "IPY_MODEL_8a16649f7a5b4fbe9203910bcc6f22ff",
              "IPY_MODEL_7341203ecd0f45a499d6b5dbe46ecdcf"
            ],
            "layout": "IPY_MODEL_2e189a3f56a44ee4b262f37249797037"
          }
        },
        "3e21f63df376479793a8d13de228a090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac3d2a9a41364907afc5afee63afb785",
            "placeholder": "​",
            "style": "IPY_MODEL_db094e7c86984c918c5a9782be1b0683",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8a16649f7a5b4fbe9203910bcc6f22ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4688fe76e164cecb1f1373e2da5f384",
            "max": 438011953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_743c34fb18df4958b1fced6611e6e96a",
            "value": 438011953
          }
        },
        "7341203ecd0f45a499d6b5dbe46ecdcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d17f3b6544040809287593ff225e125",
            "placeholder": "​",
            "style": "IPY_MODEL_4ea57d97a14543baa05812769ccadd92",
            "value": " 438M/438M [00:05&lt;00:00, 102MB/s]"
          }
        },
        "2e189a3f56a44ee4b262f37249797037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac3d2a9a41364907afc5afee63afb785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db094e7c86984c918c5a9782be1b0683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4688fe76e164cecb1f1373e2da5f384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "743c34fb18df4958b1fced6611e6e96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d17f3b6544040809287593ff225e125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ea57d97a14543baa05812769ccadd92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e867b48c5e2453881c77f32cf99f159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aab574933c9c4970b577e3db1a3a0dcf",
              "IPY_MODEL_e27796658c14456b9233caa853064098",
              "IPY_MODEL_af59357edf75402abb87e6c89875ddff"
            ],
            "layout": "IPY_MODEL_ffce8211101643c4b22f2a71d6c4b84d"
          }
        },
        "aab574933c9c4970b577e3db1a3a0dcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_367e5cc0df704ec8b9ef4789dff0902f",
            "placeholder": "​",
            "style": "IPY_MODEL_74c5262c07c440bd92b34f59164821f9",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "e27796658c14456b9233caa853064098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62e772d30b7546b4b786d4e26750083b",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4111e7a5907b412ab47a0848ebb99019",
            "value": 53
          }
        },
        "af59357edf75402abb87e6c89875ddff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30e5170f98224b2a8e2e402828f516d3",
            "placeholder": "​",
            "style": "IPY_MODEL_6b8a3fc23d61451a8cc7ca8449a56c46",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.84kB/s]"
          }
        },
        "ffce8211101643c4b22f2a71d6c4b84d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "367e5cc0df704ec8b9ef4789dff0902f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74c5262c07c440bd92b34f59164821f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62e772d30b7546b4b786d4e26750083b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4111e7a5907b412ab47a0848ebb99019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30e5170f98224b2a8e2e402828f516d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b8a3fc23d61451a8cc7ca8449a56c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "277a0d89d8c444b182a8d66c8503d91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_005c61a2029442dbad2c4d876f304f23",
              "IPY_MODEL_513fd1f899d448c5a7b277486d0bee4c",
              "IPY_MODEL_271e69a082ff4d93b051644211e7c22b"
            ],
            "layout": "IPY_MODEL_eac7a3b210654134a9500836c1ce16fa"
          }
        },
        "005c61a2029442dbad2c4d876f304f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_980d76c95a914c0c89699e8c73c51174",
            "placeholder": "​",
            "style": "IPY_MODEL_566810cc036d4cb9986cafea56e238e2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "513fd1f899d448c5a7b277486d0bee4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bb0b690ae194809933c6a93b347fe37",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9a2c49f97df4289bb52657d48c0f867",
            "value": 239
          }
        },
        "271e69a082ff4d93b051644211e7c22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dac984c9ba3a4deab3d4b85f7f072996",
            "placeholder": "​",
            "style": "IPY_MODEL_3e7db0554e0f4cf6817a10fa9bd87401",
            "value": " 239/239 [00:00&lt;00:00, 15.0kB/s]"
          }
        },
        "eac7a3b210654134a9500836c1ce16fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "980d76c95a914c0c89699e8c73c51174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "566810cc036d4cb9986cafea56e238e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bb0b690ae194809933c6a93b347fe37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a2c49f97df4289bb52657d48c0f867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dac984c9ba3a4deab3d4b85f7f072996": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e7db0554e0f4cf6817a10fa9bd87401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b18a4ae52485472682abea96a9d52ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37fbf7377a004e0a90c644107c13eecf",
              "IPY_MODEL_e42e04c3ca354553b7976ae3395e8861",
              "IPY_MODEL_6e998624454e4d578f878e1883555e59"
            ],
            "layout": "IPY_MODEL_8f71285c8b0d49e6937185e75983108f"
          }
        },
        "37fbf7377a004e0a90c644107c13eecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d427bb63e49c460ab4fc7c2aef1d511b",
            "placeholder": "​",
            "style": "IPY_MODEL_c5e70b74da634c9297a73488e3d8e177",
            "value": "tokenizer.json: 100%"
          }
        },
        "e42e04c3ca354553b7976ae3395e8861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be3bdab71eaa4d53a930d483e6f2ff3e",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5284432f4a8d49c783365f76e0fdc645",
            "value": 466021
          }
        },
        "6e998624454e4d578f878e1883555e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bc2663932524576a908949f7666a071",
            "placeholder": "​",
            "style": "IPY_MODEL_f1530d38d2cb428098a9637ccb4d3270",
            "value": " 466k/466k [00:00&lt;00:00, 26.1MB/s]"
          }
        },
        "8f71285c8b0d49e6937185e75983108f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d427bb63e49c460ab4fc7c2aef1d511b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e70b74da634c9297a73488e3d8e177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be3bdab71eaa4d53a930d483e6f2ff3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5284432f4a8d49c783365f76e0fdc645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bc2663932524576a908949f7666a071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1530d38d2cb428098a9637ccb4d3270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae8a0a38919a44c4b5afb5d6c0389d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70127e26514d4b299254d8108e46e222",
              "IPY_MODEL_bfea34e7f2ea4b56bc84164ffe76e59f",
              "IPY_MODEL_51d0c7898f3e4faca78fd5836537e5e3"
            ],
            "layout": "IPY_MODEL_79c757d47e854639a0b2098d0522ea0b"
          }
        },
        "70127e26514d4b299254d8108e46e222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67531bf406474d818273595a77d7e2cb",
            "placeholder": "​",
            "style": "IPY_MODEL_996006c0102d450188fa01d3f618d6c3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "bfea34e7f2ea4b56bc84164ffe76e59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c789cca8eff1443a82b5b1d74aeff84f",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08518cc3729049f9acbe7ba0d47022ec",
            "value": 363
          }
        },
        "51d0c7898f3e4faca78fd5836537e5e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b2c87709d34a8eb936a5b517bfc5b0",
            "placeholder": "​",
            "style": "IPY_MODEL_a99873e16f9a4f45b1c35015306c9cab",
            "value": " 363/363 [00:00&lt;00:00, 27.1kB/s]"
          }
        },
        "79c757d47e854639a0b2098d0522ea0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67531bf406474d818273595a77d7e2cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996006c0102d450188fa01d3f618d6c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c789cca8eff1443a82b5b1d74aeff84f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08518cc3729049f9acbe7ba0d47022ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77b2c87709d34a8eb936a5b517bfc5b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a99873e16f9a4f45b1c35015306c9cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56821ea9bd144e17ae8147d09a01b46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c07959e48d92419faa36153bb83e29d3",
              "IPY_MODEL_23bda95754a94295875309dbc956c77b",
              "IPY_MODEL_6f561a764f504fef9d05d1074878eb07"
            ],
            "layout": "IPY_MODEL_b514bffe5a7e4b2e85318508533e5477"
          }
        },
        "c07959e48d92419faa36153bb83e29d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e11ce430d52b4833b8f811bc373fe110",
            "placeholder": "​",
            "style": "IPY_MODEL_76b7f652f03f49c48ec36095d2e774ec",
            "value": "train_script.py: 100%"
          }
        },
        "23bda95754a94295875309dbc956c77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55d41486719a48bfa87512efb1b50524",
            "max": 13123,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9a730cd29874d548e130f59bfeecc98",
            "value": 13123
          }
        },
        "6f561a764f504fef9d05d1074878eb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2de9435f9cf249faa8a535204fd1e9fd",
            "placeholder": "​",
            "style": "IPY_MODEL_1639331afe29477282cf3bbac80119fc",
            "value": " 13.1k/13.1k [00:00&lt;00:00, 843kB/s]"
          }
        },
        "b514bffe5a7e4b2e85318508533e5477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e11ce430d52b4833b8f811bc373fe110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76b7f652f03f49c48ec36095d2e774ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55d41486719a48bfa87512efb1b50524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a730cd29874d548e130f59bfeecc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2de9435f9cf249faa8a535204fd1e9fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1639331afe29477282cf3bbac80119fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a857565e57a445d7908d59f78fac3de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e5a7d05fee2448391dde6cf67073163",
              "IPY_MODEL_9798639f72f3430b9037fcdb544943ef",
              "IPY_MODEL_ad73898660f54b83b26f49e3cb6712e3"
            ],
            "layout": "IPY_MODEL_e16252b112824e72ba52705c8cdd5141"
          }
        },
        "5e5a7d05fee2448391dde6cf67073163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48f4eae296a4b5d8c799a12e6cda6bc",
            "placeholder": "​",
            "style": "IPY_MODEL_5a0cb9513cf6485baa73b7c3c1a22ff7",
            "value": "vocab.txt: 100%"
          }
        },
        "9798639f72f3430b9037fcdb544943ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c1e24bd56a14db0b26d01fbffcf471a",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8a2f8165b7849e4ab0be17edf1a83c9",
            "value": 231536
          }
        },
        "ad73898660f54b83b26f49e3cb6712e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f619ee59b548408d64233e3d2a0b30",
            "placeholder": "​",
            "style": "IPY_MODEL_a2382b7a6d2f4e4ca17e6c5ae44e00f2",
            "value": " 232k/232k [00:00&lt;00:00, 8.43MB/s]"
          }
        },
        "e16252b112824e72ba52705c8cdd5141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48f4eae296a4b5d8c799a12e6cda6bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a0cb9513cf6485baa73b7c3c1a22ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c1e24bd56a14db0b26d01fbffcf471a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8a2f8165b7849e4ab0be17edf1a83c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2f619ee59b548408d64233e3d2a0b30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2382b7a6d2f4e4ca17e6c5ae44e00f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "336a39cc159343a49f43d5ca1b0a5089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce512c4d6fe243a3977f35c7ec6cba14",
              "IPY_MODEL_791cf9a8db9c4522af564bf231247f84",
              "IPY_MODEL_bc81d1ca229845699d457e0a9bb1ade3"
            ],
            "layout": "IPY_MODEL_5619f57069cd445297482315e63ec74e"
          }
        },
        "ce512c4d6fe243a3977f35c7ec6cba14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67a419257cc84e7f90820f50e64fa32f",
            "placeholder": "​",
            "style": "IPY_MODEL_3e7df6fa548e4e388c610b15d7e59be5",
            "value": "modules.json: 100%"
          }
        },
        "791cf9a8db9c4522af564bf231247f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3549a69f5774bbe889dad5f690a6914",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_835b489759e848ce95a3c52826575ad3",
            "value": 349
          }
        },
        "bc81d1ca229845699d457e0a9bb1ade3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8edf842f8e0349eda4f32615424bf166",
            "placeholder": "​",
            "style": "IPY_MODEL_e43a536c9b8e425bbbd2f5c7d141aa6a",
            "value": " 349/349 [00:00&lt;00:00, 19.1kB/s]"
          }
        },
        "5619f57069cd445297482315e63ec74e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67a419257cc84e7f90820f50e64fa32f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e7df6fa548e4e388c610b15d7e59be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3549a69f5774bbe889dad5f690a6914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835b489759e848ce95a3c52826575ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8edf842f8e0349eda4f32615424bf166": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e43a536c9b8e425bbbd2f5c7d141aa6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LLM model: Llama 7B\n",
        "## Langchain framework (using Memory capabilities)\n",
        "## FAISS - similarity search/indexing/clustering - can be considered as vector db\n",
        "## Embeddings - sentence-transformers\n",
        "## Web scraping"
      ],
      "metadata": {
        "id": "ou9APqlqh1zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.21.0 transformers==4.31.0 tokenizers==0.13.3\n",
        "!pip install bitsandbytes==0.40.0 einops==0.6.1\n",
        "!pip install xformers==0.0.22.post7\n",
        "!pip install langchain==0.1.4\n",
        "!pip install faiss-gpu==1.7.1.post3\n",
        "!pip install sentence_transformers\n",
        "!pip install typing_extensions\n",
        "!pip install fastapi\n",
        "!pip install uvicorn\n",
        "\n",
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "print(locale.getpreferredencoding())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2QUouhGdpPT",
        "outputId": "3ee035c4-c9b5-47d7-fea8-f55994d99159"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.21.0\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.31.0\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.13.3\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.20.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Installing collected packages: tokenizers, transformers, accelerate\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.1\n",
            "    Uninstalling tokenizers-0.15.1:\n",
            "      Successfully uninstalled tokenizers-0.15.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed accelerate-0.21.0 tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting bitsandbytes==0.40.0\n",
            "  Downloading bitsandbytes-0.40.0-py3-none-any.whl (91.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.6.1\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes, einops\n",
            "Successfully installed bitsandbytes-0.40.0 einops-0.6.1\n",
            "Collecting xformers==0.0.22.post7\n",
            "  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.22.post7) (1.23.5)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from xformers==0.0.22.post7) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers==0.0.22.post7) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->xformers==0.0.22.post7) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->xformers==0.0.22.post7) (1.3.0)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.22.post7\n",
            "Collecting langchain==0.1.4\n",
            "  Downloading langchain-0.1.4-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.4)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.4)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.14 (from langchain==0.1.4)\n",
            "  Downloading langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.16 (from langchain==0.1.4)\n",
            "  Downloading langchain_core-0.1.16-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.3/230.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain==0.1.4)\n",
            "  Downloading langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.1.4) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.4) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.4)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.4)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.4) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.4) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.1.4) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.1.4) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.4) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.4) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.4) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.4)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.4 langchain-community-0.0.16 langchain-core-0.1.16 langsmith-0.0.83 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting faiss-gpu==1.7.1.post3\n",
            "  Downloading faiss_gpu-1.7.1.post3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (90.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.1.post3\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=ef36a24083bc59c426ab5f11d0c1f8202e8cb5eb6def04ad7041f236c7bc0714\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.14)\n",
            "Collecting starlette<0.36.0,>=0.35.0 (from fastapi)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.36.0,>=0.35.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi) (1.2.0)\n",
            "Installing collected packages: typing-extensions, starlette, fastapi\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastapi-0.109.0 starlette-0.35.1 typing-extensions-4.9.0\n",
            "UTF-8\n",
            "UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, you need an access token\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "# enable evaluation mode to allow model inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799,
          "referenced_widgets": [
            "1812059d6a8c4d2e96f7a442b3ac37b8",
            "e1631141e4c940de95e8aeec2a92338e",
            "0863183d690f4d7db6e1f9e084914e41",
            "fa0036e03db549458e129d09f9a9e7b7",
            "04a5be3210a44b2fa3ff4ca645d68a85",
            "09d7a600b4174834b88aac05ca133092",
            "5140400a1423465fbc712e05af7d2c9f",
            "88bd31896e4f44c8894a4d010c972cb0",
            "f55649bdb5cb496ca12cfbd7db4eb538",
            "8a687fd92fb1474f9c0244b342d7eb6a",
            "ea2603c6b56b45de8b50326428049c33",
            "f9c417a6a4aa4c219d28eca72355d7d7",
            "8a96a329811e4b26abf171a95fe455a3",
            "5d1c842d91504a57939124346cebe7a5",
            "442385095cb54f3e9b20f6dd1151585d",
            "11750f5f543f4138867ed9acf45d3964",
            "acde734e3fed43d4a2bf8042d2193731",
            "0e0580f93ff343a9b867ea0210c55ac6",
            "58a8d3d02ff0494393169159ee4b153f",
            "acf2e5ae1e7f44e2bbe66755598ad747",
            "dc5e5f2922e24116b377366e9e02bab9",
            "02ab807eb70b4505a8229ed175d03d9e",
            "27567b319a0e4be3aba5e3cba09f27de",
            "4287d96aa1194f99b5b9c85a72c4367b",
            "b6b2cb8c4a084dcebbe7a2b13056895b",
            "c612dfc43029469285c949d62e827b0d",
            "5eaa22c0856f4cf8a45aecd870dabe18",
            "fec49054f4404d09bb65b97c33284977",
            "fcb6b8f67cae4e74923a0f64d5adb8be",
            "c115f8d9297845998e56612425285300",
            "c4ca365949d24f75a4477c23a1d2c657",
            "1032b8c0464f41458a7a3b8be67fe7f7",
            "3a448ce7326e49e8be635398d1d41786",
            "e73d2437ee2744d3890afb3a5e0b9c53",
            "31fb0be88b2d4b7fbc9411c0605b9061",
            "8a4bce78a9404b3d8eec6802e63c18bd",
            "e01502d6f4b041f788cf266a435889b5",
            "7b351308dd5d4a7aaac1436c0bfe8410",
            "0dec32d4e488431b9644fc6bbfe2b636",
            "7ac8de79941e417bb05e8381057fe95c",
            "1e2bef0025474f51be1ded54132cca28",
            "157b0390d176467aa429d6b35fcad173",
            "cbd42838a78840d8a8541bf4bf0a93e3",
            "de6d51973a9d4f33a8e265b58de69f2b",
            "ba1601a614ab4f4bbb03a0d4722d0339",
            "13d62979963b498888109adff7d4c396",
            "f571dc383bb345c59904dd4b77783ea3",
            "23b29036c6944bcbaafbe86752413ba0",
            "2506d773e71344da902d67031b258ebb",
            "47abc2942a3047b3a39d98a1c570285f",
            "8cc067c6c912401aab719a8eff66941d",
            "8d395be5a66c474eb5e0423ebd40d964",
            "fce530a08dbb494fb90c570e3fc68de3",
            "c08cfb5ce3a5493b8ff31308a30b60db",
            "848e7eac576f4ede86a6ceb2cfdc4901",
            "dd72d39b3a0d42098a6506b03d124457",
            "629e7ef76d024491804b52e9d6a5ff3f",
            "2e5026180412470bb7b1567115d4f2df",
            "060001a792aa4caf992c275f33f3d1d3",
            "40a6f6861d58440cb160f7cdb0dd7e75",
            "d625e33aee1a4fea87af66f0bc2f9f52",
            "f7658e589b614709b2f3069fdd70e052",
            "b803e9b65f10485790ab1f29899cc6b8",
            "c7279a5ee08b4066adfa6b8a09895c5c",
            "62bc6d58670f45388fc31d5ad3b9ac37",
            "36c2eaab19bd4f4b91141f90fb8d9f96",
            "03a325b0b08d47a0b0bcc2c9d2306f8e",
            "154e02ed7702410aa06b8dd29095b22b",
            "1d2faa03bd6b42218f436ebbb2ad573c",
            "aea8c793c2fc4b06aecc95ffd8192770",
            "47d1b1f816da48be853254c321a70748",
            "d23fdc7e0aa54e678abf6cbed8a48d9a",
            "4eb32378ac8f46b5b9c65110a463dd2c",
            "262e2f65c07648d28f1c24ef8e10cf4e",
            "279dd0ccbc5a4f6cb1d7cdfea0d22c8c",
            "3542af1c7a604ce1b81dc2d04cb5d77f",
            "930c2a9061d245f29b408807f6d19807"
          ]
        },
        "id": "BgqgdgC1dpuv",
        "outputId": "8f145019-94ac-480e-8f98-1402676dc914"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1812059d6a8c4d2e96f7a442b3ac37b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-v5gahxxbx83u --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 122\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9c417a6a4aa4c219d28eca72355d7d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27567b319a0e4be3aba5e3cba09f27de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e73d2437ee2744d3890afb3a5e0b9c53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba1601a614ab4f4bbb03a0d4722d0339"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd72d39b3a0d42098a6506b03d124457"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03a325b0b08d47a0b0bcc2c9d2306f8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "d0e941d161954201a6acdf3da8af650b",
            "75c7643477154ac096cf2998de43b96b",
            "47b1afc6fd934ef69893c044740083ca",
            "70b87c12bb824fef87e13fa0c7b9ba8a",
            "fa41e1745a1b4747986b91f4bf5c42e3",
            "c5f57929183f4d74a1490ddde6f60f43",
            "66e205b5baf346e0b34f2426be16a715",
            "3661a09281bd42afa9537b496577d53f",
            "755b02eea42d4d32954b4c47173e5756",
            "6f3b44e805a44c2db87e30014dc60ac4",
            "f8abe8ac460a4e63a514de30024de9c2",
            "6a74b77758984c5f946dc548cbbc0e44",
            "f177f76a27e34243821fdede0e94fa6a",
            "4df3b08219834eac88444136602e31fe",
            "54c14bed2bf640e69078c385e96f7399",
            "57656b324892419d84414fca9a65290d",
            "4642f27ad47e430b85b28065a558b218",
            "0a2504f8142c49d0a89482d4d277f22a",
            "c78b47241be0454c9782a83f03f11e96",
            "4b5109c3e28741caa5014c10d7867e29",
            "79b6008e19f049aea0651da85353ade9",
            "31ff17ab504741938ef20b1841b28541",
            "2d0d602ad8184a8aa6d7ca5069962002",
            "5411c98f054d435bb4cbb5c15b6c6fdc",
            "9f45f079d9f842ba9831ffc5f76aa59c",
            "a8b8e119abc24143a516ada2159dbd5d",
            "4caf2f7b5f8e4ce88bef225810528ee5",
            "a81566090e3f436f84b6d7d0f940ee03",
            "5aa3dc87beb046fd960f4ce71d8021a9",
            "02ad3d1204534ffb9f0e209020b38661",
            "4b4561866f524e82b31eb6a61afa2a63",
            "f6452041bd4c4edeba16ea36aba92de0",
            "30f76833ae474e179eb33a106e7f545f",
            "35336582df9e4adb9183f02e14a4937f",
            "3bd0935d6aa5436bb262f1eabe8bfbc8",
            "5985ed3476684abda9cc4ccde1c088d2",
            "9e6760cd62e74d4aac00f034605b1021",
            "e9afa380314845bd9cec850122f596b9",
            "2290190323c24610a4504c44c9ba953c",
            "a9cd52ce28074c2eb82c2e8e69cdb1af",
            "2a381b9d86a64fcdbbcefda0935718a0",
            "6b17e7d5698748cc936fa3e567267028",
            "0dbbe5bd6118484b85f1be2fa64226cf",
            "cf679ff089f24d5b8d87a9ca3f7f3fea"
          ]
        },
        "id": "D9DenADLfhKl",
        "outputId": "119f3bbd-8817-45dd-f02d-005a4728ca60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0e941d161954201a6acdf3da8af650b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a74b77758984c5f946dc548cbbc0e44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d0d602ad8184a8aa6d7ca5069962002"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35336582df9e4adb9183f02e14a4937f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6VFHPAxfh4z",
        "outputId": "b4164a67-64d3-4f28-cb28-cfc8f2d44077"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftC4lSYmfn0r",
        "outputId": "9c4cd366-d0a8-4ba2-db5e-b69cbe0899d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
              " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "_oBeikqKftBj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "i0j8B2xDfvkK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the workflow\n",
        "res = generate_text(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
        "print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbAUpQ3MfyPi",
        "outputId": "85b4b9c0-7740-4799-d7a7-61200234f079"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explain me the difference between Data Lakehouse and Data Warehouse. Unterscheidung between data lakehouse and data warehouse is a common topic of discussion in the data engineering community, as both are designed to store large amounts of data but have different architectures, use cases, and benefits. A data lakehouse is a centralized repository that stores all the raw data from various sources in its original form, without transforming or processing it. On the other hand, a data warehouse is a structured repository that stores data in a specific format, typically after cleaning, transforming, and aggregating it.\n",
            "\n",
            "Here are some key differences between a data lakehouse and a data warehouse:\n",
            "\n",
            "1. Data Structure: A data lakehouse stores data in its raw, unprocessed form, while a data warehouse stores data in a structured format, typically after cleaning, transforming, and aggregating it.\n",
            "2. Data Sources: A data lakehouse can ingest data from various sources, including databases, APIs, files, and streaming data sources, while a data warehouse typically ingests data from a limited number of sources, such as transactional systems or external databases.\n",
            "3. Data Processing: A data lakehouse does not process data, whereas a data warehouse performs extensive data processing, including data cleansing, transformation, and aggregation, to make the data ready for analysis.\n",
            "4. Analytics Workloads: A data lakehouse is optimized for advanced analytics workloads, such as machine learning, data science, and real-time analytics, while a data warehouse is optimized for reporting and querying.\n",
            "5. Cost: A data lakehouse can be more cost-effective than a data warehouse, especially for large datasets, as it does not require the same level of data processing and transformation as a data warehouse.\n",
            "6. Governance: A data lakehouse provides better governance capabilities than a data warehouse, as it allows for easier management of data access and security, with features like row-level security and data masking.\n",
            "7. Scalability: A data lakehouse is highly scalable and can handle large volumes of data, making it an ideal choice for organizations with growing data needs.\n",
            "8. Flexibility: A data lakehouse offers greater flexibility than a data warehouse, as it can accommodate various data formats and structures, and can be easily integrated with other data storage systems.\n",
            "9. Real-time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# integrating to generated text to langchain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "# checking again that everything is working fine\n",
        "llm(prompt=\"Explain me the difference between Data Lakehouse and Data Warehouse.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "EseAZ5Udf1_A",
        "outputId": "11202b81-c84a-4b35-e69e-734b537f21b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Unterscheidung between data lakehouse and data warehouse is a common topic of discussion in the data engineering community, as both are designed to store large amounts of data but have different architectures, use cases, and benefits. A data lakehouse is a centralized repository that stores all the raw data from various sources in its original form, without transforming or processing it. On the other hand, a data warehouse is a structured repository that stores data in a specific format, typically after cleaning, transforming, and aggregating it.\\n\\nHere are some key differences between a data lakehouse and a data warehouse:\\n\\n1. Data Structure: A data lakehouse stores data in its raw, unprocessed form, while a data warehouse stores data in a structured format, typically after cleaning, transforming, and aggregating it.\\n2. Data Sources: A data lakehouse can ingest data from various sources, including databases, APIs, files, and streaming data sources, while a data warehouse typically ingests data from a limited number of sources, such as transactional systems or external databases.\\n3. Data Processing: A data lakehouse does not process data, whereas a data warehouse performs extensive data processing, including data cleansing, transformation, and aggregation, to make the data ready for analysis.\\n4. Analytics Workloads: A data lakehouse is optimized for advanced analytics workloads, such as machine learning, data science, and real-time analytics, while a data warehouse is optimized for reporting and querying.\\n5. Cost: A data lakehouse can be more cost-effective than a data warehouse, especially for large datasets, as it does not require the same level of data processing and transformation as a data warehouse.\\n6. Governance: A data lakehouse provides better governance capabilities than a data warehouse, as it allows for easier management of data access and security, with features like row-level security and data masking.\\n7. Scalability: A data lakehouse is highly scalable and can handle large volumes of data, making it an ideal choice for organizations with growing data needs.\\n8. Flexibility: A data lakehouse offers greater flexibility than a data warehouse, as it can accommodate various data formats and structures, and can be easily integrated with other data storage systems.\\n9. Real-time'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"]\n",
        "\n",
        "loader = WebBaseLoader(web_links)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "EuV4N4bkgMHI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7RFcH5GgjsL",
        "outputId": "d258ff91-9628-4d21-8033-504cd44c3a49"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='The Data and AI Company — DatabricksSkip to main contentWhy Databricks DiscoverFor ExecutivesFor Startups Lakehouse Architecture CustomersFeatured StoriesSee All CustomersPartnersCloud ProvidersDatabricks on AWS, Azure, and GCPConsulting & System IntegratorsExperts to build, deploy and migrate to DatabricksTechnology PartnersConnect your existing tools to your LakehouseC&SI Partner ProgramBuild, deploy or migrate to the LakehouseData PartnersAccess the ecosystem of data consumersPartner SolutionsFind custom industry and migration solutionsBuilt on DatabricksBuild, market and grow your businessProduct Databricks PlatformPlatform OverviewA unified platform for data, analytics and AIData ManagementData reliability, security and performanceSharingAn open, secure, zero-copy sharing for all dataData WarehousingETL and orchestration for batch and streaming dataGovernanceUnified governance for all data, analytics and AI assetsReal-Time AnalyticsReal-time analytics, AI and applications made simpleArtificial IntelligenceA data-centric approach to AIData EngineeringETL and orchestration for batch and streaming dataMosaicMLTrain and deploy secure generative AI modelsData ScienceCollaborative data science at scaleIntegrations and DataMarketplaceOpen marketplace for data, analytics and AIIDE IntegrationsBuild on the Lakehouse in your favorite IDEPartner ConnectDiscover and integrate with the Databricks ecosystemPricingDatabricks PricingExplore product pricing, DBUs and moreCost CalculatorEstimate your compute costs on any cloudOpen SourceOpen Source TechnologiesLearn more about the innovations behind the platformSolutions Databricks for IndustriesFinancial ServicesRetailHealthcare & Life SciencesMedia and EntertainmentManufacturingCommunicationsPublic SectorSee All IndustriesCross Industry SolutionsConsumer Data Platform Cyber SecurityMigration & DeploymentData MigrationProfessional ServicesSolution AcceleratorsExplore AcceleratorsMove faster toward outcomes that matterResources Training and CertificationLearning OverviewHub for training, certification, events and moreTraining OverviewDiscover curriculum tailored to your needsLearning PathsGuided learning by role and career pathCertificationGain recognition and differentiationUniversity AllianceWant to teach Databricks? See how.EventsData + AI SummitData + AI World TourEvent CalendarBlog and PodcastsDatabricks BlogExplore news, product announcements, and moreDatabricks Mosaic AI Research BlogDiscover the latest in our Gen AI researchData Brew PodcastLet’s talk data!Champions of Data + AI PodcastInsights from data leaders powering innovationGet HelpCustomer SupportDocumentationCommunityDive DeepResource CenterDemo CenterAbout CompanyWho We AreOur TeamDatabricks VenturesContact UsCareersWorking at DatabricksOpen JobsPressAwards and RecognitionNewsroomSecurity and TrustSecurity and TrustReady to get started?Get a DemoLoginContact UsTry DatabricksYour data. Your AI.Your future.Own them all on the new data intelligence platformExplore demosLearn moreOn-demand WebinarImplement generative AI faster, from RAG to pre-training, with a data-centric approachWatch nowPLATFORMThe DatabricksData Intelligence PlatformDatabricks brings AI to your data to help you bring AI to the world.Succeed with AIDevelop generative AI applications on your data without sacrificing data privacy or control.Democratize insightsEmpower everyone in your organization to discover insights from your data using natural language.Drive down costsGain efficiency and simplify complexity by unifying your approach to data, AI and governance.Explore the platformUSE CASESUnify all your data + AIAIGovernanceWarehousingETLData sharingOrchestrationBuild better AI with a data-centric approachGreat models are built with great data. With Databricks, lineage, quality, control and data privacy are maintained across the entire AI workflow, powering a complete set of tools to deliver any AI use case.✔ Create, tune and deploy your own generative AI models✔\\xa0Automate experiment tracking and governance✔\\xa0Deploy and monitor models at scaleSee howSchedule demoCUSTOMER STORIESIndustry leaders are data + AI companiesOur customers achieve breakthroughs, innovate faster and drive down costs. See how you can too.2YRhigh ROI attainedJetBlue soars on data + AIWith over 40 million customers and 1,000 daily flights, JetBlue is leveraging the power of LLMs and Gen AI to optimize operations, grow new and existing revenue sources, reduce flight delays and enhance efficiency.See the full story$6Min infrastructure cost savingsCondé Nast crafts bespoke content with data + AICondé Nast aims to deliver personalized content to every consumer across\\xa0their\\xa037 brands. Unity Catalog and Databricks SQL drive faster analysis and decision-making, ensuring Condé Nast is providing compelling customer experiences at the right time.See the full story2PBof data governed by Unity CatalogBlock redefines financial services with data + AIWith brands like Square, Cash App and Afterpay, Block is unifying data + AI on Databricks, including LLMs that will provide customers with easier access to financial opportunities for economic growth.See the full storySee more Customer StoriesTOOLS AND INTEGRATIONSPlug into what you already useSpeed up success in data + AIThe Databricks Data Intelligence Platform integrates with your current tools for ETL, data ingestion, business intelligence, AI and governance. Adopt what’s next without throwing away what works.Browse integrationsRESOURCESMore than meets the AIGet helpEverything you need to succeed on lakehouseSupportTrainingCommunitySee what’s newOur latest announcements, expert analyses and eventsBlogNewsEventsIn the spotlight \\n\\nREPORTStrategies to transition to the new stage of data + AIRead nowREPORTDatabricks named a Leader in the 2023 Gartner®️ Magic Quadrant™️ for CDBMSRead nowEBOOKData-centric MLOps and LLMOpsRead nowEBOOKBig Book of Data Engineering: 2nd EditionRead nowSee more resourcesReady to become a data + AI company?Take the first steps in your transformationBrowse demosTry it freeWhy DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsWhy DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustDatabricks Inc.\\n160 Spear Street, 13th Floor\\nSan Francisco, CA 94105\\n1-866-330-0121See Careers\\nat Databricks© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the\\xa0Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='\\nDatabricks Community\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoadingÃ—Sorry to interruptCSS ErrorRefresh', metadata={'source': 'https://help.databricks.com', 'title': 'Databricks Community', 'language': 'en-US'}),\n",
              " Document(page_content='Try Databricks | Databricks\\n\\nTry Databricks free Test-drive the full Databricks platform free for 14 days on your choice of AWS, Microsoft Azure or Google Cloud. Sign-up with your work email to elevate your trial experience.Simplify data ingestion and automate ETLIngest data from hundreds of sources. Use a simple declarative approach to build data pipelines.Collaborate in your preferred languageCode in Python, R, Scala and SQL with coauthoring, automatic versioning, Git integrations\\xa0and RBAC.12x better price/performance than cloud data warehousesSee why over 7,000 customers worldwide rely on Databricks for all their workloads from BI to AI.Create your Databricks account1/2Sign up with your work email to elevate your trial with expert assistance and more.First nameLast nameEmailCompanyTitlePhone (Optional)SelectCountryContinuePrivacy Notice (Updated)Terms of UseYour Privacy ChoicesYour California Privacy Rights', metadata={'source': 'https://databricks.com/try-databricks', 'title': 'Try Databricks | Databricks', 'description': 'Discover why businesses are turning to Databricks to accelerate innovation. Try Databricks’ Full Platform Trial free for 14 days!', 'language': 'en-US'}),\n",
              " Document(page_content='\\nDatabricks Community\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoadingÃ—Sorry to interruptCSS ErrorRefresh', metadata={'source': 'https://help.databricks.com/s/', 'title': 'Databricks Community', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks documentation | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks documentation\\n\\n\\n\\n\\n\\n\\n\\nDatabricks documentation \\nDatabricks documentation provides how-to guidance and reference information for data analysts, data scientists, and data engineers solving problems in analytics and AI. The Databricks Data Intelligence Platform enables data teams to collaborate on data stored in the lakehouse. See What is a data lakehouse?.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTry Databricks \\n\\nGet a free trial & set up\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild a simple lakehouse analytics pipeline\\nFree training\\n\\n\\n\\nWhat do you want to do? \\n\\nData science & engineering\\nMachine learning\\nSQL queries & visualizations\\n\\n\\n\\nManage Databricks \\n\\nAccount & workspace administration\\nSecurity & compliance\\nData governance\\n\\n\\n\\nReference Guides \\n\\nAPI reference\\nSQL language reference\\nError handling and error messages\\n\\n\\n\\n\\nResources \\n\\nRelease notes\\nOther resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://docs.databricks.com', 'title': 'Databricks documentation | Databricks on AWS', 'description': 'How-to guidance and reference information for data analysts, data scientists, and data engineers working in the Databricks Data Science & Engineering, Databricks Machine Learning, and Databricks SQL environments.', 'language': 'en-US'}),\n",
              " Document(page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks Knowledge Base\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks Knowledge Base\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMain Navigation\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\nCommunity\\nTraining\\nFeedback\\n\\n\\n\\n\\n\\n\\nWelcome to the Databricks Knowledge Base\\n\\n\\n\\nAll Categories\\nAWS\\nAzure\\nGCP\\nAll articles\\nTraining\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContact Us\\nIf you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible.\\n\\n\\nYour email address\\n\\nSubject\\n\\nDescription\\n\\nPlease enter the details of your request. A member of our support staff will respond as soon as possible.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStart Here\\nSearch & Browse the Databricks Knowledge Base.\\n\\n\\n\\n\\nAWS\\nTopics on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\nAzure\\nTopics on Azure\\n\\n\\n\\n\\n\\n\\n\\n\\nGCP\\nTopics on GCP\\n\\n\\n\\n\\n\\n\\n\\nHelp Topics\\nWhether it's a very specific question, or a vague one, we have it covered in our knowledge base.\\n\\n\\n\\n\\n\\n\\nAWS\\n\\n\\n\\n\\n\\n\\n\\nAzure\\n\\n\\n\\n\\n\\n\\n\\nGCP\\n\\n\\n\\n\\n\\n\\n\\nAll articles\\n\\n\\n\\n\\n\\n\\n\\nTraining\\n15 Articles by 1 Authors\\n\\n\\n\\n\\n\\n\\n\\n\\nPopular Articles\\n\\n\\nUnexpected cluster termination\\nSometimes a cluster is terminated unexpectedly, not as a result of a manual termi...\\n\\n\\nSpark job fails with Driver is temporarily unavailable\\nProblem When running notebooks or jobs on a cluster, they run successfully multip...\\n\\n\\nApache Spark job fails with Parquet column cannot be converted error\\nProblem You are reading data in Parquet format and writing to a Delta table when ...\\n\\n\\nHow to improve performance of Delta Lake MERGE INTO queries using partition pruning\\nThis article explains how to trigger partition pruning in Delta Lake MERGE INTO (...\\n\\n\\nCreate a DataFrame from a JSON string or Python dictionary\\nIn this article we are going to review how you can create an Apache Spark DataFra...\\n\\n\\nAppend to a DataFrame\\nTo append to a DataFrame, use the union method. %scala val firstDF = spark.range(...\\n\\n\\nGet and set Apache Spark configuration properties in a notebook\\nIn most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe...\\n\\n\\nPrevent duplicated columns when joining two DataFrames\\nIf you perform a join in Spark and don’t specify your join correctly you’ll end u...\\n\\n\\nGenerate unique increasing numeric values\\nThis article shows you how to use Apache Spark functions to generate unique incre...\\n\\n\\nForbidden error while accessing S3 data\\nProblem While trying to access S3 data using DBFS mount or directly in Spark APIs...\\n\\n\\n\\n\\nCan't Find What you're Looking for?\\nFeel free to suggest an article topic by clicking on the button below.\\n\\nOffer feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n            © Databricks 2022-2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\r\\n        \\n\\nSend us feedback\\r\\n        \\r\\n       | Privacy Notice (Updated) | Terms of Use | Your Privacy Choices | Your California Privacy Rights \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDefinition by Author\\n\\n\\n\\n0\\n\\n\\n\\n\\n\\n0\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://kb.databricks.com/', 'title': 'Databricks Knowledge Base', 'description': 'Databricks Support Center helps you to find FAQ, how-to guides and step-by-step tutorials.', 'language': ''}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started: Account and workspace setup | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup\\n\\n\\n\\n\\n\\n\\n\\nGet started: Account and workspace setup \\nIf you’re new to Databricks, you’ve found the place to start. This article walks you through the minimum steps required to create your account and get your first workspace up and running.\\nFor information about online training resources, see Get free Databricks training.\\n\\n\\n\\nRequirements \\nTo use your Databricks account on AWS, you need an existing AWS account. If you don’t have an AWS account, you can sign up for an AWS Free Tier account at https://aws.amazon.com/free/.\\n\\n\\nStep 1: Sign up for a free trial \\nYou can sign up for your free Databricks trial either on the AWS Marketplace or through the Databricks website. The only difference between the two is where you’ll handle the account billing after the free trial ends.\\nFor detailed instructions on the free trial and billing, see Databricks free trial.\\n\\n\\nStep 2: Create your first Databricks workspace \\nAfter you sign up for the free trial, you’re prompted to set up your first workspace using the AWS Quick Start. This automated template is the recommended method for workspace creation. It creates Databricks-enabled AWS resources for you so you can get your workspace up and running quickly.\\nFor instructions on the AWS Quick Start, see Create a workspace using the AWS Quick Start (Recommended).\\nIf you’re more familiar with AWS and want to create your own AWS resources, see Manually create a workspace (existing Databricks accounts).\\n\\n\\nStep 3: Explore and use the Databricks platform \\nAt this point, you have a functional Databricks workspace. To learn how to navigate the platform, see Navigate the workspace. To jump in and start querying data, run the Tutorial: Query data with notebooks tutorial.\\n\\n\\nNext steps \\nYour next steps depend on whether you want to continue setting up your account organization and security or want to start building out data pipelines:\\n\\nTo onboard data to your workspace in Databricks SQL, see Load data using streaming tables in Databricks SQL.\\nTo continuing building out your account organization and security, follow the steps in Get started with Databricks administration.\\n\\n\\n\\nGet help \\nIf you have any questions about setting up Databricks and need live help, please e-mail onboarding-help@databricks.com.\\nIf you have a Databricks support package, you can open and manage support cases with Databricks. See Learn how to use Databricks support.\\nIf your organization does not have a Databricks support subscription, or if you are not an authorized contact for your company’s support subscription, you can get answers to many questions in Databricks Office Hours or from the Databricks Community.\\nIf you need additional help, sign up for a live weekly demo to ask questions and practice alongside Databricks experts. Or, follow this blog series on best practices for managing and maintaining your environments.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/getting-started/index.html', 'title': 'Get started: Account and workspace setup | Databricks on AWS', 'description': 'Learn how to set up a Databricks free trial and a cloud provider account with AWS.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Databricks? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\n  What is a data lakehouse?\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations\\n\\n\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Databricks?\\n\\n\\n\\n\\n\\n\\n\\nWhat is Databricks? \\nDatabricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. The Databricks Data Intelligence Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.\\n\\nHow does a data intelligence platform work? \\nDatabricks uses generative AI with the data lakehouse to understand the unique semantics of your data. Then, it automatically optimizes performance and manages infrastructure to match your business needs.\\nNatural language processing learns your business’s language, so you can search and discover data by asking a question in your own words. Natural language assistance helps you write code, troubleshoot errors, and find answers in documentation.\\nFinally, your data and AI applications can rely on strong governance and security. You can integrate APIs such as OpenAI without compromising data privacy and IP control.\\n\\n\\nWhat is Databricks used for? \\nDatabricks provides tools that help you connect your sources of data to one platform to process, store, share, analyze, model, and monetize datasets with solutions from BI to generative AI.\\nThe Databricks workspace provides a unified interface and tools for most data tasks, including:\\n\\nData processing scheduling and management, in particular ETL\\nGenerating dashboards and visualizations\\nManaging security, governance, high availability, and disaster recovery\\nData discovery, annotation, and exploration\\nMachine learning (ML) modeling, tracking, and model serving\\nGenerative AI solutions\\n\\n\\n\\nManaged integration with open source \\nDatabricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects originally created by Databricks employees:\\n\\nDelta Lake and Delta Sharing\\nMLflow\\nApache Spark and Structured Streaming\\nRedash\\n\\n\\n\\nTools and programmatic access \\nDatabricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:\\n\\nWorkflows\\nUnity Catalog\\nDelta Live Tables\\nDatabricks SQL\\nPhoton compute clusters\\n\\nIn addition to the workspace UI, you can interact with Databricks programmatically with the following tools:\\n\\nREST API\\nCLI\\nTerraform\\n\\n\\n\\nHow does Databricks work with AWS? \\nThe Databricks platform architecture comprises two primary parts:\\n\\nThe infrastructure used by Databricks to deploy, configure, and manage the platform and services.\\nThe customer-owned infrastructure managed in collaboration by Databricks and your company.\\n\\nUnlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.\\nUnity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\\nDatabricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.\\n\\n\\nWhat are common use cases for Databricks? \\nUse cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.\\n\\n\\nBuild an enterprise data lakehouse \\nThe data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See What is a data lakehouse?.\\n\\n\\nETL and data engineering \\nWhether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.\\nDelta Live Tables simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.\\nDatabricks provides a number of custom tools for data ingestion, including Auto Loader, an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.\\n\\n\\nMachine learning, AI, and data science \\nDatabricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including MLflow and Databricks Runtime for Machine Learning.\\n\\nLarge language models and generative AI \\nDatabricks Runtime for Machine Learning includes libraries like Hugging Face Transformers that allow you to integrate existing pre-trained models or other open-source libraries into your workflow. The Databricks MLflow integration makes it easy to use the MLflow tracking service with transformer pipelines, models, and processing components. In addition, you can integrate OpenAI models or solutions from partners like John Snow Labs in your Databricks workflows.\\nWith Databricks, you can customize a LLM on your data for your specific task. With the support of open source tooling, such as Hugging Face and DeepSpeed, you can efficiently take a foundation LLM and start training with your own data to have more accuracy for your domain and workload.\\nIn addition, Databricks provides AI functions that SQL data analysts can use to access LLM models, including from OpenAI, directly within their data pipelines and workflows. See AI Functions on Databricks.\\n\\n\\n\\nData warehousing, analytics, and BI \\nDatabricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as SQL warehouses, allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the SQL query editor or in notebooks. Notebooks support Python, R, and Scala in addition to SQL, and allow users to embed the same visualizations available in dashboards alongside links, images, and commentary written in markdown.\\n\\n\\nData governance and secure data sharing \\nUnity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.\\nUnity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See What is Unity Catalog?.\\nThe lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of Delta Sharing.\\n\\n\\nDevOps, CI/CD, and task orchestration \\nThe development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations. Workflows schedule Databricks notebooks, SQL queries, and other arbitrary code. Repos let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see Developer tools and guidance.\\n\\n\\nReal-time and streaming analytics \\nDatabricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See Streaming on Databricks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBest practice articles | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nPlatform administration cheat sheet\\nCompute creation cheat sheet\\nProduction job scheduling cheat sheet\\n\\n\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nBest practice articles\\n\\n\\n\\n\\n\\n\\n\\nBest practice articles \\nThis article provides a reference of best practice articles you can use to optimize your Databricks activity.\\n\\n\\nThe Databricks documentation includes a number of best practices articles to help you get the best performance at the lowest cost when using and administering Databricks.\\n\\nCheat sheets \\nCheat sheets provide you with a high-level view of practices you should be implementing in your Databricks account and workflows. Each cheat sheet includes a table of best practices, their impact, and helpful resources. Available cheat sheets include the following:\\n\\nPlatform administration cheat sheet\\nCompute creation cheat sheet\\nProduction job scheduling cheat sheet\\n\\n\\n\\nBest practice articles \\nThe following articles provide you with best practice guidance for various Databricks features.\\n\\nDelta Lake best practices\\nHyperparameter tuning with Hyperopt\\nDeep learning in Databricks\\nRecommendations for MLOps\\nUnity Catalog best practices\\nCluster configuration best practices\\nInstance pool configuration best practices\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/getting-started/tutorials/index.html', 'title': 'Best practice articles | Databricks on AWS', 'description': 'Explore best practice articles to help you make the most out of Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks release notes | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\nDatabricks platform release notes\\nDatabricks Runtime release notes versions and compatibility\\nDatabricks SQL release notes\\nDatabricks developer tools and SDKs release notes\\nDatabricks Connect release notes\\nDelta Live Tables release notes\\nDatabricks preview releases\\n\\n\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks release notes\\n\\n\\n\\n\\n\\n\\n\\nDatabricks release notes \\nDatabricks release notes are organized by release vehicle:\\n\\nDatabricks platform release notes cover the features that we develop for the Databricks environment.\\nDatabricks Runtime release notes versions and compatibility cover the features that we develop for Databricks Runtime. This includes proprietary features and optimizations. A Databricks Runtime version includes the set of core components that run on the clusters managed by Databricks. Each new verion provides updates that substantially improve the usability, performance, and security of big data analytics.\\nDatabricks SQL release notes cover the features that we develop for the Databricks SQL user interface and SQL warehouses. You can also find the available Databricks SQL versions, the rollout schedule, and features related to each version.\\nDatabricks developer tools and SDKs release notes cover the features for our IDE extensions and plugins, command-line interfaces, and SDKs.\\nDelta Live Tables release notes cover the features, bug fixes, and runtime upgrade process for Delta Live Tables.\\n\\n\\n\\n\\nLearn about the kinds of preview releases and how Databricks supports them.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/release-notes/index.html', 'title': 'Databricks release notes | Databricks on AWS', 'description': 'Learn about Databricks releases for the Databricks platform, the Databricks Runtime, Databricks SQL, Delta Live Tables, and more.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoad data into a Databricks lakehouse | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nOnboard data from S3\\nAdd data UI\\nCOPY INTO\\nAuto Loader\\n\\n\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nLoad data into a Databricks lakehouse\\n\\n\\n\\n\\n\\n\\n\\nLoad data into a Databricks lakehouse \\nDatabricks offers a variety of ways to help you load data into a lakehouse backed by Delta Lake. Databricks recommends using Auto Loader for incremental data ingestion from cloud object storage. The add data UI provides a number of options for quickly uploading local files or connecting to external data sources.\\n\\n\\n\\nRun your first ETL workload \\nIf you haven’t used Auto Loader on Databricks, start with a tutorial. See Run your first ETL workload on Databricks.\\n\\n\\nAuto Loader \\nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without additional setup. Auto Loader provides a Structured Streaming source called cloudFiles. Given an input directory path on the cloud file storage, the cloudFiles source automatically processes new files as they arrive, with the option of also processing existing files in that directory.\\n\\n\\nAutomate ETL with Delta Live Tables and Auto Loader \\nYou can simplify deployment of scalable, incremental ingestion infrastructure with Auto Loader and Delta Live Tables. Note that Delta Live Tables does not use the standard interactive execution found in notebooks, instead emphasizing deployment of infrastructure ready for production.\\n\\nTutorial: Run your first ETL workload on Databricks\\nLoad data using streaming tables (Python/SQL notebook)\\n\\n\\nLoad data using streaming tables in Databricks SQL\\n\\n\\n\\nUpload local data files or connect external data sources \\nYou can securely upload local data files or ingest data from external sources to create tables. See Load data using the add data UI.\\n\\n\\nLoad data into Databricks using third-party tools \\nDatabricks validates technology partner integrations that enable you to load data into Databricks. These integrations enable low-code, scalable data ingestion from a variety of sources into Databricks. See Technology partners. Some technology partners are featured in Databricks Partner Connect, which provides a UI that simplifies connecting third-party tools to your lakehouse data.\\n\\n\\nCOPY INTO \\nCOPY INTO allows SQL users to idempotently and incrementally load data from cloud object storage into Delta tables. It can be used in Databricks SQL, notebooks, and Databricks Jobs.\\n\\n\\nWhen to use COPY INTO and when to use Auto Loader \\nHere are a few things to consider when choosing between Auto Loader and COPY INTO:\\n\\nIf you’re going to ingest files in the order of thousands, you can use COPY INTO. If you are expecting files in the order of millions or more over time, use Auto Loader. Auto Loader requires fewer total operations to discover files compared to COPY INTO and can split the processing into multiple batches, meaning that Auto Loader is less expensive and more efficient at scale.\\nIf your data schema is going to evolve frequently, Auto Loader provides better primitives around schema inference and evolution. See Configure schema inference and evolution in Auto Loader for more details.\\nLoading a subset of re-uploaded files can be a bit easier to manage with COPY INTO. With Auto Loader, it’s harder to reprocess a select subset of files. However, you can use COPY INTO to reload the subset of files while an Auto Loader stream is running simultaneously.\\n\\n\\nFor an even more scalable and robust file ingestion experience, Auto Loader enables SQL users to leverage streaming tables. See Load data using streaming tables in Databricks SQL.\\n\\nFor a brief overview and demonstration of Auto Loader, as well as COPY INTO, watch the following YouTube video (2 minutes).\\n\\n\\xa0\\n\\n\\nReview file metadata captured during data ingestion \\nApache Spark automatically captures data about source files during data loading. Databricks lets you access this data with the File metadata column.\\n\\n\\nUpload spreadsheet exports to Databricks \\nUse the Create or modify table from file upload page to upload CSV, TSV, or JSON files. See Create or modify a table using file upload.\\n\\n\\nMigrate data applications to Databricks \\nMigrate existing data applications to Databricks so you can work with data from many source systems on a single platform. See Migrate data applications to Databricks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/ingestion/index.html', 'title': 'Load data into a Databricks lakehouse | Databricks on AWS', 'description': 'Learn about the different ways to ingest data into a lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExploratory data analysis on Databricks: Tools and techniques | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nExplore storage and find data files\\nExplore database objects\\nView frequent queries and users of a table\\nEDA\\nVisualizations in Databricks SQL\\nVisualizations in notebooks\\nVisualization types\\nPreview chart visualizations\\nNo-code EDA with bamboolib\\n\\n\\nSample datasets\\n\\n\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDiscover data \\nExploratory data analysis on Databricks: Tools and techniques\\n\\n\\n\\n\\n\\n\\n\\nExploratory data analysis on Databricks: Tools and techniques \\nThis article describes tools and techniques for exploratory data analysis (EDA) on Databricks.\\n\\n\\n\\nWhat is EDA and why is it useful? \\nExploratory data analysis (EDA) includes methods for exploring data sets to summarize their main characteristics and identify any problems with the data. Using statistical methods and visualizations, you can learn about a data set to determine its readiness for analysis and inform what techniques to apply for data preparation. EDA can also influence which algorithms you choose to apply for training ML models.\\n\\n\\nWhat are the EDA tools in Databricks? \\nDatabricks has built-in analysis and visualization tools in both Databricks SQL and in Databricks Runtime. For an illustrated list of the types of visualizations available in Databricks, see Visualization types.\\n\\nEDA in Databricks SQL \\nHere are some helpful articles about data visualization and exploration tools in Databricks SQL:\\n\\nVisualize queries and create a dashboard in Databricks SQL\\nCreate data visualizations in Databricks SQL\\n\\n\\n\\nEDA in Databricks Runtime \\nDatabricks Runtime provides a pre-built environment that has popular data exploration libraries already installed. You can see the list of the built-in libraries in the release notes.\\nIn addition, the following articles show examples of visualization tools in Databricks Runtime:\\n\\nCreate data visualizations in Databricks notebooks\\nDo no-code EDA with bamboolib\\n\\nIn a Databricks Python notebook, you can combine SQL and Python to explore data. When you run code in a SQL language cell in a Python notebook, the table results are automatically made available as a Python DataFrame. For details, see Explore SQL cell results in Python notebooks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/exploratory-data-analysis/index.html', 'title': 'Exploratory data analysis on Databricks: Tools and techniques | Databricks on AWS', 'description': 'Learn about tools and techniques for doing exploratory data analysis (EDA) on Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction to data preparation in Databricks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nIntroduction to data preparation in Databricks\\n\\n\\n\\n\\n\\n\\n\\nIntroduction to data preparation in Databricks \\nThis article describes how Databricks can help you with data preparation for analytics and machine learning. Data preparation is typically the most time-consuming component of an analytics and machine learning project, and good data is important to ensure accurate and useful results.\\n\\nData preparation tasks \\nData preparation includes the following tasks:\\n\\nCleaning and formatting data. This includes tasks such as handling missing values or outliers, ensuring data is in the correct format, and removing unneeded columns.\\nPreprocessing data. This includes tasks like numerical transformations, aggregating data, encoding text or image data, and creating new features.\\nCombining data. This includes tasks like joining tables or merging datasets.\\n\\n\\n\\nData preparation resources and information \\nThe Databricks platform provides a unified platform for data ingestion, preparation, analytics and machine learning, and monitoring.\\n\\nThe medallion lakehouse architecture guides you in data preparation by specifying a set of data layers of increasing quality. The architecture maintains ACID guarantees as data passes through multiple layers of validations and transformations before being stored in a layout optimized for efficient analytics.\\nDelta Live Tables is a framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data, and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.\\nDatabricks Partner Connect lets you connect your Databricks workspace directly to third-party data preparation and transformation partners. Partner Connect provisions the required Databricks resources on your behalf, then passes resource details to the partner.\\nDatabricks Runtime and Databricks Runtime ML provide pre-built environments that come with many of the most widely used data preparation libraries already installed. A list of all built-in libraries is available in the release notes.\\nFeature engineering for machine learning is the process of converting raw data into features that can be used to develop machine learning models. For ML applications, Databricks Feature Store helps your team discover and re-use features, track feature lineage, and publish features to online stores for realtime serving and automatic lookup.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare data securely using Delta Sharing | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nSet up Delta Sharing for your account (for providers)\\nDatabricks-to-Databricks sharing\\nOpen sharing\\nCreate and manage recipients\\nCreate and manage shares\\nGrant access to shares\\nAccess data shared with you\\nRead shared data (Databricks-to-Databricks)\\nRead shared data (open sharing)\\nAudit data sharing (for recipients)\\nAudit data sharing (for providers)\\nManage providers (for recipients)\\nUse IP access lists to restrict access\\nTroubleshoot common sharing errors\\n\\n\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nShare data securely using Delta Sharing\\n\\n\\n\\n\\n\\n\\n\\nShare data securely using Delta Sharing \\nThis article introduces Delta Sharing in Databricks, the secure data sharing platform that lets you share data in Databricks with users outside your organization, whether those users use Databricks or not.\\n\\nImportant\\nThe Delta Sharing articles on this site focus on sharing Databricks data and notebooks. Delta Sharing is also available as an open-source project that you can use to share Delta tables from other platforms. Delta Sharing also provides the backbone for Databricks Marketplace, an open forum for exchanging data products.\\n\\n\\nNote\\nIf you are a data recipient who has been granted access to shared data through Delta Sharing, and you just want to learn how to access that data, see Access data shared with you using Delta Sharing (for recipients).\\n\\n\\n\\n\\nWhat is Delta Sharing? \\nDelta Sharing is an open protocol developed by Databricks for secure data sharing with other organizations regardless of the computing platforms they use.\\nThere are three ways to share data using Delta Sharing:\\n\\nThe Databricks-to-Databricks sharing protocol, which lets you share data from your Unity Catalog-enabled workspace with users who also have access to a Unity Catalog-enabled Databricks workspace.\\nThis approach uses the Delta Sharing server that is built into Databricks. It supports some Delta Sharing features that are not suppported in the other protocols, including notebook sharing, Unity Catalog volume sharing, Unity Catalog model sharing, Unity Catalog data governance, auditing, and usage tracking for both providers and recipients. The integration with Unity Catalog simplifies setup and governance for both providers and recipients and improves performance.\\nSee Share data using the Delta Sharing Databricks-to-Databricks protocol (for providers).\\n\\nThe Databricks open sharing protocol, which lets you share tabular data that you manage in a Unity Catalog-enabled Databricks workspace with users on any computing platform.\\nThis approach uses the Delta Sharing server that is built into Databricks and is useful when you manage data using Unity Catalog and want to share it with users who don’t use Databricks or don’t have access to a Unity Catalog-enabled Databricks workspace. The integration with Unity Catalog on the provider side simplifies setup and governance for providers.\\nSee Share data using the Delta Sharing open sharing protocol (for providers).\\n\\nA customer-managed implementation of the open-source Delta Sharing server, which lets you share from any platform to any platform, whether Databricks or not.\\nThe Databricks documentation does not cover instructions for setting up your own Delta Sharing server. See github.com/delta-io/delta-sharing.\\n\\n\\n\\n\\n\\nShares, providers, and recipients \\nThe primary concepts underlying Delta Sharing in Databricks are shares, providers, and recipients.\\n\\n\\nWhat is a share? \\nIn Delta Sharing, a share is a read-only collection of tables and table partitions that a provider wants to share with one or more recipients. If your recipient uses a Unity Catalog-enabled Databricks workspace, you can also include notebook files, views (including dynamic views that restrict access at the row and column level), Unity Catalog volumes, and Unity Catalog models in a share.\\nYou can add or remove tables, views, volumes, models, and notebook files from a share at any time, and you can assign or revoke data recipient access to a share at any time.\\nIn a Unity Catalog-enabled Databricks workspace, a share is a securable object registered in Unity Catalog. If you remove a share from your Unity Catalog metastore, all recipients of that share lose the ability to access it.\\nSee Create and manage shares for Delta Sharing.\\n\\n\\nWhat is a provider? \\nA provider is an entity that shares data with a recipient. If you are a provider and you want to take advantage of the built-in Databricks Delta Sharing server and manage shares and recipients using Unity Catalog, you need at least one Databricks workspace that is enabled for Unity Catalog. You do not need to migrate all of your existing workspaces to Unity Catalog. You can simply create a new Unity Catalog-enabled workspace for your Delta Sharing needs.\\nIf a recipient is on a Unity Catalog-enabled Databricks workspace, the provider is also a Unity Catalog securable object that represents the provider organization and associates that organization with a set of shares.\\n\\n\\nWhat is a recipient? \\nA recipient is an entity that receives shares from a provider. In Unity Catalog, a share is a securable object that represents an organization and associates it with a credential or secure sharing identifier that allows that organization to access one or more shares.\\nAs a data provider (sharer), you can define multiple recipients for any given Unity Catalog metastore, but if you want to share data from multiple metastores with a particular user or group of users, you must define the recipient separately for each metastore. A recipient can have access to multiple shares.\\nIf a provider deletes a recipient from their Unity Catalog metastore, that recipient loses access to all shares it could previously access.\\nSee Create and manage data recipients for Delta Sharing.\\n\\n\\n\\nOpen sharing versus Databricks-to-Databricks sharing \\nThis section describes the two protocols for sharing from a Databricks workspace that is enabled for Unity Catalog.\\n\\nNote\\nThis section assumes that the provider is on a Unity Catalog-enabled Databricks workspace. To learn about setting up an open-source Delta Sharing server to share from a non-Databricks platform or non-Unity Catalog workspace, see github.com/delta-io/delta-sharing.\\n\\nThe way a provider uses Delta Sharing in Databricks depends on who they are sharing data with:\\n\\nOpen sharing lets you share data with any user, whether or not they have access to Databricks.\\nDatabricks-to-Databricks sharing lets you share data with Databricks users whose workspace is attached to a Unity Catalog metastore that is different from yours. Databricks-to-Databricks also supports notebook, volume, and model sharing, which is not available in open sharing.\\n\\n\\nWhat is open Delta Sharing? \\nIf you want to share data with users outside of your Databricks workspace, regardless of whether they use Databricks, you can use open Delta Sharing to share your data securely. As a data provider, you generate a token and share it securely with the recipient. They use the token to authenticate and get read access to the tables you’ve included in the shares you’ve given them access to.\\nRecipients can access the shared data using many computing tools and platforms, including:\\n\\nDatabricks\\nApache Spark\\nPandas\\nPower BI\\n\\nFor a full list of Delta Sharing connectors and information about how to use them, see the Delta Sharing documentation.\\nSee also Share data using the Delta Sharing open sharing protocol (for providers).\\n\\n\\nWhat is Databricks-to-Databricks Delta Sharing? \\nIf you want to share data with users who have a Databricks workspace that is enabled for Unity Catalog, you can use Databricks-to-Databricks Delta Sharing. Databricks-to-Databricks sharing lets you share data with users in other Databricks accounts, whether they’re on AWS, Azure, or GCP. It’s also a great way to securely share data across different Unity Catalog metastores in your own Databricks account. Note that there is no need to use Delta Sharing to share data between workspaces attached to the same Unity Catalog metastore, because in that scenario you can use Unity Catalog itself to manage access to data across workspaces.\\nOne advantage of Databricks-to-Databricks sharing is that the share recipient doesn’t need a token to access the share, and the provider doesn’t need to manage recipient tokens. The security of the sharing connection—including all identity verification, authentication, and auditing—is managed entirely through Delta Sharing and the Databricks platform. Another advantage is the ability to share Databricks notebook files, views, Unity Catalog volumes, and Unity Catalog models.\\nSee also Share data using the Delta Sharing Databricks-to-Databricks protocol (for providers).\\n\\n\\n\\nHow do provider admins set up Delta Sharing? \\nThis section gives an overview of how providers can enable Delta Sharing and initiate sharing from a Unity Catalog-enabled Databricks workspace. For open-source Delta Sharing, see github.com/delta-io/delta-sharing.\\nDatabricks-to-Databricks sharing between Unity Catalog metastores in the same account is always enabled. If you are a provider who wants to enable Delta Sharing to share data with Databricks workspaces in other accounts or non-Databricks clients, a Databricks account admin or metastore admin performs the following setup steps (at a high level):\\n\\nEnable Delta Sharing for the Unity Catalog metastore that manages the data you want to share.\\n\\nNote\\nYou do not need to enable Delta Sharing on your metastore if you intend to use Delta Sharing to share data only with users on other Unity Catalog metastores in your account. Metastore-to-metastore sharing within a single Databricks account is enabled by default.\\n\\nSee Enable Delta Sharing on a metastore.\\n\\nCreate a share that includes data assets registered in the Unity Catalog metastore.\\nIf you are sharing with a non-Databricks recipient (known as open sharing) you can include tables in the Delta or Parquet format. If you plan to use Databricks-to-Databricks sharing, you can also add views, Unity Catalog volumes, Unity Catalog models, and notebook files to a share.\\nSee Create and manage shares for Delta Sharing.\\n\\nCreate a recipient.\\nSee Create and manage data recipients for Delta Sharing.\\nIf your recipient is not a Databricks user, or does not have access to a Databricks workspace that is enabled for Unity Catalog, you must use open sharing. A set of token-based credentials is generated for that recipient.\\nIf your recipient has access to a Databricks workspace that is enabled for Unity Catalog, you can use Databricks-to-Databricks sharing, and no token-based credentials are required. You request a sharing identifier from the recipient and use it to establish the secure connection.\\n\\nTip\\nUse yourself as a test recipient to try out the setup process.\\n\\n\\nGrant the recipient access to one or more shares.\\nSee Grant and manage access to Delta Sharing data shares (for providers).\\n\\nNote\\nThis step can also be performed by a non-admin user with the USE SHARE, USE RECIPIENT and SET SHARE PERMISSION privileges. See Unity Catalog privileges and securable objects.\\n\\n\\nSend the recipient the information they need to connect to the share (open sharing only).\\nSee Send the recipient their connection information.\\nFor open sharing, use a secure channel to send the recipient an activation link that allows them to download their token-based credentials.\\nFor Databricks-to-Databricks sharing, the data included in the share becomes available in the recipient’s Databricks workspace as soon as you grant them access to the share.\\n\\n\\nThe recipient can now access the shared data.\\n\\n\\nHow do recipients access the shared data? \\nRecipients access shared data assets in read-only format. Shared notebook files are read-only, but they can be cloned and then modified and run in the recipient workspace just like any other notebook.\\nSecure access depends on the sharing model:\\n\\nOpen sharing (recipient does not have a Databricks workspace enabled for Unity Catalog): The recipient provides the credential whenever they access the data in their tool of choice, including Apache Spark, pandas, Power BI, Databricks, and many more.  See Read data shared using Delta Sharing open sharing (for recipients).\\nDatabricks-to-Databricks (recipient workspace is enabled for Unity Catalog): The recipient accesses the data using Databricks. They can use Unity Catalog to grant and deny access to other users in their Databricks account. See Read data shared using Databricks-to-Databricks Delta Sharing (for recipients).\\n\\nWhenever the data provider updates data tables or volumes in their own Databricks account, the updates appear in near real time in the recipient’s system.\\n\\n\\nHow do you keep track of who is sharing and accessing shared data? \\nData providers on Unity Catalog-enabled Databricks workspaces can use Databricks audit logging and system tables to monitor the creation and modification of shares and recipients, and can monitor recipient activity on shares. See Audit and monitor data sharing using Delta Sharing (for providers).\\nData recipients who use shared data in a Databricks workspace can use Databricks audit logging and system tables to understand who is accessing which data. See Audit and monitor data access using Delta Sharing (for recipients).\\n\\n\\nSharing volumes \\nYou can share volumes using the Databricks-to-Databricks sharing flow. See Add volumes to a share (for providers) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients) (for recipients).\\n\\n\\nSharing models \\nYou can share models using the Databricks-to-Databricks sharing flow. See Add models to a share (for providers) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients) (for recipients).\\n\\n\\nSharing notebooks \\nYou can use Delta Sharing to share notebook files using the Databricks-to-Databricks sharing flow. See Add notebook files to a share (for providers) and Read shared notebooks (for recipients).\\n\\n\\nRestricting access at the row and column level \\nYou can share dynamic views that restrict access to certain table data based on recipient properties. Dynamic view sharing requires the Databricks-to-Databricks sharing flow. See Add dynamic views to a share to filter rows and columns.\\n\\n\\nDelta Sharing and streaming \\nDelta Sharing supports Spark Structured Streaming. A provider can share a table with history so that a recipient can use it as a Structured Streaming source, processing shared data incrementally with low latency. Recipients can also perform Delta Lake time travel queries on tables shared with history.\\nTo learn how to share tables with history, see Add tables to a share. To learn how to use shared tables as streaming sources, see Query a table using Apache Spark Structured Streaming (for recipients of Databricks-to-Databricks sharing) or Access a shared table using Spark Structured Streaming (for recipients of open sharing data).\\nSee also Streaming on Databricks.\\n\\n\\nDelta Sharing FAQs \\nThe following are frequently asked questions about Delta Sharing.\\n\\nDo I need Unity Catalog to use Delta Sharing? \\nNo, you do not need Unity Catalog to share (as a provider) or consume shared data (as a recipient). However, Unity Catalog provides benefits such as support for non-tabular asset sharing, out-of-the-box governance, simplicity, and query performance.\\nProviders can share data in two ways:\\n\\nPut the assets to share under Unity Catalog management and share them using the built-in Databricks Delta Sharing server.\\nYou do do not need to migrate all assets to Unity Catalog. You need only one Databricks workspace that is enabled for Unity Catalog to manage assets that you want to share. In some accounts, new workspaces are enabled for Unity Catalog automatically. See Automatic enablement of Unity Catalog.\\n\\nImplement the open Delta Sharing server to share data, without necessarily using your Databricks account.\\n\\nRecipients can consume data in two ways:\\n\\nWithout a Databricks workspace. Use open source Delta Sharing connectors that are available for many data platforms, including Power BI, pandas, and open source Apache Spark. See Read data shared using Delta Sharing open sharing (for recipients) and the Delta Sharing open source project.\\nIn a Databricks workspace. Recipient workspaces don’t need to be enabled for Unity Catalog, but there are advantages of governance, simplicity, and performance if they are.\\nRecipient organizations who want these advantages don’t need to migrate all assets to Unity Catalog. You need only one Databricks workspace that is enabled for Unity Catalog to manage assets that are shared with you. In some accounts, new workspaces are enabled for Unity Catalog automatically. See Automatic enablement of Unity Catalog.\\nRecipient organizations who want these advantages don’t need to migrate all assets to Unity Catalog. You need only one Databricks workspace that is enabled for Unity Catalog to manage assets that are shared with you.\\nSee Read data shared using Delta Sharing open sharing (for recipients) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients).\\n\\n\\n\\n\\nDo I need to be a Databricks customer to use Delta Sharing? \\nNo, Delta Sharing is an open protocol. You can share non-Databricks data with recipients on any data platform. Providers can configure an open Delta Sharing server to share from any computing platform. Recipients can consume shared data using open source Delta Sharing connectors for many data products, including Power BI, pandas, and open source Spark.\\nHowever, using Delta Sharing on Databricks, especially sharing from a Unity Catalog-enabled workspace, has many advantages.\\nFor details, see the first question in this FAQ.\\n\\n\\nAre egress costs high? \\n\\nDelta Sharing within a region incurs no egress cost.\\nDelta Sharing is the only solution that enables cross-cloud and cross-region sharing without data replication, which is why it can incur egress charges.\\nTo learn about how to reduce egress charges, talk to your Databricks account team.\\n\\n\\nCan providers revoke recipient access? \\nYes, recipient access can be revoked on-demand and at specified levels of granularity. You can deny recipient access to specific shares and specific IP addresses, filter tabular data for a recipient, revoke recipient tokens, and delete recipients entirely.  See Revoke recipient access to a share and Create and manage data recipients for Delta Sharing.\\n\\n\\nIsn’t it insecure to use pre-signed URLs? \\nDelta Sharing uses pre-signed URLs to provide temporary access to a file in object storage. They are only given to recipients that already have access to the shared data. They are secure because they are short-lived and don’t expand the level of access beyond what recipients have already been granted.\\n\\n\\n\\nAre the tokens used in the Delta Sharing open sharing protocol secure? \\nBecause Delta Sharing enables cross-platform sharing—unlike other available data sharing platforms—the sharing protocol requires an open token. Providers can ensure token security by configuring the token lifetime, setting networking controls, and revoking access on demand. In addition, the token does not expand the level of access beyond what recipients have already been granted. See Security considerations for tokens.\\nIf you prefer not to use tokens to manage access to recipient shares, you should use Databricks-to-Databricks sharing or contact your Databricks account team for alternatives.\\n\\n\\nDoes Delta Sharing support view sharing? \\nYes, Delta Sharing supports view sharing. See Add views to a share.\\nTo learn about planned enhancements to viewing sharing, contact your Databricks account team.\\n\\n\\n\\nLimitations \\n\\nTabular data must be in the Delta table format. You can easily convert Parquet tables to Delta—and back again. See CONVERT TO DELTA.\\nView sharing is supported only in Databricks-to-Databricks sharing. Shareable views must be defined on Delta tables or other shareable views. See (for providers)Add views to a share and (for consumers) Read shared views.\\nVolume sharing is supported only in Databricks-to-Databricks sharing. See Add volumes to a share (for providers) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients) (for recipients).\\nModel sharing is supported only in Databricks-to-Databricks sharing. See Add models to a share (for providers) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients) (for recipients).\\nThere are limits on the number of files in metadata allowed for a shared table. To learn more, see Resource limit exceeded errors.\\nSchemas named information_schema cannot be imported into a Unity Catalog metastore, because that schema name is reserved in Unity Catalog.\\nTable constraints (primary and foreign key constraints) are not available in shared tables.\\n\\n\\n\\nResource quotas \\nThe values below indicate the quotas for Delta Sharing resources. Quota values below are expressed relative to the parent object in Unity Catalog.\\n\\n\\n\\n\\n\\n\\n\\nObject\\nParent\\nValue\\n\\n\\n\\n provider\\n metastore\\n 1000\\n\\n recipients\\n metastore\\n 5000\\n\\n shares\\n metastore\\n 1000\\n\\n tables\\n share\\n 1000\\n\\n volumes\\n share\\n 1000\\n\\n models\\n share\\n 1000\\n\\n schemas\\n share\\n 500\\n\\n notebooks\\n share\\n 100\\n\\n\\n\\nIf you expect to exceed these resource limits, contact your Databricks account team.\\n\\n\\nNext steps \\n\\nEnable your Databricks account for Delta Sharing\\nCreate shares\\nCreate recipients\\nLearn more about the open sharing and Databricks-to-Databricks sharing models\\nLearn how recipients access shared data\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Databricks Marketplace? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\nRequest and access data products using Unity Catalog (consumer)\\nRequest and access data products using external platforms (consumer)\\nManage requests and installed products (consumer)\\nList data products (provider)\\nManage listings (provider)\\nManage consumer requests (provider)\\nCreate private exchanges (provider)\\nProvider policies\\n\\n\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Databricks Marketplace?\\n\\n\\n\\n\\n\\n\\n\\nWhat is Databricks Marketplace? \\nThis article introduces Databricks Marketplace, an open forum for exchanging data products. Databricks Marketplace takes advantage of Delta Sharing to give data providers the tools to share data products securely and data consumers the power to explore and expand their access to the data and data services they need.\\n\\n\\n\\n\\n\\n\\nWhat kinds of data assets are shared on Databricks Marketplace? \\nMarketplace assets include datasets, Databricks notebooks, Databricks Solution Accelerators, and machine learning (AI) models. Datasets are typically made available as catalogs of tabular data, although non-tabular data, in the form of Databricks volumes, is also supported. Solution Accelerators are available as clonable Git repos.\\n\\n\\nHow do consumers get access to data in Databricks Marketplace? \\nTo find a data product you want on the Databricks Marketplace, simply browse or search provider listings.\\nYou can browse:\\n\\nThe Open Marketplace, which does not require access to a Databricks workspace.\\nThe Databricks Marketplace on your Databricks workspace. Just click  Marketplace.\\n\\nTo request access to data products in the Marketplace, you must use the Marketplace on a Databricks workspace. You do not need a Databricks workspace to access and work with data once it is shared, although using a Databricks workspace with Unity Catalog enabled lets you take advantage of the deep integration of Unity Catalog with Delta Sharing.\\nSome data products are available to everyone in the public marketplace, and others are available as part of a private exchange, in which a provider shares their listings only with member consumers. Whether public or private, some data products are available instantly, as soon as you request them and agree to the terms. Others might require provider approval and transaction completion using provider interfaces. In either case, the Delta Sharing protocol that powers the Marketplace ensures that you can access shared data securely.\\n\\nGet started accessing data products \\nTo learn how to get started as a data consumer:\\n\\nUsing a Databricks workspace that is enabled for Unity Catalog, see Request and access data products in Databricks Marketplace (Unity Catalog-enabled workspaces).\\nUsing third-party platforms like Power BI, pandas, or Apache Spark, along with Databricks workspaces that are not enabled for Unity Catalog, see Request and access data products in Databricks Marketplace using external platforms.\\n\\n\\n\\n\\nHow do providers list data products in Databricks Marketplace? \\nDatabricks Marketplace gives data providers a secure platform for sharing data products that data scientists and analysts can use to help their organizations succeed. Databricks Marketplace uses Delta Sharing to provide security and control over your shared data. You can share public data, free sample data, and commercialized data offerings. You can share data products in public listings or as part of private exchanges that you create, making listings discoverable only by member consumers. In addition to datasets, you can also share Databricks notebooks and other content to demonstrate use cases and show customers how to take full advantage of your data products.\\n\\nGet started listing data products \\nTo list your data products on Databricks Marketplace, you must:\\n\\nHave a Databricks account and premium workspace that is enabled for Unity Catalog. You do not need to enable all of your workspaces for Unity Catalog. You can create one specifically for managing Marketplace listings.\\nApply to be a provider through the Databricks Data Partner Program.\\nReview the Marketplace provider policies.\\n\\nTo learn how to get started, see List your data product in Databricks Marketplace.\\n\\n\\n\\nView a demo \\nThis video introduces Databricks Marketplace, shows how consumers access listings, and demonstrates how providers create them.\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/marketplace/index.html', 'title': 'What is Databricks Marketplace? | Databricks on AWS', 'description': 'Learn how to use the Databricks Marketplace to provide and consume shared data securely.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks data engineering | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nDelta Live Tables\\nStructured Streaming\\nApache Spark\\nCompute\\nNotebooks\\nWorkflows\\nLibraries\\nInit scripts\\nRepos\\nDBFS\\nFiles\\nMigration\\nOptimization & performance\\n\\n\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks data engineering\\n\\n\\n\\n\\n\\n\\n\\nDatabricks data engineering \\nDatabricks data engineering features are a robust environment for collaboration among data scientists, data engineers, and data analysts. Data engineering tasks are also the backbone of  Databricks machine learning solutions.\\n\\nNote\\nIf you are a data analyst who works primarily with SQL queries and BI tools, you might prefer Databricks SQL.\\n\\nThe data engineering documentation provides how-to guidance to help you get the most out of the Databricks collaborative analytics platform. For getting started tutorials and introductory information, see Get started: Account and workspace setup and What is Databricks?.\\n\\n\\n\\nDelta Live TablesLearn how to build data pipelines for ingestion and transformation with Databricks Delta Live Tables.\\n\\n\\n\\nStructured StreamingLearn about streaming, incremental, and real-time workloads powered by Structured Streaming on Databricks.\\n\\n\\n\\nApache SparkLearn how Apache Spark works on Databricks and the Databricks platform.\\n\\n\\n\\nComputeLearn about Databricks clusters and how to create and manage them.\\n\\n\\n\\nNotebooksLearn what a Databricks notebook is, and how to use and manage notebooks to process, analyze, and visualize your data.\\n\\n\\n\\nWorkflowsLearn how to orchestrate data processing, machine learning, and data analysis workflows on the Databricks Data Intelligence Platform.\\n\\n\\n\\nLibrariesLearn how to make third-party or custom code available in Databricks using libraries. Learn about the different modes for installing libraries on Databricks.\\n\\n\\n\\nInit scriptsLearn how to use initialization (init) scripts to install packages and libraries, set system properties and environment variables, modify Apache Spark config parameters, and set other configurations on Databricks clusters.\\n\\n\\n\\nReposLearn how to use Git to version control your notebooks and other files for development in Databricks.\\n\\n\\n\\nDBFSLearn about Databricks File System (DBFS), a distributed file system mounted into a Databricks workspace and available on Databricks clusters\\n\\n\\n\\nFilesLearn about options for working with files on Databricks.\\n\\n\\n\\nMigrationLearn how to migrate data applications such as ETL jobs, enterprise data warehouses, ML, data science, and analytics to Databricks.\\n\\n\\n\\nOptimization & performanceLearn about optimizations and performance recommendations on Databricks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/workspace-index.html', 'title': 'Databricks data engineering | Databricks on AWS', 'description': 'Learn how get started with the Databricks data engineering tools and features.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI and Machine Learning on Databricks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\n10-minute tutorials\\nStep-by-step: AI and ML on Databricks\\nPrepare data and environment\\nFeature engineering\\nTrain machine learning models\\nTrain deep learning models\\nAutoML\\nUse Ray on Databricks\\nManage the ML lifecycle with MLflow\\nBatch inference\\nReference solutions\\nMLOps\\nGraphFrames\\n\\n\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nAI and Machine Learning on Databricks\\n\\n\\n\\n\\n\\n\\n\\nAI and Machine Learning on Databricks \\nThis article describes the tools that Databricks provides to help you build and monitor AI and ML workflows. The diagram shows how these components work together to help you implement your model development and deployment process.\\n\\n\\n\\n\\nWhy use Databricks for machine learning and deep learning? \\nWith Databricks, you can implement the full ML lifecycle on a single platform with end-to-end governance throughout the ML pipeline. Databricks includes the following built-in tools to support ML workflows:\\n\\nUnity Catalog for governance, discovery, versioning, and access control for data, features, models, and functions.\\nLakehouse Monitoring for data monitoring.\\nFeature engineering and serving.\\nSupport for the model lifecycle:\\n\\nDatabricks AutoML for automated model training.\\nMLflow for model development tracking.\\nUnity Catalog for model management.\\nDatabricks Model Serving for high-availability, low-latency model serving. This includes deploying LLMs using:\\n\\nFoundation Model APIs which allow you to access and query state-of-the-art open models from a serving endpoint.\\nExternal models which allow you to access models hosted outside of Databricks.\\n\\n\\nLakehouse Monitoring to track model prediction quality and drift.\\n\\n\\nDatabricks Workflows for automated workflows and production-ready ETL pipelines.\\nDatabricks Repos for code management and Git integration.\\n\\n\\n\\nDeep learning applications on Databricks \\nConfiguring infrastructure for deep learning applications can be difficult.\\nDatabricks Runtime for Machine Learning takes care of that for you, with clusters that have built-in compatible versions of the most common deep learning libraries like TensorFlow, PyTorch, and Keras, and supporting libraries such as Petastorm, Hyperopt, and Horovod. Databricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries.\\nFor machine learning applications, Databricks recommends using a cluster running Databricks Runtime for Machine Learning. See Create a cluster using Databricks Runtime ML.\\n\\n\\nUse Databricks for deep learning applications \\nDatabricks Machine Learning provides pre-built deep learning infrastructure, enabling development across the entire deep learning lifecycle. Databricks Model Serving and Databricks Runtime for Machine Learning include built-in, pre-configured GPU support with drivers and supporting libraries. Databricks Model Serving enables creation of scalable GPU endpoints for deep learning models with no extra configuration. Databricks Runtime for Machine Learning includes the most common deep learning libraries like TensorFlow, PyTorch, and Keras and supporting libraries like Petastorm, Hyperopt, and Horovod.\\nTo get started with deep learning on Databricks, see:\\n\\nBest practices for deep learning on Databricks\\nDeep learning on Databricks\\nReference solutions for deep learning\\n\\n\\n\\nLarge language models (LLMs) and generative AI on Databricks \\nDatabricks Runtime for Machine Learning includes libraries like Hugging Face Transformers and LangChain that allow you to integrate existing pre-trained models or other open-source libraries into your workflow. The Databricks MLflow integration makes it easy to use the MLflow tracking service with transformer pipelines, models, and processing components. In addition, you can integrate OpenAI models or solutions from partners like John Snow Labs in your Databricks workflows.\\nWith Databricks, you can customize a LLM on your data for your specific task. With the support of open source tooling, such as Hugging Face and DeepSpeed, you can efficiently take a foundation LLM and train it with your own data to improve its accuracy for your specific domain and workload. You can then leverage the custom LLM in your generative AI applications.\\nIn addition, Databricks provides Foundation Model APIs and external models which allows you to access and query state-of-the-art open models from a serving endpoint. Using Foundation Model APIs, developers can quickly and easily build applications that leverage a high-quality generative AI model without maintaining their own model deployment.\\nFor SQL users, Databricks provides AI functions that SQL data analysts can use to access LLM models, including from OpenAI, directly within their data pipelines and workflows. See AI Functions on Databricks.\\n\\n\\nDatabricks Runtime for Machine Learning \\nDatabricks Runtime for Machine Learning (Databricks Runtime ML) automates the creation of a cluster with pre-built machine learning and deep learning infrastructure including the most common ML and DL libraries. For the full list of libraries in each version of Databricks Runtime ML, see the release notes.\\nTo access data in Unity Catalog for machine learning workflows, the access mode for the cluster must be single user (assigned). Shared clusters are not compatible with Databricks Runtime for Machine Learning. In addition, Databricks Runtime ML is not supported on TableACLs clusters or clusters with spark.databricks.pyspark.enableProcessIsolation config set to true.\\n\\nCreate a cluster using Databricks Runtime ML \\nWhen you create a cluster, select a Databricks Runtime ML version from the Databricks runtime version drop-down menu. Both CPU and GPU-enabled ML runtimes are available.\\n\\n\\n\\nIf you select a cluster from the drop-down menu in the notebook, the Databricks Runtime version appears at the right of the cluster name:\\n\\n\\n\\nIf you select a GPU-enabled ML runtime, you are prompted to select a compatible Driver type and Worker type. Incompatible instance types are grayed out in the drop-down menu. GPU-enabled instance types are listed under the GPU accelerated label.\\n\\nNote\\nTo access data in Unity Catalog for machine learning workflows, the access mode for the cluster must be single user (assigned). Shared clusters are not compatible with Databricks Runtime for Machine Learning.\\n\\n\\n\\nLibraries included in Databricks Runtime ML \\nDatabricks Runtime ML includes a variety of popular ML libraries. The libraries are updated with each release to include new features and fixes.\\nDatabricks has designated a subset of the supported libraries as top-tier libraries. For these libraries, Databricks provides a faster update cadence, updating to the latest package releases with each runtime release (barring dependency conflicts). Databricks also provides advanced support, testing, and embedded optimizations for top-tier libraries.\\nFor a full list of top-tier and other provided libraries, see the release notes for Databricks Runtime ML.\\n\\n\\n\\nNext steps \\nTo get started, see:\\n\\nTutorials: Get started with ML\\n\\nFor a recommended MLOps workflow on Databricks Machine Learning, see:\\n\\nMLOps workflows on Databricks\\n\\nTo learn about key Databricks Machine Learning features, see:\\n\\nWhat is AutoML?\\nWhat is a feature store?\\nModel serving with Databricks\\nLakehouse Monitoring\\nManage model lifecycle\\nMLflow experiment tracking\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is data warehousing on Databricks? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\n   Get started\\n   What are SQL Warehouses\\n   SQL editor\\n   Queries\\n   Lakeview dashboards\\n   DBSQL dashboards\\n   Alerts\\n   Use materialized views\\n   Load data using streaming tables\\n\\n\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is data warehousing on Databricks?\\n\\n\\n\\n\\n\\n\\n\\nWhat is data warehousing on Databricks? \\nData warehousing refers to collecting and storing data from multiple sources so it can be quickly accessed for business insights and reporting. This article contains key concepts for building a data warehouse in your data lakehouse.\\n\\nData warehousing in your lakehouse \\nThe lakehouse architecture and Databricks SQL bring cloud data warehousing capabilities to your data lakes. Using familiar data structures, relations, and management tools, you can model a highly-performant, cost-effective data warehouse that runs directly on your data lake. For more information, see What is a data lakehouse?\\n\\n\\n\\nAs with a traditional data warehouse, you model data according to business requirements and then serve it to your end users for analytics and reports. Unlike a traditional data warehouse, you can avoid siloing your business analytics data or creating redundant copies that quickly become stale.\\nBuilding a data warehouse inside your lakehouse lets you bring all your data into a single system and lets you take advantage of features such as Unity Catalog and Delta Lake.\\nUnity Catalog adds a unified governance model so that you can secure and audit data access and provide lineage information on downstream tables. Delta Lake adds ACID transactions and schema evolution, among other powerful tools for keeping your data reliable, scalable, and high-quality.\\n\\n\\nWhat is Databricks SQL? \\nDatabricks SQL is the collection of services that bring data warehousing capabilities and performance to your existing data lakes. Databricks SQL supports open formats and standard ANSI SQL. An in-platform SQL editor and dashboarding tools allow team members to collaborate with other Databricks users directly in the workspace. Databricks SQL also integrates with a variety of tools so that analysts can author queries and dashboards in their favorite environments without adjusting to a new platform.\\nDatabricks SQL provides general compute resources that are executed against the tables in the lakehouse. Databricks SQL is powered by SQL warehouses, offering scalable SQL compute resources decoupled from storage.\\nSee What are SQL Warehouses? for more information on SQL Warehouse defaults and options.\\nDatabricks SQL integrates with Unity Catalog so that you can discover, audit, and govern data assets from one place. To learn more, see What is Unity Catalog?\\n\\n\\nData modeling on Databricks \\nA lakehouse supports a variety of modeling styles. The following image shows how data is curated and modeled as it moves through different layers of a lakehouse.\\n\\n\\n\\n\\nMedallion architecture \\nThe medallion architecture is a data design pattern that describes a series of incrementally refined data layers that provide a basic structure in the lakehouse. The bronze, silver, and gold layers signify increasing data quality at each level, with gold representing the highest quality. For more information, see What is the medallion lakehouse architecture?.\\nInside a lakehouse, each layer can contain one or more tables. The data warehouse is modeled at the silver layer and feeds specialized data marts in the gold layer.\\n\\n\\nBronze layer \\nData can enter your lakehouse in any format and through any combination of batch or steaming transactions. The bronze layer provides the landing space for all of your raw data in its original format. That data is converted to Delta tables.\\n\\n\\nSilver layer \\nThe silver layer brings the data from different sources together. For the part of the business that focuses on data science and machine learning applications, this is where you start to curate meaningful data assets. This process is often marked by a focus on speed and agility.\\nThe silver layer is also where you can carefully integrate data from disparate sources to build a data warehouse in alignment with your existing business processes. Often, this data follows a Third Normal Form (3NF) or Data Vault model. Specifying primary and foreign key constraints allows end users to understand table relationships when using Unity Catalog.  Your data warehouse should serve as the single source of truth for your data marts.\\nThe data warehouse itself is schema-on-write and atomic. It is optimized for change, so you can quickly modify the data warehouse to match your current needs when your business processes change or evolve.\\n\\n\\nGold layer \\nThe gold layer is the presentation layer, which can contain one or more data marts. Frequently, data marts are dimensional models in the form of a set of related tables that capture a specific business perspective.\\nThe gold layer also houses departmental and data science sandboxes to enable self-service analytics and data science across the enterprise. Providing these sandboxes and their own separate compute clusters prevents the Business teams from creating copies of data outside the lakehouse.\\n\\n\\n\\nNext step \\nTo learn more about the principles and best practices for implementing and operating a lakehouse using Databricks, see Data lakehouse architecture: Databricks well-architected framework.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Delta Lake? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDelta tutorial\\nMerge data\\nSelective overwrite\\nDrop or replace\\nHistory and data retention\\nVacuum\\nLiquid clustering\\nData skipping\\nOptimize\\nChange data feed\\nColumn mapping\\nTable constraints\\nView table details\\nUser-defined metadata\\nGenerated columns\\nIdempotent writes\\nDeletion vectors\\nSchema validation\\nUpdate schema\\nTune file size\\nBest practices\\nTable properties reference\\nManage feature compatibility\\nDrop table features\\nGenerate manifest file\\nPartitioning tables\\nDeep and shallow clone\\nClone with Unity Catalog\\nClone Parquet and Iceberg tables\\nConvert to Delta\\nUniversal Format\\n\\n\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Delta Lake?\\n\\n\\n\\n\\n\\n\\n\\nWhat is Delta Lake? \\nDelta Lake is the optimized storage layer that provides the foundation for storing data and tables in the Databricks lakehouse. Delta Lake is open source software that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.\\nDelta Lake is the default storage format for all operations on Databricks. Unless otherwise specified, all tables on Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project. Many of the optimizations and products in the Databricks platform build upon the guarantees provided by Apache Spark and Delta Lake. For information on optimizations on Databricks, see Optimization recommendations on Databricks.\\nFor reference information on Delta Lake SQL commands, see Delta Lake statements.\\nThe Delta Lake transaction log has a well-defined open protocol that can be used by any system to read the log. See Delta Transaction Log Protocol.\\n\\n\\n\\nGetting started with Delta Lake \\nAll tables on Databricks are Delta tables by default. Whether you’re using Apache Spark DataFrames or SQL, you get all the benefits of Delta Lake just by saving your data to the lakehouse with default settings.\\nFor examples of basic Delta Lake operations such as creating tables, reading, writing, and updating data, see Tutorial: Delta Lake.\\nDatabricks has many recommendations for best practices for Delta Lake.\\n\\n\\nConverting and ingesting data to Delta Lake \\nDatabricks provides a number of products to accelerate and simplify loading data to your lakehouse.\\n\\nDelta Live Tables:\\n\\nTutorial: Run your first ETL workload on Databricks\\nLoad data using streaming tables (Python/SQL notebook)\\n\\n\\nLoad data using streaming tables in Databricks SQL\\n\\n\\nCOPY INTO\\nAuto Loader\\nAdd data UI\\nIncrementally convert Parquet or Iceberg data to Delta Lake\\nOne-time conversion of Parquet or Iceberg data to Delta Lake\\nThird-party partners\\n\\nFor a full list of ingestion options, see Load data into a Databricks lakehouse.\\n\\n\\nUpdating and modifying Delta Lake tables \\nAtomic transactions with Delta Lake provide many options for updating data and metadata. Databricks recommends you avoid interacting directly with data and transaction log files in Delta Lake file directories to avoid corrupting your tables.\\n\\nDelta Lake supports upserts using the merge operation.\\nDelta Lake provides numerous options for selective overwrites based on filters and partitions.\\nYou can manually or automatically update your table schema without rewriting data.\\nColumn mapping enables columns to be renamed or deleted without rewriting data.\\n\\n\\n\\nIncremental and streaming workloads on Delta Lake \\nDelta Lake is optimized for Structured Streaming on Databricks. Delta Live Tables extends native capabilities with simplified infrastructure deployment, enhanced scaling, and managed data dependencies.\\n\\nDelta table streaming reads and writes\\nUse Delta Lake change data feed on Databricks\\nEnable idempotent writes across jobs\\n\\n\\n\\nQuerying previous versions of a table \\nEach write to a Delta table creates a new table version. You can use the transaction log to review modifications to your table and query previous table versions. See Work with Delta Lake table history.\\n\\n\\nDelta Lake schema enhancements \\nDelta Lake validates schema on write, ensuring that all data written to a table matches the requirements you’ve set.\\n\\nDelta Lake schema validation\\nConstraints on Databricks\\nUse Delta Lake generated columns\\nEnrich Delta Lake tables with custom metadata\\n\\n\\n\\nManaging files and indexing data with Delta Lake \\nDatabricks sets many default parameters for Delta Lake that impact the size of data files and number of table versions that are retained in history. Delta Lake uses a combination of metadata parsing and physical data layout to reduce the number of files scanned to fulfill any query.\\n\\nUse liquid clustering for Delta tables\\nData skipping for Delta Lake\\nCompact data files with optimize on Delta Lake\\nRemove unused data files with vacuum\\nConfigure Delta Lake to control data file size\\n\\n\\n\\nConfiguring and reviewing Delta Lake settings \\nDatabricks stores all data and metadata for Delta Lake tables in cloud object storage. Many configurations can be set at either the table level or within the Spark session. You can review the details of the Delta table to discover what options are configured.\\n\\nReview Delta Lake table details with describe detail\\nDelta table properties reference\\n\\n\\n\\nData pipelines using Delta Lake and Delta Live Tables \\nDatabricks encourages users to leverage a medallion architecture to process data through a series of tables as data is cleaned and enriched. Delta Live Tables simplifies ETL workloads through optimized execution and automated infrastructure deployment and scaling.\\n\\n\\nTroubleshooting Delta Lake features \\nNot all Delta Lake features are in all versions of Databricks Runtime. You can find information about Delta Lake versioning and answers to frequent questions in the following articles:\\n\\nHow does Databricks manage Delta Lake feature compatibility?\\nGenerate a manifest file\\n\\n\\n\\nDelta Lake API documentation \\nFor most read and write operations on Delta tables, you can use Spark SQL or Apache Spark DataFrame APIs.\\nFor Delta Lake-spefic SQL statements, see Delta Lake statements.\\nDatabricks ensures binary compatibility with Delta Lake APIs in Databricks Runtime. To view the Delta Lake API version packaged in each Databricks Runtime version, see the System environment section on the relevant article in the Databricks Runtime release notes. Delta Lake APIs exist for Python, Scala, and Java:\\n\\nPython API docs\\nScala API docs\\nJava API docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDeveloper tools and guidance | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nAuthentication\\nIDEs\\nSDKs\\nSQL connectors/drivers\\nCLIs\\nUtilities\\nREST API reference\\nIaC\\nCI/CD\\nSQL tools\\nService principals\\n\\n\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDeveloper tools and guidance\\n\\n\\n\\n\\n\\n\\n\\nDeveloper tools and guidance \\nLearn about tools and guidance you can use to work with Databricks resources and data and to develop Databricks applications.\\n\\n\\n\\n\\n\\n\\nSection\\nUse this section when you want to…\\n\\n\\n\\nAuthentication\\nAuthenticate with Databricks from your tools, scripts, and apps. You must authenticate with Databricks\\nbefore you can work with Databricks resources and data.\\n\\nIDEs\\nConnect to Databricks by using popular integrated development environments (IDEs) such as\\nVisual Studio Code, PyCharm, IntelliJ IDEA, Eclipse, and RStudio, as well as automate Databricks by using IDE\\nplugins.\\n\\nSDKs\\nAutomate Databricks from code libraries written for popular languages such as Python, Java, Go, and R.\\n\\nSQL connectors/drivers\\nRun SQL commands on Databricks from code written in popular languages such as Python, Go, JavaScript, and TypeScript.\\nConnect tools and clients to Databricks through ODBC and JDBC connections.\\n\\nCLIs\\nAutomate Databricks by using the Databricks command-line interface (CLI).\\nQuery data warehouses from the command line by using the Databricks SQL CLI.\\n\\nUtilities\\nUse Databricks Utilities from within notebooks to do things such as work with object storage efficiently,\\nchain and parameterize notebooks, and work with sensitive credential information.\\n\\nREST API Reference\\nLook up reference information for the Databricks REST APIs.\\n\\nIaC\\nAutomate the provision and maintenance of Databricks infrastructure and resources by using popular\\ninfrastructure-as-code (IaC) products such as Terraform, the Cloud Development Kit for Terraform, and Pulumi.\\n\\nCI/CD\\nImplement industry-standard continuous integration and continuous delivery (CI/CD) practices for Databricks\\nby using Databricks Asset Bundles, and popular systems and frameworks such as GitHub Actions, DevOps pipelines,\\nJenkins, and Apache Airflow.\\n\\nSQL tools\\nRun SQL commands and scripts in Databricks by using the Databricks SQL CLI, the Databricks Driver for SQLTools, and\\npopular tools such as DataGrip, DBeaver, and SQL Workbench/J.\\n\\nService principals\\nDatabricks recommends that you use service principals instead of users to authenticate automated scripts, tools,\\napps, and systems with Databricks workspaces and resources.\\n\\n\\n\\n\\n\\n\\nTip\\nYou can also connect many additional popular third-party tools to clusters and SQL warehouses to access data in Databricks. See the Technology partners.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/dev-tools/index.html', 'title': 'Developer tools and guidance | Databricks on AWS', 'description': 'Learn how to use Databricks developer tools and follow Databricks developer guidance.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTechnology partners | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\nDatabricks ODBC and JDBC Drivers\\nGet compute resource connection details\\nDatabricks sign-on from partner solutions\\nPartner Connect\\nIngestion\\nData preparation and transformation\\nMachine Learning\\nBI and visualization\\nReverse ETL\\nSemantic layer\\nData governance\\nData security\\n\\n\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nTechnology partners\\n\\n\\n\\n\\n\\n\\n\\nTechnology partners \\nDatabricks has validated integrations with various third-party solutions that allow you to work with data through Databricks clusters and SQL warehouses, in many cases with low-code and no-code experiences. These solutions enable common scenarios such as data ingestion, data preparation and transformation, business intelligence (BI), and machine learning.\\nDatabricks also includes Partner Connect, a user interface that allows some of these validated solutions to integrate more quickly and easily with your Databricks clusters and SQL warehouses.\\n\\n\\nBuild an integration with Databricks \\nThis section provides instructions and best practices for technology partners to build and maintain their integrations with Databricks.\\n\\nBest practices for ingestion partners using Unity Catalog volumes as staging locations for data\\n\\n\\n\\n\\nAll Databricks technology partners \\nFor a list of all Databricks partner solutions, see Databricks Technology Partners. Some of these partner solutions are featured in Databricks Partner Connect.\\n\\n\\n\\nDatabricks Partner Connect partners \\n\\nThis section lists the partner solutions that are featured in Partner Connect.\\n\\nData ingestion \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Fivetran using Partner Connect\\n\\n\\nNo\\nConnect to Hevo Data using Partner Connect\\n\\n\\nNo\\nConnect to Rivery using Partner Connect\\n\\n\\nYes\\nConnect to RudderStack using Partner Connect\\n\\n\\nYes\\nConnect to Snowplow using Partner Connect\\n\\n\\n\\n\\n\\nData preparation and transformation \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to dbt Cloud using Partner Connect\\n\\n\\nYes\\nConnect to Matillion using Partner Connect\\n\\n\\nN/A\\nConnect to Prophecy using Partner Connect\\n\\n\\n\\n\\n\\nMachine learning \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Dataiku using Partner Connect\\n\\n\\nN/A\\nConnect to John Snow Labs using Partner Connect\\n\\n\\nN/A\\nConnect to Labelbox using Partner Connect\\n\\n\\n\\n\\n\\nBI and visualization \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Hex using Partner Connect\\n\\n\\nYes\\nConnect Power BI Desktop to Databricks using Partner Connect\\n\\n\\nNo\\nConnect to Preset using Partner Connect\\n\\n\\n\\nPartner Connect: No\\nManual connection: Yes\\n\\n\\nConnect to Qlik Sense using Partner Connect\\n\\n\\nYes\\nConnect to Sigma using Partner Connect\\n\\n\\nYes\\nConnect to Tableau Desktop using Partner Connect\\n\\n\\nYes\\nConnect to ThoughtSpot using Partner Connect\\n\\n\\n\\n\\n\\nReverse ETL \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Census using Partner Connect\\n\\n\\n\\nPartner Connect: No\\nManual connection: Yes\\n\\n\\nConnect to Hightouch using Partner Connect\\n\\n\\n\\n\\nSecurity \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Hunters using Partner Connect\\n\\n\\nYes\\nConnect to Privacera using Partner Connect\\n\\n\\n\\n\\n\\n\\nData governance \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Alation using Partner Connect\\n\\n\\nYes\\nConnect to Anomalo using Partner Connect\\n\\n\\nNo\\nConnect to erwin Data Modeler using Partner Connect\\n\\n\\n\\nPartner Connect: No\\nManual connection: Yes\\n\\n\\nConnect to Lightup using Partner Connect\\n\\n\\n\\nPartner Connect: Yes\\nManual connection: Yes\\n\\n\\nConnect to Monte Carlo using Partner Connect\\n\\n\\n\\n\\n\\nSemantic layer \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to AtScale using Partner Connect\\n\\n\\nYes\\nConnect to Stardog using Partner Connect\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Databricks lakehouse data.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks administration introduction | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nGet started with Databricks administration\\nAccount administration\\nWorkspace deployment\\nManage a workspace\\nIdentity management\\nCompute policies\\nAudit logs\\nSystem tables\\n\\n\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks administration introduction\\n\\n\\n\\n\\n\\n\\n\\nDatabricks administration introduction \\n\\n\\nThis article provides an introduction to Databricks administrator privileges and responsibilities.\\n\\nNote\\nTo fully administer your Databricks instance, you will also need administrative access to your AWS account.\\n\\n\\nDatabricks admin types \\nThere are two main levels of admin privileges available on the Databricks platform:\\n\\nAccount admins: Manage the Databricks account, including workspace creation, user management, cloud resources, and account usage monitoring.\\n\\n\\nWorkspace admins: Manage workspace identities, access control, settings, and features for individual workspaces in the account.\\n\\nAdditionally, users can be assigned these feature-specific admin roles, which have narrower sets of privileges:\\n\\nMarketplace admins: Manage their account’s Databricks Marketplace provider profile, including creating and managing Marketplace listings.\\nMetastore admins: Manage privileges and ownership for all securable objects within a Unity Catalog metastore, such as who can create catalogs or query a table.\\n\\n\\n\\nWhat are account admins? \\nAccount admins have privileges over the entire Databricks account. As an account admin, you can create workspaces, configure cloud resources, view usage data, and manage account identities, settings, and subscriptions.\\nAccount admins can also delegate the account admin and workspace admin roles to any other user.\\n\\nAccess the account console \\nThe account console is where account admins manage their Databricks account.\\n\\n\\n\\nAccount admins can access the account console at https://accounts.cloud.databricks.com or by clicking their email address at the top of the workspace UI and selecting Manage Account.\\n\\n\\n\\nAccount admin responsibilities \\nAs an account admin, your responsibilities include:\\n\\nCreating and managing workspaces\\nEnabling Unity Catalog\\nManaging identities\\nMonitoring account usage logs\\nManaging the account subscription\\n\\n\\nCreate and manage workspaces \\nOnly account admins can create new workspaces. There are a few different methods you can use to create workspaces. See Create and manage workspaces for instructions on each method. You can also use the Workspaces section of the account console to view and manage all the workspaces in your account.\\n\\n\\nEnable Unity Catalog \\n\\nNote\\nIf your Databricks account was created after November 8, 2023, your workspaces might have Unity Catalog enabled by default. For more information, see Automatic enablement of Unity Catalog.\\n\\nAn account admin is needed to enable Unity Catalog in your account. The process involves creating a Unity Catalog metastore, which can only be done by an account admin.\\nFor instructions on enabling Unity Catalog, see Get started using Unity Catalog.\\n\\n\\nManage identities \\nAccount admins should sync their identity provider with Databricks if applicable. See Sync users and groups from your identity provider.\\nIf you’ve enabled Unity Catalog for at least one workspace in your account, identities (users, groups, and service principals) should be managed in the account console. Account admins can grant permissions and assign workspaces to these identities.\\nFor more information, see Manage users and groups.\\nSingle sign-on (SSO) enables you to authenticate your users using your organization’s identity provider. Databricks recommends configuring SSO in your account for greater security and improved user experience. Once SSO is configured, you can enable multi-factor authentication via your identity provider. For instructions, see Setting up SSO.\\n\\n\\nMonitor account usage logs \\nOnly account admins can configure audit logs for their account. For information on audit logs, see Audit log reference.\\nAccount admins can also view and download billable usage logs from the account console. For more information, see View billable usage using the account console.\\n\\n\\nManage account subscription \\nAccount admins can manage aspects of their Databricks subscription from the account console. For more information, see Manage subscription and billing.\\n\\n\\n\\nWhat are workspace admins? \\nWorkspace admins have admin privileges within a single workspace. They can manage workspace-level identities, regulate compute use, and enable and delegate role-based access control (Premium plan or above only).\\n\\nAccess the admin settings \\nWorkspace admins are the only users who have access to the workspace’s admin settings page. As a workspace admin, you can access admin settings by clicking your username in the top bar of the Databricks workspace and selecting Admin Settings.\\n\\n\\n\\n\\n\\n\\nWorkspace admin responsibilities \\nAs a workspace admin, your responsibilities include:\\n\\nManaging identities in your workspace\\nCreating and managing compute resources\\nManaging workspace features and settings\\n\\n\\nManage identities in your workspace \\nIf your workspace is enabled for Unity Catalog, identities should be added at the account level. Workspace admins can then assign users, groups, and service principals to their workspace. For more information on adding and removing identities in a workspace, see Manage users, service principals, and groups.\\n\\nNote\\nDatabricks Academy has a free course on Identity Administration. Before you can access the course, you first need to register for Databricks Academy if you haven’t already.\\n\\n\\n\\nCreate and manage compute resources \\nWorkspace admins can create SQL warehouses (a compute resource that lets you run SQL commands on data objects within Databricks SQL) and clusters for their workspace users. For instructions on creating SQL warehouses, see Create a SQL warehouse.\\nIt is also the workspace admin’s job to regulate how compute resources are used in their workspace. Workspace admins have the following tools:\\n\\nLimit workspace users’ cluster creation options with cluster policies.\\n\\nDatabricks recommends managing all init scripts as cluster-scoped init scripts. Instead of using global init scripts, manage init scipts using cluster policies.\\n\\n\\nLearn which compute resources have Unity Catalog access.\\n\\n\\nGrant S3 bucket access through clusters using instance profiles.\\n\\n\\nNote\\nDatabricks Academy has a free course on Compute Resources Administration.\\n\\n\\n\\nManage workspaces features and settings \\nWorkspace admins are responsible for managing select workspace behavior and settings. For information on other available workspace settings, see Managing workspace settings.\\n\\nNote\\nDatabricks Academy has a free course on Databricks Workspace Administration and Security.\\n\\n\\n\\n\\nAdditional resources \\nDatabricks Academy has a free self-paced learning path for platform administrators. Before you can access the course, you first need to register for Databricks Academy if you haven’t already.\\nYou can also sign up to attend a live platform administration training.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSecurity and compliance guide | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nAuthentication and access control\\nNetwork connectivity\\nData security and encryption\\nSecret management\\nAuditing, privacy, and compliance\\n\\n\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nSecurity and compliance guide\\n\\n\\n\\n\\n\\n\\n\\nSecurity and compliance guide \\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their Databricks environment according to their risk profile and governance policy.\\nThis guide does not cover information about securing your data. For that information, see Data governance guide.\\n\\nNote\\nThis article focuses on the most recent (E2) version of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.\\n\\n\\n\\n\\nAuthentication and access control \\nIn Databricks, a workspace is a Databricks deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their Databricks assets. Your organization can choose to have multiple workspaces or just one, depending on your needs. A Databricks account represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and Unity Catalog metastores.\\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage Databricks users, service principals, and groups, as well as authentication settings and access control.\\nDatabricks provides security features, such as single sign-on, to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\\nAccess control lists determine who can view and perform operations on objects in Databricks workspaces, such as notebooks and SQL warehouses.\\nTo learn more about authentication and access control in Databricks, see Authentication and access control.\\n\\n\\nNetwork access \\nDatabricks provides network protections that enable you to secure Databricks workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of Databricks users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see Network connectivity.\\n\\n\\nData security and encryption \\nSecurity-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. Databricks has an extremely strong security program which manages the risk of such an incident. See the Security and Trust Center for an overview on the program. That said, no company can completely eliminate all risk, and Databricks provides encryption features for additional control of your data. See Data security and encryption.\\n\\n\\nSecret management \\nSometimes accessing data requires that you authenticate to external data sources. Databricks recommends that you  use Databricks secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see Secret management.\\n\\n\\nAuditing, privacy, and compliance \\nDatabricks provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\\nDatabricks also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\\nFor more information, see Auditing, privacy, and compliance.\\n\\nSecurity Analysis Tool \\n\\nExperimental\\nThe Security Analysis Tool (SAT) is a productivity tool in an Experimental state. It’s not meant to be used as a certification of your deployments. The SAT project is regularly updated to improve correctness of checks, add new checks, and fix bugs.\\n\\nYou can use the Security Analysis Tool (SAT) to analyze your Databricks account and workspace security configurations. SAT provides recommendations that help you follow Databricks security best practices. SAT is typically run daily as an automated workflow. The details of these check results are persisted in Delta tables in your storage so that trends can be analyzed over time. These results are displayed in a centralized Databricks dashboard.\\nFor more information, see the Security Analysis Tool GitHub repo.\\n\\n\\n\\n\\n\\n\\nLearn more \\nHere are some resources to help you build a comprehensive security solution that meets your organization’s needs:\\n\\nThe Databricks Security and Trust Center, which provides information about the ways in which security is built into every layer of the Databricks platform.\\nSecurity Best Practices, which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nData governance guide | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nUnity Catalog\\nWhat is Catalog Explorer?\\nHive metastore table access control (legacy)\\n\\n\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nData governance guide\\n\\n\\n\\n\\n\\n\\n\\nData governance guide \\nThis guide shows how to manage data and data access in Databricks. For information on Databricks security, see the Security and compliance guide. Databricks provides centralized governance for data and AI with Unity Catalog and Delta Sharing.\\n\\n\\n\\nCentralize access control using Unity Catalog \\nUnity Catalog is a fine-grained governance solution for data and AI on the Databricks platform. It helps simplify security and governance of your data and AI assets by providing a central place to administer and audit access to data and AI assets.\\nFor best practices on adopting Unity Catalog, see Unity Catalog best practices.\\n\\nTrack data lineage using Unity Catalog \\nYou can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows, and dashboards related to the query. To learn more, see Capture and view data lineage with Unity Catalog.\\n\\n\\n\\nDiscover data using Catalog Explorer \\nDatabricks Catalog Explorer provides a UI to explore and manage data and AI assets, including schemas (databases), tables, volumes (non-tabular data), and registered ML models, along with asset permissions, data owners, external locations, and credentials. You can use the Insights tab in Catalog Explorer to view the most frequent recent queries and users of any table registered in Unity Catalog.\\n\\n\\nShare data using Delta Sharing \\nDelta Sharing is an open protocol developed by Databricks for secure data and AI asset sharing with other organizations, or with other teams within your organization, regardless of which computing platforms they use.\\n\\n\\nConfigure audit logging \\nDatabricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns.\\nUnity Catalog lets you easily access and query your account’s operational data, including audit logs, billable usage, and lineage using system tables (Public Preview).\\n\\n\\nConfigure identity \\nEvery good data governance story starts with a strong identity foundation. To learn how to best configure identity in Databricks, see Identity best practices.\\n\\n\\nLegacy data governance solutions \\n\\nTable access control is a legacy data governance model that lets you programmatically grant and revoke access to objects managed by your workspace’s built-in Hive metastore. Databricks recommends that you use Unity Catalog instead of table access control. Unity Catalog simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account.\\n\\n\\nIAM role credential passthrough is also a legacy data governance feature that allows users to authenticate automatically to S3 buckets from Databricks clusters using the identity that they use to log in to Databricks. Databricks recommends that you use Unity Catalog instead.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/data-governance/index.html', 'title': 'Data governance guide | Databricks on AWS', 'description': 'Learn about data governance in Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nData lakehouse architecture: Databricks well-architected framework | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\nData governance\\nInteroperability & usability\\nOperational excellence\\nSecurity, compliance & privacy\\nReliability\\nPerformance efficiency\\nCost optimization\\n\\n\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nData lakehouse architecture: Databricks well-architected framework\\n\\n\\n\\n\\n\\n\\n\\nData lakehouse architecture: Databricks well-architected framework \\nThis set of data lakehouse architecture articles provides principles and best practices for the implementation and operation of a lakehouse using Databricks.\\n\\nDatabricks well-architected framework for the lakehouse \\n\\n\\n\\nThe well-architected lakehouse consists of 7 pillars which describe different areas of concern for the implementation of a data lakehouse in the cloud:\\n\\nData governance\\nThe oversight to ensure that data brings value and supports your business strategy.\\n\\nInteroperability and usability\\nThe ability of the lakehouse to interact with users and other systems.\\n\\nOperational excellence\\nAll operations processes that keep the lakehouse running in production.\\n\\nSecurity, privacy, compliance\\nProtect the Databricks application, customer workloads and customer data from threats.\\n\\nReliability\\nThe ability of a system to recover from failures and continue to function.\\n\\nPerformance efficiency\\nThe ability of a system to adapt to changes in load.\\n\\nCost optimization\\nManaging costs to maximize the value delivered.\\n\\n\\nThe well-architected lakehouse extends the AWS Well-Architected Framework to the Databricks platform and shares the pillars “Operational Excellence”, “Security” (as “Security, privacy, compliance”), “Reliability”, “Performance Efficiency” and “Cost Optimization”.\\nFor these five pillars, the principles and best practices of the cloud framework still apply to the lakehouse. The well-architected lakehouse extends these with principles and best practices that are specific for the lakehouse and important to build an effective and efficient lakehouse.\\n\\n\\nData Governance and Interoperability & Usability in lakehouse architectures \\nThe pillars “Data Governance” and “Interoperability and Usability” cover concerns specific for the lakehouse.\\nData governance encapsulates the policies and practices implemented to securely manage the data assets within an organization. One of the fundamental aspects of a lakehouse is centralized data governance: The lakehouse unifies data warehousing and AI uses cases on a single platform. This simplifies the modern data stack by eliminating the data silos that traditionally separate and complicate data engineering, analytics, BI, data science, and machine learning. To simplify data governance, the lakehouse offers a unified governance solution for data, analytics and AI. By minimizing the copies of your data and moving to a single data processing layer where all your data governance controls can run together, you improve your chances of staying in compliance and detecting a data breach.\\nAnother important tenet of the lakehouse is to provide a great user experience for all the personas that work with it, and to be able to interact with a wide ecosystem of external systems. AWS already has a variety of data tools that perform most tasks a data-driven enterprise might need. However, these tools must be properly assembled to provide all the functionality, with each service offering a different user experience. This approach can lead to high implementation costs and typically does not provide the same user experience as a native lakehouse platform: Users are limited by inconsistencies between tools and a lack of collaboration capabilities, and often have to go through complex processes to gain access to the system and thus to the data.\\nAn integrated lakehouse on the other side provides a consistent user experience across all workloads and therefore increases usability. This reduces training and onboarding costs and improves collaboration between functions. In addition, new features are automatically added over time - to further improve the user experience - without the need to invest internal resources and budgets.\\nA multicloud approach can be a deliberate strategy of a company or the result of mergers and acquisitions or independent business units selecting different cloud providers. In this case, using a multicloud lakehouse results in a unified user experience across all clouds. This reduces the proliferation of systems across the enterprise, which in turn reduces the skill and training requirements of employees involved in data-driven tasks.\\nFinally, in a networked world with cross-company business processes, systems must work together as seamlessly as possible. The degree of interoperability is a crucial criterion here, and the most recent data, as a core asset of any business, must flow securely between internal and external partners systems.\\n\\n\\nPrinciples and best practices \\n\\n\\nData governance\\nInteroperability & usability\\nOperational excellence\\nSecurity, compliance & privacy\\nReliability\\nPerformance efficiency\\nCost optimization\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks reference documentation | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nPython, SparkR & Scala intros\\nREST API reference\\nMLFlow API\\nFeature Store Python API\\nApache Spark API\\nDelta Lake API\\nDelta Live Tables API\\nSQL language reference\\n\\n\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks reference documentation\\n\\n\\n\\n\\n\\n\\n\\nDatabricks reference documentation \\nThis article contains links to Databricks API reference documentation and guidance.\\n\\n\\n\\n\\nCommon error codes in Databricks \\n\\nError messages in Databricks\\n\\n\\n\\nPython, SparkR, and Scala on Databricks \\n\\nDatabricks for Python developers\\nDatabricks for R developers\\nDatabricks for Scala developers\\n\\n\\n\\nAPI reference documentation \\nDatabricks provides the following API reference documentation:\\n\\nREST API reference\\nMLflow API\\nFeature Store Python API\\nApache Spark API\\nDelta Lake API\\nDelta Live Tables API\\n\\n\\n\\nSQL language reference documentation \\n\\nSQL language reference\\nDelta Live Tables SQL language reference\\n\\n\\n\\nCLI reference documentation \\n\\nDatabricks CLI\\nDatabricks SQL CLI\\n\\n\\n\\nSDK reference documentation \\n\\nDatabricks SDK for Python\\nDatabricks SDK for R\\nDatabricks SDK for Java\\nDatabricks SDK for Go\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/reference/api.html', 'title': 'Databricks reference documentation | Databricks on AWS', 'description': 'Reference documentation for Databricks APIs, SQL language, command-line interfaces, and more. Databricks reference docs cover tasks from automation to data queries.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResources | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nStatus Page\\nLimits\\nDatabricks clouds and regions\\nSupported browsers\\nConfigure domain name firewall rules\\nPlatform release process\\nSubmit product feedback\\nSupport\\nError messages\\n\\n\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nResources\\n\\n\\n\\n\\n\\n\\n\\nResources \\nThis section provides information on limits, the Databricks release process, support plans, how to give product feedback, and how to monitor system status.\\n\\n\\nStatus Page\\nLimits\\nDatabricks clouds and regions\\nSupported browsers\\nConfigure domain name firewall rules\\nPlatform release process\\nSubmit product feedback\\nSupport\\nError messages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/resources/index.html', 'title': 'Resources | Databricks on AWS', 'description': 'Learn how to submit support tickets, manage your support contract, submit product feedback, and monitor Databricks system status.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat’s coming? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat’s coming?\\n\\n\\n\\n\\n\\n\\n\\nWhat’s coming? \\nLearn about upcoming Databricks releases.\\n\\nLegacy Git integration is EOL on January 31 \\nAfter January 31st, 2024, Databricks will remove legacy notebook Git integrations. This feature has been in legacy status for more than two years, and a deprecation notice has been displayed in the product UI since November 2023.\\n\\n\\nChanges to query, dashboard, and alert listing pages \\nDatabricks plans to remove the Admin view tab from listing pages for queries, dashboards, and alerts. Workspace admins automatically have “Can Manage” permissions on workspace objects, so all queries, dashboards, and alerts can be accessed from the All tab on each listing page.\\n\\n\\nExternal support ticket submission will be deprecated \\nDatabricks plans to transition the support ticket submission experience from help.databricks.com to the help menu in the Databricks workspace. Support ticket submission via help.databricks.com will be deprecated. You’ll still view and triage your tickets at help.databricks.com.\\nThe in-product experience, which is available if your organization has a Databricks Support contract, integrates with Databricks Assistant to help address your issues quickly without having to submit a ticket.\\nTo access the in-product experience, click the help icon , and then click Create Support Ticket or type “I need help” into the assistant.\\nThe Contact support modal opens.\\n\\n\\n\\nIf the in-product experience is down, send requests for support with detailed information about your issue to help@databricks.com. For more information, see Get help.\\n\\n\\nGoogle Data Studio (Looker Studio) integration \\nUsage of the Databricks Connector for Data Studio and the transfer of information received from Google APIs to any other app will adhere to the Google API Services User Data Policy, including the Limited Use requirements.\\n\\n\\nJDK8 and JDK11 will be unsupported \\nDatabricks plans to remove JDK 8 support with the next major Databricks Runtime version, when Spark 4.0 releases. Databricks plans to remove JDK 11 support with the next LTS version of Databricks Runtime 14.x.\\n\\n\\nAutomatic enablement of Unity Catalog for new workspaces \\nDatabricks will soon start to enable Unity Catalog automatically for new workspaces. This removes the need for account admins to configure Unity Catalog after a workspace is created. Rollout will proceed gradually across accounts.\\n\\n\\nNew charts and chart improvements \\nDatabricks plans to add new charts to the SQL editor, SQL dashboards, and notebooks. This change will bring faster chart rendering performance, improved colors, and faster interactivity. See New chart visualizations in Databricks\\n\\n\\nFavorites functionality \\nDatabricks plans to enable favorites functionality in the workspace. You’ll be able to save content such as notebooks, dashboards, experiments, and queries to a list of favorites, and then access your favorites from the homepage.\\n\\n\\nsqlite-jdbc upgrade \\nDatabricks Runtime plans to upgrade the sqlite-jdbc version from 3.8.11.2 to 3.42.0.0 in all Databricks Runtime maintenance releases. The APIs of version 3.42.0.0 are not fully compatible with 3.8.11.2. Confirm your methods and return type use version 3.42.0.0.\\nIf you are using sqlite-jdbc in your code, check the sqlite-jdbc compatibility report.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/whats-coming.html', 'title': 'What’s coming? | Databricks on AWS', 'description': 'Learn about upcoming Databricks releases.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks documentation archive | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\nAdministration guide (legacy)\\nCreate cluster UI (legacy)\\nCluster UI preview\\nInstall a library with an init script\\n(Legacy) Cluster-named init scripts\\n(Legacy) Global init scripts\\nCompute policies best practices\\n(Legacy) Manage libraries with %conda commands\\nExplore and create tables in DBFS\\nTransactional writes to cloud storage with DBIO\\nKoalas\\nDatabricks CLI (legacy)\\nWhat is dbx by Databricks Labs?\\ndbutils.library\\nMigrate to Spark 3.x\\nVScode with Repos\\nExternal metastores (legacy)\\nCredential passthrough (legacy)\\nShare feature tables across workspaces (legacy)\\nGenomics guide\\nMLeap ML model export\\nTrain a PySpark model and save in MLeap format\\nDeploy MLeap model on SageMaker\\nModel serving (legacy)\\nServerless Real-Time Inference (preview)\\nDatabricks light\\nDatabricks runtime release notes (unsupported)\\nSpark SQL 2.x reference\\nUnity Catalog GA release note\\nAudit log schemas for security monitoring\\nAmazon S3 source with Amazon SQS (legacy)\\nAzure Blob storage file source with Azure Queue Storage (legacy)\\nConnecting Databricks and Azure Synapse with PolyBase (legacy)\\nNeo4j\\nAccessing Azure Data Lake Storage Gen1 from Databricks\\nConfigure Delta storage credentials\\nConnect to Azure Blob Storage with WASB (legacy)\\n\\n\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks documentation archive\\n\\n\\n\\n\\n\\n\\n\\nDatabricks documentation archive \\n\\nImportant\\nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.\\n\\nIn this archive, you can find earlier versions of documentation for Databricks products, features, APIs, and workflows.\\n\\nAdministration \\n\\n\\nAdministration guide (legacy)\\n\\n\\n\\n\\nCompute \\n\\n\\nCreate cluster UI (legacy)\\nCluster UI preview\\nInstall a library with an init script\\n(Legacy) Cluster-named init scripts\\n(Legacy) Global init scripts\\nCompute policies best practices\\n\\n\\n\\n\\nDev tools \\n\\n\\n(Legacy) Manage libraries with %conda commands\\nExplore and create tables in DBFS\\nTransactional writes to cloud storage with DBIO\\nKoalas\\nDatabricks CLI (legacy)\\nWhat is dbx by Databricks Labs?\\ndbutils.library\\nMigrate to Spark 3.x\\nVScode with Repos\\n\\n\\n\\n\\nGovernance \\n\\n\\nExternal metastores (legacy)\\nCredential passthrough (legacy)\\n\\n\\n\\n\\nMachine learning and AI \\n\\n\\nShare feature tables across workspaces (legacy)\\nGenomics guide\\nMLeap ML model export\\nTrain a PySpark model and save in MLeap format\\nDeploy MLeap model on SageMaker\\nModel serving (legacy)\\nServerless Real-Time Inference (preview)\\n\\n\\n\\n\\nRelease notes \\n\\n\\nDatabricks light\\nDatabricks runtime release notes (unsupported)\\nSpark SQL 2.x reference\\nUnity Catalog GA release note\\n\\n\\n\\n\\nSecurity \\n\\n\\nAudit log schemas for security monitoring\\n\\n\\n\\n\\nStorage \\n\\n\\nAmazon S3 source with Amazon SQS (legacy)\\nAzure Blob storage file source with Azure Queue Storage (legacy)\\nConnecting Databricks and Azure Synapse with PolyBase (legacy)\\nNeo4j\\nAccessing Azure Data Lake Storage Gen1 from Databricks\\nConfigure Delta storage credentials\\nConnect to Azure Blob Storage with WASB (legacy)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is a data lakehouse? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\n  What is a data lakehouse?\\nWhat are ACID guarantees on Databricks?\\nWhat is the medallion lakehouse architecture?\\nWhat does it mean to build a single source of truth?\\nData discovery and collaboration in the lakehouse\\nData objects in the Databricks lakehouse\\n\\n\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations\\n\\n\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Databricks? \\nWhat is a data lakehouse?\\n\\n\\n\\n\\n\\n\\n\\nWhat is a data lakehouse? \\nA data lakehouse is a data management system that combines the benefits of data lakes and data warehouses. This article describes the lakehouse architectural pattern and what you can do with it on Databricks.\\n\\n\\n\\n\\n\\n\\nWhat is a data lakehouse used for? \\nA data lakehouse provides scalable storage and processing capabilities for modern organizations that want to avoid isolated systems for processing different workloads, like machine learning (ML) and business intelligence (BI). A data lakehouse can help establish a single source of truth, eliminate redundant costs, and ensure data freshness.\\nData lakehouses often use a data design pattern that incrementally improves, enriches, and refines data as it moves through layers of staging and transformation. Each layer of the lakehouse can include one or more layers. This pattern is frequently referred to as a medallion architecture. For more information, see What is the medallion lakehouse architecture?\\n\\n\\nHow does the Databricks lakehouse work? \\nDatabricks is built on Apache Spark. Apache Spark enables a massively scalable engine that runs on compute resources decoupled from storage. For more information, see Apache Spark on Databricks\\nThe Databricks lakehouse uses two additional key technologies:\\n\\nDelta Lake: an optimized storage layer that supports ACID transactions and schema enforcement.\\nUnity Catalog: a unified, fine-grained governance solution for data and AI.\\n\\n\\nData ingestion \\nAt the ingestion layer, batch or streaming data arrives from a variety of sources and in a variety of formats. This first logical layer provides a place for that data to land in its raw format. As you convert those files to Delta tables, you can use the schema enforcement capabilities of Delta Lake to check for missing or unexpected data. You can use Unity Catalog to register tables according to your data governance model and required data isolation boundaries. Unity Catalog allows you to track the lineage of your data as it is transformed and refined, as well as apply a unified governance model to keep sensitive data private and secure.\\n\\n\\nData processing, curation, and integration \\nOnce verified, you can start curating and refining your data. Data scientists and machine learning practitioners frequently work with data at this stage to start combining or creating new features and complete data cleansing. Once your data has been thoroughly cleansed, it can be integrated and reorganized into tables designed to meet your particular business needs.\\nA schema-on-write approach, combined with Delta schema evolution capabilities, means that you can make changes to this layer without necessarily having to rewrite the downstream logic that serves data to your end users.\\n\\n\\nData serving \\nThe final layer serves clean, enriched data to end users. The final tables should be designed to serve data for all your use cases. A unified governance model means you can track data lineage back to your single source of truth. Data layouts, optimized for different tasks, allow end users to access data for machine learning applications, data engineering, and business intelligence and reporting.\\nTo learn more about Delta Lake, see What is Delta Lake?\\nTo learn more about Unity Catalog, see What is Unity Catalog?\\n\\n\\n\\nCapabilities of a Databricks lakehouse \\nA lakehouse built on Databricks replaces the current dependency on data lakes and data warehouses for modern data companies. Some key tasks you can perform include:\\n\\nReal-time data processing: Process streaming data in real-time for immediate analysis and action.\\nData integration: Unify your data in a single system to enable collaboration and establish a single source of truth for your organization.\\nSchema evolution: Modify data schema over time to adapt to changing business needs without disrupting existing data pipelines.\\nData transformations: Using Apache Spark and Delta Lake brings speed, scalability, and reliability to your data.\\nData analysis and reporting: Run complex analytical queries with an engine optimized for data warehousing workloads.\\nMachine learning and AI: Apply advanced analytics techniques to all of your data. Use ML to enrich your data and support other workloads.\\nData versioning and lineage: Maintain version history for datasets and track lineage to ensure data provenance and traceability.\\nData governance: Use a single, unified system to control access to your data and perform audits.\\nData sharing: Facilitate collaboration by allowing the sharing of curated data sets, reports, and insights across teams.\\nOperational analytics: Monitor data quality metrics, model quality metrics, and drift by applying machine learning to lakehouse monitoring data.\\n\\n\\n\\nLakehouse vs Data Lake vs Data Warehouse \\nData warehouses have powered business intelligence (BI) decisions for about 30 years, having evolved as a set of design guidelines for systems controlling the flow of data. Enterprise data warehouses optimize queries for BI reports, but can take minutes or even hours to generate results. Designed for data that is unlikely to change with high frequency, data warehouses seek to prevent conflicts between concurrently running queries. Many data warehouses rely on proprietary formats, which often limit support for machine learning. Data warehousing on Databricks leverages the capabilities of a Databricks lakehouse and Databricks SQL. For more information, see What is data warehousing on Databricks?.\\nPowered by technological advances in data storage and driven by exponential increases in the types and volume of data, data lakes have come into widespread use over the last decade. Data lakes store and process data cheaply and efficiently. Data lakes are often defined in opposition to data warehouses: A data warehouse delivers clean, structured data for BI analytics, while a data lake permanently and cheaply stores data of any nature in any format. Many organizations use data lakes for data science and machine learning, but not for BI reporting due to its unvalidated nature.\\nThe data lakehouse combines the benefits of data lakes and data warehouses and provides:\\n\\nOpen, direct access to data stored in standard data formats.\\nIndexing protocols optimized for machine learning and data science.\\nLow query latency and high reliability for BI and advanced analytics.\\n\\nBy combining an optimized metadata layer with validated data stored in standard formats in cloud object storage, the data lakehouse allows data scientists and ML engineers to build models from the same data-driven BI reports.\\n\\n\\nNext step \\nTo learn more about the principles and best practices for implementing and operating a lakehouse using Databricks, see Data lakehouse architecture: Databricks well-architected framework\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTutorial: Query data with notebooks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nTutorial: Query data with notebooks\\n\\n\\n\\n\\n\\n\\n\\nTutorial: Query data with notebooks \\nThis tutorial walks you through using the Databricks notebooks user interface to create a cluster and a notebook, create a table from a dataset, query the table, and display the query results.\\n\\nTip\\nYou can also use the Databricks Terraform provider to create this article’s resources. See Create clusters, notebooks, and jobs with Terraform.\\n\\n\\n\\nRequirements \\n\\nYou are logged into a Databricks workspace.\\nYou have permission to create a cluster.\\n\\n\\nNote\\nIf you do not have cluster control privileges, you can still complete most of the steps below as long as you have access to a cluster.\\n\\nFrom the left sidebar on the landing page, you access fundamental workspace entities: the Workspace, Catalog, Workflows, and Compute. The workspace is the special root folder that stores your Databricks assets, such as notebooks and libraries.\\nFor guidance about how to navigate a Databricks notebook, see Databricks notebook interface and controls.\\n\\n\\nStep 1: Create a cluster \\nA cluster is a collection of Databricks computation resources. To create a cluster:\\n\\nIn the sidebar, click  Compute.\\nOn the Compute page, click Create Compute.\\nOn the New Compute page, select 12.2 LTS (Scala 2.12, Spark 3.3.2) or higher from the Databricks Runtime version dropdown.\\nClick Create Cluster.\\n\\n\\n\\nStep 2: Create a notebook \\nA notebook is a collection of cells that run computations on an Apache Spark cluster. For more information on using notebooks, see Introduction to Databricks notebooks. To create a notebook in the workspace:\\n\\nIn the sidebar, click  Workspace.\\nIn your Home  folder, click the blue  Add button > Notebook.\\nReplace the default name of your notebook with your own title and select SQL in the language drop-down. This selection determines the default language of the notebook.\\n\\n\\n\\n\\nAttach the notebook to the cluster you created. Click the cluster selector in the notebook toolbar and select your cluster from the dropdown menu. If you don’t see your cluster, click More… and select the cluster from the dropdown menu in the dialog.\\n\\n\\n\\nStep 3: Create a table \\nCreate a table using data from a sample CSV data file available in Sample datasets, a collection of datasets mounted to What is the Databricks File System (DBFS)?, a distributed file system installed on Databricks clusters. You have two options for creating the table.\\n\\nOption 1: Create a Spark table from the CSV data \\nUse this option if you want to get going quickly, and you only need standard levels of performance. Copy and paste this code snippet into a notebook cell:\\nDROP TABLE IF EXISTS diamonds;\\n\\nCREATE TABLE diamonds USING CSV OPTIONS (path \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header \"true\")\\n\\n\\n\\n\\nOption 2: Write the CSV data to Delta Lake format and create a Delta table \\nDelta Lake offers a powerful transactional storage layer that enables fast reads and other benefits. Delta Lake format consists of Parquet files plus a transaction log. Use this option to get the best performance on future operations on the table.\\n\\nRead the CSV data into a DataFrame and write out in Delta Lake format. This command uses a Python language magic command, which allows you to interleave commands in languages other than the notebook default language (SQL). Copy and paste this code snippet into a notebook cell:\\n%python\\n\\ndiamonds = (spark.read\\n  .format(\"csv\")\\n  .option(\"header\", \"true\")\\n  .option(\"inferSchema\", \"true\")\\n  .load(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\\n)\\n\\ndiamonds.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/diamonds\")\\n\\n\\n\\nCreate a Delta table at the stored location. Copy and paste this code snippet into a notebook cell:\\nDROP TABLE IF EXISTS diamonds;\\n\\nCREATE TABLE diamonds USING DELTA LOCATION \\'/mnt/delta/diamonds/\\'\\n\\n\\n\\n\\nRun cells by pressing SHIFT + ENTER. The notebook automatically attaches to the cluster you created in Step 2 and runs the command in the cell.\\n\\n\\n\\nStep 4: Query the table \\nRun a SQL statement to query the table for the average diamond price by color.\\n\\nTo add a cell to the notebook, mouse over the cell bottom and click the  icon.\\n\\n\\n\\n\\nCopy this snippet and paste it in the cell.\\nSELECT color, avg(price) AS price FROM diamonds GROUP BY color ORDER BY COLOR\\n\\n\\n\\nPress SHIFT + ENTER. The notebook displays a table of diamond color and average price.\\n\\n\\n\\n\\n\\n\\n\\nStep 5: Display the data \\nDisplay a chart of the average diamond price by color.\\n\\nNext to the Table tab, click + and then click Visualization.\\nThe visualization editor displays.\\n\\nIn the Visualization Type drop-down, verify that Bar is selected.\\nClear the Horizontal chart checkbox.\\nChange the aggregation type for the y columns from Sum to Average.\\nClick Save.\\n\\n\\n\\n\\n\\n\\n\\nNext steps \\nTo learn more about the primary tools you use and tasks you can perform with Databricks Data Science & Engineering workspace, see:\\n\\nWhat is Databricks?\\nNavigate the workspace\\nIntroduction to Databricks notebooks and Visualizations in Databricks notebooks\\nLibraries\\nCompute and Introduction to Databricks Workflows\\nLoad data using the add data UI and Create or modify a table using file upload\\nWhat is Catalog Explorer?\\nDeveloper tools and guidance\\nTechnology partners\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRun your first ETL workload on Databricks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nRun your first ETL workload on Databricks\\n\\n\\n\\n\\n\\n\\n\\nRun your first ETL workload on Databricks \\nLearn how to use production-ready tools from Databricks to develop and deploy your first extract, transform, and load (ETL) pipelines for data orchestration.\\nBy the end of this article, you will feel comfortable:\\n\\nLaunching a Databricks all-purpose compute cluster.\\nCreating a Databricks notebook.\\nConfiguring incremental data ingestion to Delta Lake with Auto Loader.\\nExecuting notebook cells to process, query, and preview data.\\nScheduling a notebook as a Databricks job.\\n\\nThis tutorial uses interactive notebooks to complete common ETL tasks in Python or Scala.\\nYou can also use Delta Live Tables to build ETL pipelines. Databricks created Delta Live Tables to reduce the complexity of building, deploying, and maintaining production ETL pipelines. See Tutorial: Declare a data pipeline with SQL in Delta Live Tables.\\nYou can also use the Databricks Terraform provider to create this article’s resources. See Create clusters, notebooks, and jobs with Terraform.\\n\\nRequirements \\n\\nYou are logged into a Databricks workspace.\\nYou have permission to create a cluster.\\n\\n\\nNote\\nIf you do not have cluster control privileges, you can still complete most of the steps below as long as you have access to a cluster.\\nIf you only have access to the Databricks SQL workspace, see Set up your workspace to use Databricks SQL.\\n\\n\\n\\nStep 1: Create a cluster \\nTo do exploratory data analysis and data engineering, create a cluster to provide the compute resources needed to execute commands.\\n\\nClick  Compute in the sidebar.\\nOn the Compute page, click Create Cluster. This opens the New Cluster page.\\nSpecify a unique name for the cluster, leave the remaining values in their default state, and click Create Cluster.\\n\\nTo learn more about Databricks clusters, see Compute.\\n\\n\\nStep 2: Create a Databricks notebook \\nTo get started writing and executing interactive code on Databricks, create a notebook.\\n\\nClick  New in the sidebar, then click Notebook.\\nOn the Create Notebook page:\\n\\nSpecify a unique name for your notebook.\\nMake sure the default language is set to Python or Scala.\\nSelect the cluster you created in step 1 from the Cluster dropdown.\\nClick Create.\\n\\n\\n\\nA notebook opens with an empty cell at the top.\\nTo learn more about creating and managing notebooks, see Manage notebooks.\\n\\n\\nStep 3: Configure Auto Loader to ingest data to Delta Lake \\nDatabricks recommends using Auto Loader for incremental data ingestion. Auto Loader automatically detects and processes new files as they arrive in cloud object storage.\\nDatabricks recommends storing data with Delta Lake. Delta Lake is an open source storage layer that provides ACID transactions and enables the data lakehouse. Delta Lake is the default format for tables created in Databricks.\\nTo configure Auto Loader to ingest data to a Delta Lake table, copy and paste the following code into the empty cell in your notebook:\\n\\n# Import functions\\nfrom pyspark.sql.functions import col, current_timestamp\\n\\n# Define variables used in code below\\nfile_path = \"/databricks-datasets/structured-streaming/events\"\\nusername = spark.sql(\"SELECT regexp_replace(current_user(), \\'[^a-zA-Z0-9]\\', \\'_\\')\").first()[0]\\ntable_name = f\"{username}_etl_quickstart\"\\ncheckpoint_path = f\"/tmp/{username}/_checkpoint/etl_quickstart\"\\n\\n# Clear out data from previous demo execution\\nspark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\\ndbutils.fs.rm(checkpoint_path, True)\\n\\n# Configure Auto Loader to ingest JSON data to a Delta table\\n(spark.readStream\\n  .format(\"cloudFiles\")\\n  .option(\"cloudFiles.format\", \"json\")\\n  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\\n  .load(file_path)\\n  .select(\"*\", col(\"_metadata.file_path\").alias(\"source_file\"), current_timestamp().alias(\"processing_time\"))\\n  .writeStream\\n  .option(\"checkpointLocation\", checkpoint_path)\\n  .trigger(availableNow=True)\\n  .toTable(table_name))\\n\\n\\n// Imports\\nimport org.apache.spark.sql.functions.current_timestamp\\nimport org.apache.spark.sql.streaming.Trigger\\nimport spark.implicits._\\n\\n// Define variables used in code below\\nval file_path = \"/databricks-datasets/structured-streaming/events\"\\nval username = spark.sql(\"SELECT regexp_replace(current_user(), \\'[^a-zA-Z0-9]\\', \\'_\\')\").first.get(0)\\nval table_name = s\"${username}_etl_quickstart\"\\nval checkpoint_path = s\"/tmp/${username}/_checkpoint\"\\n\\n// Clear out data from previous demo execution\\nspark.sql(s\"DROP TABLE IF EXISTS ${table_name}\")\\ndbutils.fs.rm(checkpoint_path, true)\\n\\n// Configure Auto Loader to ingest JSON data to a Delta table\\nspark.readStream\\n  .format(\"cloudFiles\")\\n  .option(\"cloudFiles.format\", \"json\")\\n  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\\n  .load(file_path)\\n  .select($\"*\", $\"_metadata.file_path\".as(\"source_file\"), current_timestamp.as(\"processing_time\"))\\n  .writeStream\\n  .option(\"checkpointLocation\", checkpoint_path)\\n  .trigger(Trigger.AvailableNow)\\n  .toTable(table_name)\\n\\n\\n\\n\\nNote\\nThe variables defined in this code should allow you to safely execute it without risk of conflicting with existing workspace assets or other users. Restricted network or storage permissions will raise errors when executing this code; contact your workspace administrator to troubleshoot these restrictions.\\n\\nTo learn more about Auto Loader, see What is Auto Loader?.\\n\\n\\nStep 4: Process and interact with data \\nNotebooks execute logic cell-by-cell. To execute the logic in your cell:\\n\\nTo run the cell you completed in the previous step, select the cell and press SHIFT+ENTER.\\nTo query the table you’ve just created, copy and paste the following code into an empty cell, then press SHIFT+ENTER to run the cell.\\n\\ndf = spark.read.table(table_name)\\n\\n\\nval df = spark.read.table(table_name)\\n\\n\\n\\n\\nTo preview the data in your DataFrame, copy and paste the following code into an empty cell, then press SHIFT+ENTER to run the cell.\\n\\ndisplay(df)\\n\\n\\ndisplay(df)\\n\\n\\n\\n\\n\\nTo learn more about interactive options for visualizing data, see Visualizations in Databricks notebooks.\\n\\n\\nStep 5: Schedule a job \\nYou can run Databricks notebooks as production scripts by adding them as a task in a Databricks job. In this step, you will create a new job that you can trigger manually.\\nTo schedule your notebook as a task:\\n\\nClick Schedule on the right side of the header bar.\\nEnter a unique name for the Job name.\\nClick Manual.\\nIn the Cluster drop-down, select the cluster you created in step 1.\\nClick Create.\\nIn the window that appears, click Run now.\\nTo see the job run results, click the  icon next to the Last run timestamp.\\n\\nFor more information on jobs, see What is Databricks Jobs?.\\n\\n\\nAdditional Integrations \\nLearn more about integrations and tools for data engineering with Databricks:\\n\\nConnect your favorite IDE\\nUse dbt with Databricks\\nLearn about the Databricks Command Line Interface (CLI)\\nLearn about the Databricks Terraform Provider\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nTutorial: Run an end-to-end lakehouse analytics pipeline\\n\\n\\n\\n\\n\\n\\n\\nTutorial: Run an end-to-end lakehouse analytics pipeline \\nThis tutorial shows you how to set up an end-to-end analytics pipeline for a Databricks lakehouse.\\n\\nImportant\\nThis tutorial uses interactive notebooks to complete common ETL tasks in Python on Unity Catalog enabled clusters. If you are not using Unity Catalog, see Run your first ETL workload on Databricks.\\n\\n\\nTasks in this tutorial \\nBy the end of this article, you will feel comfortable:\\n\\nLaunching a Unity Catalog enabled compute cluster.\\nCreating a Databricks notebook.\\nWriting and reading data from a Unity Catalog external location.\\nConfiguring incremental data ingestion to a Unity Catalog table with Auto Loader.\\nExecuting notebook cells to process, query, and preview data.\\nScheduling a notebook as a Databricks job.\\nQuerying Unity Catalog tables from Databricks SQL\\n\\nDatabricks provides a suite of production-ready tools that allow data professionals to quickly develop and deploy extract, transform, and load (ETL) pipelines. Unity Catalog allows data stewards to configure and secure storage credentials, external locations, and database objects for users throughout an organization. Databricks SQL allows analysts to run SQL queries against the same tables used in production ETL workloads, allowing for real time business intelligence at scale.\\n\\n\\n\\nRequirements \\n\\nYou are logged into Databricks.\\n\\n\\nNote\\nIf you do not have cluster control privileges, you can still complete most of the steps below as long as you have access to a cluster.\\nIf you only have access to the Databricks SQL workspace, see Set up your workspace to use Databricks SQL.\\n\\n\\n\\nStep 1: Create a cluster \\nTo do exploratory data analysis and data engineering, create a cluster to provide the compute resources needed to execute commands.\\n\\nClick  Compute in the sidebar.\\nClick  New in the sidebar, then select Cluster. This opens the New Cluster/Compute page.\\nSpecify a unique name for the cluster.\\nSelect the Single node radio button.\\nSelect Single User from the Access mode dropdown.\\nMake sure your email address is visible in the Single User field.\\nSelect the desired Databricks runtime version, 11.1 or above to use Unity Catalog.\\nClick Create compute to create the cluster.\\n\\nTo learn more about Databricks clusters, see Compute.\\n\\n\\nStep 2: Create a Databricks notebook \\nTo get started writing and executing interactive code on Databricks, create a notebook.\\n\\nClick  New in the sidebar, then click Notebook.\\nOn the Create Notebook page:\\n\\nSpecify a unique name for your notebook.\\nMake sure the default language is set to Python.\\nUse the Connect dropdown menu to select the cluster you created in step 1 from the Cluster dropdown.\\n\\n\\n\\nThe notebook opens with one empty cell.\\nTo learn more about creating and managing notebooks, see Manage notebooks.\\n\\n\\nStep 3: Write and read data from an external location managed by Unity Catalog \\nDatabricks recommends using Auto Loader for incremental data ingestion. Auto Loader automatically detects and processes new files as they arrive in cloud object storage.\\nUse Unity Catalog to manage secure access to external locations. Users or service principals with READ FILES permissions on an external location can use Auto Loader to ingest data.\\nNormally, data will arrive in an external location due to writes from other systems. In this demo, you can simulate data arrival by writing out JSON files to an external location.\\nCopy the code below into a notebook cell. Replace the string value for catalog with the name of a catalog with CREATE CATALOG and USE CATALOG permissions. Replace the string value for external_location with the path for an external location with READ FILES, WRITE FILES, and CREATE EXTERNAL TABLE permissions.\\nExternal locations can be defined as an entire storage container, but often point to a directory nested in a container.\\nThe correct format for an external location path is \"s3://bucket-name/path/to/external_location\".\\n\\n external_location = \"<your-external-location>\"\\n catalog = \"<your-catalog>\"\\n\\n dbutils.fs.put(f\"{external_location}/filename.txt\", \"Hello world!\", True)\\n display(dbutils.fs.head(f\"{external_location}/filename.txt\"))\\n dbutils.fs.rm(f\"{external_location}/filename.txt\")\\n\\n display(spark.sql(f\"SHOW SCHEMAS IN {catalog}\"))\\n\\n\\nExecuting this cell should print a line that reads 12 bytes, print the string “Hello world!”, and display all the databases present in the catalog provided. If you are unable to get this cell to run, confirm that you are in a Unity Catalog enabled workspace and request proper permissions from your workspace administrator to complete this tutorial.\\nThe Python code below uses your email address to create a unique database in the catalog provided and a unique storage location in external location provided. Executing this cell will remove all data associated with this tutorial, allowing you to execute this example idempotently. A class is defined and instantiated that you will use to simulate batches of data arriving from a connected system to your source external location.\\nCopy this code to a new cell in your notebook and execute it to configure your environment.\\n\\nNote\\nThe variables defined in this code should allow you to safely execute it without risk of conflicting with existing workspace assets or other users. Restricted network or storage permissions will raise errors when executing this code; contact your workspace administrator to troubleshoot these restrictions.\\n\\n\\nfrom pyspark.sql.functions import col\\n\\n# Set parameters for isolation in workspace and reset demo\\nusername = spark.sql(\"SELECT regexp_replace(current_user(), \\'[^a-zA-Z0-9]\\', \\'_\\')\").first()[0]\\ndatabase = f\"{catalog}.e2e_lakehouse_{username}_db\"\\nsource = f\"{external_location}/e2e-lakehouse-source\"\\ntable = f\"{database}.target_table\"\\ncheckpoint_path = f\"{external_location}/_checkpoint/e2e-lakehouse-demo\"\\n\\nspark.sql(f\"SET c.username=\\'{username}\\'\")\\nspark.sql(f\"SET c.database={database}\")\\nspark.sql(f\"SET c.source=\\'{source}\\'\")\\n\\nspark.sql(\"DROP DATABASE IF EXISTS ${c.database} CASCADE\")\\nspark.sql(\"CREATE DATABASE ${c.database}\")\\nspark.sql(\"USE ${c.database}\")\\n\\n# Clear out data from previous demo execution\\ndbutils.fs.rm(source, True)\\ndbutils.fs.rm(checkpoint_path, True)\\n\\n\\n# Define a class to load batches of data to source\\nclass LoadData:\\n\\n    def __init__(self, source):\\n        self.source = source\\n\\n    def get_date(self):\\n        try:\\n            df = spark.read.format(\"json\").load(source)\\n        except:\\n            return \"2016-01-01\"\\n        batch_date = df.selectExpr(\"max(distinct(date(tpep_pickup_datetime))) + 1 day\").first()[0]\\n        if batch_date.month == 3:\\n            raise Exception(\"Source data exhausted\")\\n        return batch_date\\n\\n    def get_batch(self, batch_date):\\n        return (\\n            spark.table(\"samples.nyctaxi.trips\")\\n            .filter(col(\"tpep_pickup_datetime\").cast(\"date\") == batch_date)\\n        )\\n\\n    def write_batch(self, batch):\\n        batch.write.format(\"json\").mode(\"append\").save(self.source)\\n\\n    def land_batch(self):\\n        batch_date = self.get_date()\\n        batch = self.get_batch(batch_date)\\n        self.write_batch(batch)\\n\\nRawData = LoadData(source)\\n\\n\\nYou can now land a batch of data by copying the following code into a cell and executing it. You can manually execute this cell up to 60 times to trigger new data arrival.\\nRawData.land_batch()\\n\\n\\n\\n\\nStep 4: Configure Auto Loader to ingest data to Unity Catalog \\nDatabricks recommends storing data with Delta Lake. Delta Lake is an open source storage layer that provides ACID transactions and enables the data lakehouse. Delta Lake is the default format for tables created in Databricks.\\nTo configure Auto Loader to ingest data to a Unity Catalog table, copy and paste the following code into an empty cell in your notebook:\\n# Import functions\\nfrom pyspark.sql.functions import col, current_timestamp\\n\\n# Configure Auto Loader to ingest JSON data to a Delta table\\n(spark.readStream\\n  .format(\"cloudFiles\")\\n  .option(\"cloudFiles.format\", \"json\")\\n  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\\n  .load(file_path)\\n  .select(\"*\", col(\"_metadata.file_path\").alias(\"source_file\"), current_timestamp().alias(\"processing_time\"))\\n  .writeStream\\n  .option(\"checkpointLocation\", checkpoint_path)\\n  .trigger(availableNow=True)\\n  .option(\"mergeSchema\", \"true\")\\n  .toTable(table))\\n\\n\\nTo learn more about Auto Loader, see What is Auto Loader?.\\nTo learn more about Structured Streaming with Unity Catalog, see Using Unity Catalog with Structured Streaming.\\n\\n\\nStep 5: Process and interact with data \\nNotebooks execute logic cell-by-cell. Use these steps to execute the logic in your cell:\\n\\nTo run the cell you completed in the previous step, select the cell and press SHIFT+ENTER.\\nTo query the table you’ve just created, copy and paste the following code into an empty cell, then press SHIFT+ENTER to run the cell.\\ndf = spark.read.table(table_name)\\n\\n\\n\\nTo preview the data in your DataFrame, copy and paste the following code into an empty cell, then press SHIFT+ENTER to run the cell.\\ndisplay(df)\\n\\n\\n\\n\\nTo learn more about interactive options for visualizing data, see Visualizations in Databricks notebooks.\\n\\n\\nStep 6: Schedule a job \\nYou can run Databricks notebooks as production scripts by adding them as a task in a Databricks job. In this step, you will create a new job that you can trigger manually.\\nTo schedule your notebook as a task:\\n\\nClick Schedule on the right side of the header bar.\\nEnter a unique name for the Job name.\\nClick Manual.\\nIn the Cluster drop-down, select the cluster you created in step 1.\\nClick Create.\\nIn the window that appears, click Run now.\\nTo see the job run results, click the  icon next to the Last run timestamp.\\n\\nFor more information on jobs, see What is Databricks Jobs?.\\n\\n\\nStep 7: Query table from Databricks SQL \\nAnyone with the USE CATALOG permission on the current catalog, the USE SCHEMA permission on the current schema, and SELECT permissions on the table can query the contents of the table from their preferred Databricks API.\\nYou need access to a running SQL warehouse to execute queries in Databricks SQL.\\nThe table you created earlier in this tutorial has the name target_table. You can query it using the catalog you provided in the first cell and the database with the patern e2e_lakehouse_<your-username>. You can use Catalog Explorer to find the data objects that you created.\\n\\n\\nAdditional Integrations \\nLearn more about integrations and tools for data engineering with Databricks:\\n\\nConnect your favorite IDE\\nUse dbt with Databricks\\nLearn about the Databricks Command Line Interface (CLI)\\nLearn about the Databricks Terraform Provider\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGet free Databricks training | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nGet free Databricks training\\n\\n\\n\\n\\n\\n\\n\\nGet free Databricks training \\nAs a customer, you have access to all Databricks free customer training offerings. These offerings include courses, recorded webinars, and quarterly product roadmap webinars. You can access the material from your Databricks Academy account. Follow these steps to get started:\\n\\nGo to Databricks Academy and click the red Academy login button in the top navigation.\\n\\nIf you’ve logged into Databricks Academy before, use your existing credentials.\\nIf you’ve never logged into Databricks Academy, a customer account has been created for you, using your Databricks username, usually your work email address. You must reset your password. It may take up to 24 hours for the training pathway to appear in your account.\\n\\n\\nAfter you log into your Databricks Academy account, click  in the top left corner.\\n\\nClick Course Catalog.\\nThe catalogs available to you appear. Databricks Academy organizes groupings of learning content into catalogs, which include courses and learning paths.\\n\\n\\n\\nIf you’ve followed the steps above and do not see the pathways in your account, please file a training support ticket.\\nThe Databricks documentation also provides many tutorials and quickstarts that can help you get up to speed on the platform, both here in the Getting Started section and in other sections:\\n\\nQuickstart\\nApache Spark\\nLoad data into a Databricks lakehouse\\nSample datasets\\nDataFrames\\nDelta Lake\\nStructured Streaming\\nMachine Learning\\n\\nThe Knowledge Base provides troubleshooting tips and answers to frequently asked questions.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/getting-started/free-training.html', 'title': 'Get free Databricks training | Databricks on AWS', 'description': 'Access free training for Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSQL language reference | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nPython, SparkR & Scala intros\\nREST API reference\\nMLFlow API\\nFeature Store Python API\\nApache Spark API\\nDelta Lake API\\nDelta Live Tables API\\nSQL language reference\\n\"Applies to\" label\\nHow to read a syntax diagram\\nHow to add comments to SQL statements\\nConfiguration parameters\\nData types and literals\\nFunctions\\nSQL data type rules\\nDatetime patterns\\nH3 geospatial functions\\nLambda functions\\nWindow functions\\nIdentifiers\\nNames\\nIDENTIFIER clause\\nNULL semantics\\nExpressions\\nParameter markers\\nVariables\\nName resolution\\nJSON path expressions\\nPartitions\\nANSI compliance in Databricks Runtime\\nApache Hive compatibility\\nPrincipals\\nPrivileges and securable objects in Unity Catalog\\nPrivileges and securable objects in the Hive metastore\\nRefresh Unity Catalog metadata\\nExternal locations\\nExternal tables\\nStorage credentials\\nVolumes\\nDelta Sharing\\nFederated queries (Lakehouse Federation)\\nInformation schema\\nReserved words\\nALTER CATALOG\\nALTER CONNECTION\\nALTER CREDENTIAL\\nALTER DATABASE\\nALTER LOCATION\\nALTER PROVIDER\\nALTER RECIPIENT\\nALTER STREAMING TABLE\\nADD CONSTRAINT clause\\nDROP CONSTRAINT clause\\nALTER TABLE … COLUMN clause\\nALTER TABLE … PARTITION\\nCLUSTER BY clause (TABLE)\\nColumn mask clause\\nROW FILTER clause\\nALTER TABLE\\nALTER SCHEMA\\nALTER SHARE\\nALTER VIEW\\nALTER VOLUME\\nCOMMENT ON\\nCREATE BLOOMFILTER INDEX\\nCREATE CATALOG\\nCREATE CONNECTION\\nCREATE DATABASE\\nCREATE FUNCTION (SQL)\\nCREATE FUNCTION (External)\\nCREATE LOCATION\\nCREATE MATERIALIZED VIEW\\nCREATE RECIPIENT\\nCREATE SCHEMA\\nCREATE SERVER\\nCREATE SHARE\\nCREATE STREAMING TABLE\\nCREATE TABLE [USING]\\nCREATE TABLE LIKE\\nCONSTRAINT clause\\nCREATE TABLE CLONE\\nTable properties and table options\\nCREATE TABLE with Hive format\\nCREATE TABLE\\nCREATE VIEW\\nCREATE VOLUME\\nDECLARE VARIABLE\\nDROP BLOOMFILTER INDEX\\nDROP CATALOG\\nDROP CONNECTION\\nDROP DATABASE\\nDROP CREDENTIAL\\nDROP FUNCTION\\nDROP LOCATION\\nDROP PROVIDER\\nDROP RECIPIENT\\nDROP SCHEMA\\nDROP SHARE\\nDROP TABLE\\nDROP VARIABLE\\nDROP VIEW\\nDROP VOLUME\\nMSCK REPAIR TABLE\\nREFRESH FOREIGN (CATALOG, SCHEMA, or TABLE)\\nREFRESH (MATERIALIZED VIEW or STREAMING TABLE)\\nSYNC\\nTRUNCATE TABLE\\nUNDROP TABLE\\nCOPY INTO\\nDELETE FROM\\nINSERT INTO\\nINSERT OVERWRITE DIRECTORY\\nINSERT OVERWRITE DIRECTORY with Hive format\\nLOAD DATA\\nMERGE INTO\\nUPDATE\\nQuery\\nSELECT\\nVALUES\\nEXPLAIN\\nCACHE SELECT\\nCONVERT TO DELTA\\nDESCRIBE HISTORY\\nFSCK REPAIR TABLE\\nGENERATE\\nOPTIMIZE\\nREORG TABLE\\nRESTORE\\nVACUUM\\nANALYZE TABLE\\nCACHE TABLE\\nCLEAR CACHE\\nREFRESH CACHE\\nREFRESH FUNCTION\\nREFRESH TABLE\\nUNCACHE TABLE\\nDESCRIBE CATALOG\\nDESCRIBE CONNECTION\\nDESCRIBE CREDENTIAL\\nDESCRIBE DATABASE\\nDESCRIBE FUNCTION\\nDESCRIBE LOCATION\\nDESCRIBE PROVIDER\\nDESCRIBE QUERY\\nDESCRIBE RECIPIENT\\nDESCRIBE SCHEMA\\nDESCRIBE SHARE\\nDESCRIBE TABLE\\nDESCRIBE VOLUME\\nLIST\\nSHOW ALL IN SHARE\\nSHOW CATALOGS\\nSHOW COLUMNS\\nSHOW CONNECTIONS\\nSHOW CREATE TABLE\\nSHOW CREDENTIALS\\nSHOW DATABASES\\nSHOW FUNCTIONS\\nSHOW GROUPS\\nSHOW LOCATIONS\\nSHOW PARTITIONS\\nSHOW PROVIDERS\\nSHOW RECIPIENTS\\nSHOW SCHEMAS\\nSHOW SHARES\\nSHOW SHARES IN PROVIDER\\nSHOW TABLE\\nSHOW TABLES\\nSHOW TABLES DROPPED\\nSHOW TBLPROPERTIES\\nSHOW USERS\\nSHOW VIEWS\\nSHOW VOLUMES\\nEXECUTE IMMEDIATE\\nRESET\\nSET\\nSET TIMEZONE\\nSET VARIABLE\\nUSE CATALOG\\nUSE DATABASE\\nUSE SCHEMA\\nADD ARCHIVE\\nADD FILE\\nADD JAR\\nLIST ARCHIVE\\nLIST FILE\\nLIST JAR\\nALTER GROUP\\nCREATE GROUP\\nDENY\\nDROP GROUP\\nGRANT\\nGRANT SHARE\\nREPAIR PRIVILEGES\\nREVOKE\\nREVOKE SHARE\\nSHOW GRANTS\\nSHOW GRANTS ON SHARE\\nSHOW GRANTS TO RECIPIENT\\n\\n\\n\\n\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks reference documentation \\nLanguage-specific introductions to Databricks \\nSQL language reference\\n\\n\\n\\n\\n\\n\\n\\nSQL language reference \\nThis is a SQL command reference for Databricks SQL and Databricks Runtime.\\nFor information about using SQL with Delta Live Tables, see Delta Live Tables SQL language reference.\\n\\nGeneral reference \\nThis general reference describes data types, functions, identifiers, literals, and semantics:\\n\\n\\n\"Applies to\" label\\nHow to read a syntax diagram\\nHow to add comments to SQL statements\\nConfiguration parameters\\nData types and literals\\nFunctions\\nSQL data type rules\\nDatetime patterns\\nH3 geospatial functions\\nLambda functions\\nWindow functions\\nIdentifiers\\nNames\\nIDENTIFIER clause\\nNULL semantics\\nExpressions\\nParameter markers\\nVariables\\nName resolution\\nJSON path expressions\\nPartitions\\nANSI compliance in Databricks Runtime\\nApache Hive compatibility\\nPrincipals\\nPrivileges and securable objects in Unity Catalog\\nPrivileges and securable objects in the Hive metastore\\nRefresh Unity Catalog metadata\\nExternal locations\\nExternal tables\\nStorage credentials\\nVolumes\\nDelta Sharing\\nFederated queries (Lakehouse Federation)\\nInformation schema\\nReserved words\\n\\n\\n\\n\\nDDL statements \\nYou use data definition statements to create or modify the structure of database objects in a database:\\n\\n\\nALTER CATALOG\\nALTER CONNECTION\\nALTER CREDENTIAL\\nALTER DATABASE\\nALTER LOCATION\\nALTER PROVIDER\\nALTER RECIPIENT\\nALTER STREAMING TABLE\\nALTER TABLE\\nALTER SCHEMA\\nALTER SHARE\\nALTER VIEW\\nALTER VOLUME\\nCOMMENT ON\\nCREATE BLOOMFILTER INDEX\\nCREATE CATALOG\\nCREATE CONNECTION\\nCREATE DATABASE\\nCREATE FUNCTION (SQL)\\nCREATE FUNCTION (External)\\nCREATE LOCATION\\nCREATE MATERIALIZED VIEW\\nCREATE RECIPIENT\\nCREATE SCHEMA\\nCREATE SERVER\\nCREATE SHARE\\nCREATE STREAMING TABLE\\nCREATE TABLE\\nCREATE VIEW\\nCREATE VOLUME\\nDECLARE VARIABLE\\nDROP BLOOMFILTER INDEX\\nDROP CATALOG\\nDROP CONNECTION\\nDROP DATABASE\\nDROP CREDENTIAL\\nDROP FUNCTION\\nDROP LOCATION\\nDROP PROVIDER\\nDROP RECIPIENT\\nDROP SCHEMA\\nDROP SHARE\\nDROP TABLE\\nDROP VARIABLE\\nDROP VIEW\\nDROP VOLUME\\nMSCK REPAIR TABLE\\nREFRESH FOREIGN (CATALOG, SCHEMA, or TABLE)\\nREFRESH (MATERIALIZED VIEW or STREAMING TABLE)\\nSYNC\\nTRUNCATE TABLE\\nUNDROP TABLE\\n\\n\\n\\n\\nDML statements \\nYou use data manipulation statements to add, change, or delete data from a Delta Lake table:\\n\\n\\nCOPY INTO\\nDELETE FROM\\nINSERT INTO\\nINSERT OVERWRITE DIRECTORY\\nINSERT OVERWRITE DIRECTORY with Hive format\\nLOAD DATA\\nMERGE INTO\\nUPDATE\\n\\n\\n\\n\\nData retrieval statements \\nYou use a query to retrieve rows from one or more tables according to the specified clauses. The full syntax\\nand brief description of supported clauses are explained in the Query article.\\nThe related SQL statements SELECT and VALUES are also included in this section.\\n\\n\\nQuery\\nSELECT\\nVALUES\\n\\n\\nDatabricks SQL also provides the ability to generate the logical and physical plan for a query using the EXPLAIN statement.\\n\\n\\nEXPLAIN\\n\\n\\n\\n\\nDelta Lake statements \\nYou use Delta Lake SQL statements to manage tables stored in Delta Lake format:\\n\\n\\nCACHE SELECT\\nCONVERT TO DELTA\\nDESCRIBE HISTORY\\nFSCK REPAIR TABLE\\nGENERATE\\nOPTIMIZE\\nREORG TABLE\\nRESTORE\\nVACUUM\\n\\n\\n\\nFor details on using Delta Lake statements, see What is Delta Lake?.\\n\\n\\nAuxiliary statements \\nYou use auxiliary statements to collect statistics, manage caching,\\nexplore metadata, set configurations, and manage resources:\\n\\n\\nAnalyze statement\\nApache Spark Cache statements\\nDescribe statements\\nShow statements\\nConfiguration, variable management, and misc statements\\nResource management\\n\\n\\n\\nAnalyze statement \\n\\n\\nANALYZE TABLE\\n\\n\\n\\n\\nApache Spark Cache statements \\nApplies to:  Databricks Runtime\\n\\n\\nCACHE TABLE\\nCLEAR CACHE\\nREFRESH CACHE\\nREFRESH FUNCTION\\nREFRESH TABLE\\nUNCACHE TABLE\\n\\n\\n\\n\\nDescribe statements \\n\\n\\nDESCRIBE CATALOG\\nDESCRIBE CONNECTION\\nDESCRIBE CREDENTIAL\\nDESCRIBE DATABASE\\nDESCRIBE FUNCTION\\nDESCRIBE LOCATION\\nDESCRIBE PROVIDER\\nDESCRIBE QUERY\\nDESCRIBE RECIPIENT\\nDESCRIBE SCHEMA\\nDESCRIBE SHARE\\nDESCRIBE TABLE\\nDESCRIBE VOLUME\\n\\n\\n\\n\\nShow statements \\n\\n\\nLIST\\nSHOW ALL IN SHARE\\nSHOW CATALOGS\\nSHOW COLUMNS\\nSHOW CONNECTIONS\\nSHOW CREATE TABLE\\nSHOW CREDENTIALS\\nSHOW DATABASES\\nSHOW FUNCTIONS\\nSHOW GROUPS\\nSHOW LOCATIONS\\nSHOW PARTITIONS\\nSHOW PROVIDERS\\nSHOW RECIPIENTS\\nSHOW SCHEMAS\\nSHOW SHARES\\nSHOW SHARES IN PROVIDER\\nSHOW TABLE\\nSHOW TABLES\\nSHOW TABLES DROPPED\\nSHOW TBLPROPERTIES\\nSHOW USERS\\nSHOW VIEWS\\nSHOW VOLUMES\\n\\n\\n\\n\\nConfiguration, variable management, and misc statements \\n\\n\\nEXECUTE IMMEDIATE\\nRESET\\nSET\\nSET TIMEZONE\\nSET VARIABLE\\nUSE CATALOG\\nUSE DATABASE\\nUSE SCHEMA\\n\\n\\n\\n\\nResource management \\nApplies to:  Databricks Runtime\\n\\n\\nADD ARCHIVE\\nADD FILE\\nADD JAR\\nLIST ARCHIVE\\nLIST FILE\\nLIST JAR\\n\\n\\n\\n\\n\\nSecurity statements \\nYou use security SQL statements to manage access to data:\\n\\n\\nALTER GROUP\\nCREATE GROUP\\nDENY\\nDROP GROUP\\nGRANT\\nGRANT SHARE\\nREPAIR PRIVILEGES\\nREVOKE\\nREVOKE SHARE\\nSHOW GRANTS\\nSHOW GRANTS ON SHARE\\nSHOW GRANTS TO RECIPIENT\\n\\n\\nFor details about using these statements, see Hive metastore privileges and securable objects (legacy).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nError handling in Databricks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nStatus Page\\nLimits\\nDatabricks clouds and regions\\nSupported browsers\\nConfigure domain name firewall rules\\nPlatform release process\\nSubmit product feedback\\nSupport\\nError messages\\nSQLSTATE codes\\nError classes in Databricks\\n\\n\\n\\n\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nResources \\nError handling in Databricks\\n\\n\\n\\n\\n\\n\\n\\nError handling in Databricks \\nApplies to:  Databricks SQL  Databricks Runtime 12.2 and above\\n\\n\\n\\nError components \\nWhen Databricks raises an error it includes the following components:\\n\\nError Class\\nA descriptive, human-readable, string unique to the error condition.\\nSome error classes include sublasses.\\nFor example: ‘TABLE_OR_VIEW_NOT_FOUND’, and ‘INCOMPLETE_TYPE_DEFINITION.ARRAY’.\\nFor a list of all error classes see Error Classes.\\n\\nSQLSTATE\\nA five character long string, grouping error classes into a standard format supported by many products and APIs.\\nFor example: \\'42P01\\'\\nFor a full list of all SQLSTATEs used by Databricks see SQLSTATEs.\\n\\nParameterized Message\\nThe error message with placeholders for the parameters.\\nFor example : ‘TABLE_OR_VIEW_NOT_FOUND’ includes the following message:\\nThe table or view <relationName> cannot be found.\\n\\n\\nYou can use the parameterized message to render an error message by mapping message parameter values to the parameter tags <parameter>.\\n\\nMessage Parameters\\nA map of parameters and values that provide additional information about the error.\\nFor example: \\'relationName\\' -> \\'main.default.tab1\\'.\\n\\nMessage\\nThe completely rendered error message, including the error class and the SQLSTATE, with the parameters filled in.\\nFor example:\\n[TABLE_OR_VIEW_NOT_FOUND] The table or view `does_not_exist` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 14;\\n\\'Project [*]\\n+- \\'UnresolvedRelation [does_not_exist], [], false\\n\\n\\n\\n\\n\\nWarning\\nMessage and Parameterized Message are not stable across releases.\\nThe message text may be changed or localized without notice.\\nTo programmatically handle an error condition, use the Error Class, SQLSTATE, and Message Parameters instead.\\n\\n\\n\\nHandling error conditions \\nApplies to:  Databricks SQL  Databricks Runtime 14.2 and above\\n\\nPreview\\nThis feature is in Public Preview.\\n\\nDatabricks provides language specific APIs to handle error conditions.\\n\\nPython \\nFor Python use pySparkException\\n\\nPySparkException.getErrorClass(): Returns the error class of the exception as a string.\\nPySparkException.getMessageParameters(): Returns the message parameters of the exception as a dictionary.\\nPySparkException.getSqlState(): Returns the SQLSTATE of the expression as a string.\\n\\n\\n\\nScala \\nFor Scala use SparkThrowable\\n\\ngetErrorClass(): Returns an error class as a string.\\ngetMessageParameters(): Returns a message parameters as a map.\\ngetSqlState(): Returns an SQLSTATE as a string.\\n\\n\\n\\nExamples \\n\\nCatch any exception and display error class, message parameters and SQLSTATE. Also display the default error message\\n\\nimport org.apache.spark.SparkThrowable\\n\\ntry {\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\n}\\ncatch {\\n  case ex: SparkThrowable =>\\n    println(\"Error Class       : \" + ex.getErrorClass)\\n    println(\"Message parameters: \" + ex.getMessageParameters())\\n    println(\"SQLSTATE          : \" + ex.getSqlState)\\n    println(ex)\\n}\\n\\n\\nfrom pyspark.errors import PySparkException\\n\\ntry:\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\nexcept PySparkException as ex:\\n  print(\"Error Class       : \" + ex.getErrorClass())\\n  print(\"Message parameters: \" + str(ex.getMessageParameters()))\\n  print(\"SQLSTATE          : \" + ex.getSqlState())\\n  print(ex)\\n\\n\\n\\nResult\\n  Error Class       : TABLE_OR_VIEW_NOT_FOUND\\n  Message parameters: {\\'relationName\\': \\'`does_not_exist`\\'}\\n  SQLSTATE          : 42P01\\n  [TABLE_OR_VIEW_NOT_FOUND] The table or view `does_not_exist` cannot be found. Verify the spelling and correctness of the schema and catalog.\\n  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\n  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 14;\\n  \\'Project [*]\\n  +- \\'UnresolvedRelation [does_not_exist], [], false\\n\\n\\n\\nCatch the SQLSTATE 42P01 only and display a custom message:\\n\\nimport org.apache.spark.SparkThrowable\\n\\ntry {\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\n}\\ncatch {\\n  case ex: SparkThrowable if (ex.getSqlState == \"42P01\") =>\\n    println(\"I\\'m so sorry, but I cannot find: \" + ex.getMessageParameters().get(\"relationName\"))\\n}\\n\\n\\nfrom pyspark.errors import PySparkException\\n\\ntry:\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\nexcept PySparkException as ex:\\n  if (ex.getSqlState() == \"42P01\"):\\n    print(\"I\\'m so sorry, but I cannot find: \" + ex.getMessageParameters()[\\'relationName\\'])\\n  else:\\n    raise\\n\\n\\n\\nResult\\nI\\'m so sorry, but I cannot find: `does_not_exist`\\n\\n\\n\\nCatch the error class TABLE_OR_VIEW_NOT_FOUND only and display a custom message:\\n\\nimport org.apache.spark.SparkThrowable\\n\\ntry {\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\n}\\ncatch {\\n  case ex: SparkThrowable if (ex.getErrorClass == \"TABLE_OR_VIEW_NOT_FOUND\") =>\\n    println(\"I\\'m so sorry, but I cannot find: \" + ex.getMessageParameters().get(\"relationName\"))\\n}\\n\\n\\nfrom pyspark.errors import PySparkException\\n\\ntry:\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\nexcept PySparkException as ex:\\n  if (ex.getErrorClass() == \"TABLE_OR_VIEW_NOT_FOUND\"):\\n    print(\"I\\'m so sorry, but I cannot find: \" + ex.getMessageParameters()[\\'relationName\\'])\\n  else:\\n    raise\\n\\n\\n\\nResult\\nI\\'m so sorry, but I cannot find: `does_not_exist`\\n\\n\\n\\n\\n\\n\\n\\nUser raised exceptions \\nDatabricks provides the following functions to raise user defined errors:\\n\\nraise_error\\nRaises an exception with a custom error message.\\n\\nassert_true\\nRaises an error with an optional error message, if a condition is not met.\\n\\n\\nBoth functions return the error class ‘USER_RAISED_EXCEPTION’ and the SQLSTATE \\'P0001\\' along with a user defined  message.\\n\\nExamples \\n> SELECT raise_error(\\'This is a custom error message\\');\\n [USER_RAISED_EXCEPTION] This is a custom error message. SQLSTATE: P0001\\n\\n> SELECT assert_true(1 = 2, \\'One is not two!\\');\\n [USER_RAISED_EXCEPTION] One is not two! SQLSTATE: P0001\\n\\n> SELECT assert_true(1 = 2);\\n [USER_RAISED_EXCEPTION] \\'(1 = 2)\\' is not true! SQLSTATE: P0001\\n\\n\\n\\n\\n\\nRelated \\n\\nassert_true function\\nraise_error function\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWelcome to The Apache Software Foundation!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to Main Content\\n\\n\\n\\n\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSponsor the ASF\\n\\n\\n\\nCommunity\\xa0\\n\\nContributor Getting Started\\nBecoming a Committer\\nCode of Conduct\\nCommunity Resources\\nCommunity Over Code\\nEvents\\nStore\\n\\n\\n\\nProjects\\xa0\\n\\nProjects\\nIncubator Projects\\nProjects Directory \\nMailing Lists \\nReport a Vulnerability\\n\\n\\n\\nDownloads\\xa0\\n\\nDistributions\\nReleases\\nInfrastructure Status\\nInfrastructure Statistics\\n\\n\\n\\nLearn\\xa0\\n\\nBlog\\nHow the ASF Works\\nThe Apache Way\\nLegal & Trademark\\nLicenses\\nGlossary\\nFAQ\\n\\n\\n\\nResources & Tools\\xa0\\n\\nDeveloper Information\\nWiki\\nIssues\\nSlack\\nSelf Serve Portal\\nInfrastructure\\nWhimsy\\nBrand Guidelines\\nProject Logos\\n\\n\\n\\nAbout\\xa0\\n\\nAbout\\nOur Sponsors\\nCorporate Sponsorship\\nIndividual Supporters\\nLeadership\\nMembers\\nDiversity & Inclusion\\nNewsroom\\nContact\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSoftware for the Public Good¶\\nASF’s open source software is used ubiquitously around the world with more than 8,400 committers contributing to more than 320 active projects.\\nSee All ProjectsContribute\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"The most popular open source software is\\xa0Apache…\"\\nDZone, “What Open Source Software Do You\\xa0Use?”\\n\\n\\n\\n\\n\"Apache-style licensing may yield more adoption and\\xa0money.\"\\nMatt Asay, c|net\\n\\n\\n\\n\\n\"Apache’s \\'survival of the fittest\\' ethos breeds better\\xa0software\"\\nZDNet\\n\\n\\n\\n❮\\n❯\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLatest News¶\\nKeep up with the ASF\\'s news and announcements by subscribing to the\\n   \\t\\tApache Announcements List, as well as following the Foundation Blog,\\n\\t\\tApache Weekly News Round-Ups,\\n\\t\\t@TheASF on Twitter,\\n\\t\\tThe Apache Software Foundation on LinkedIn, on the ASF\\'s YouTube channel, and on Feathercast, the voice of the ASF.\\n\\n\\n\\n\\n\\nFoundation Blog\\nWhy Generative AI Guidance is Essential to Contributors of Open Source\\xa0\\n\\nContinue Reading\\xa0→\\n\\n\\nFoundation Blog\\nUpdate on EU Software Regulation: Lots of improvements &#038; good news\\n\\nContinue Reading\\xa0→\\n\\n\\nFoundation Blog\\nNew year, new goals!\\xa0\\n\\nContinue Reading\\xa0→\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReports¶\\nOfficial ASF reports and statements,\\n\\t\\t\\t\\t\\t\\t including Quarterly and Annual Reports, Vision Statement,\\n\\t\\t\\t\\t\\t\\t \"Apache is Open\", 5-Year Strategic Plan, and more.\\n\\n\\n\\n\\n\\n\\n\\nCommunity¶\\nGuidance and mentoring for those interested in participating in Apache projects and their communities.\\n\\t\\t\\t\\t\\t\\t\\tFrom Google Summer of Code to community events, get started here to learn how to become an\\n\\t\\t\\t\\t\\t\\t\\tApache contributor.\\n\\n\\n\\n\\n\\n\\n\\nThe Apache Way¶\\nOur consensus-driven, open development process was refined over the past 20 years and produced some of\\n\\t\\t\\t\\t\\t\\t\\tthe largest and longest-lived Open Source projects that have revolutionized the industry.\\n\\n\\n\\n\\n\\n\\n\\nConferences¶\\n\"Tomorrow\\'s Technology Today\" since 1998. Intentionally intimate, offering unparalleled educational,\\n\\t\\t\\t\\t\\t\\t\\tnetworking, and collaboration opportunities.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout the Foundation¶\\nThe ASF is a 501(c)(3) charitable organization run almost exclusively by volunteers who oversee hundreds of projects. Our sponsors enable us to maintain the infrastructure needed to support them.\\n\\n\\nGovernanceSee our sponsors\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nASF Projects¶\\nThe ASF develops, stewards, and incubates hundreds of freely available, enterprise-grade projects that serve as the backbone for the most visible and widely used applications in computing today.\\n\\n\\nContribute¶\\nIf you wish to contribute to ASF projects, the Community Development site has tools, processes, and advice to help you get started.\\nGet Started\\n\\n\\nHost a Project¶\\nThe Apache Incubator provides a path for projects and their communities that want to enter the ASF.\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeatured Projects¶\\n\\n\\n\\n\\n\\nCXF\\n\\nYetus\\n\\nMINA\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCXF\\nService Framework\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\nYetus\\nCollection of libraries and tools that enable contribution and release processes for software projects\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\nMINA\\nMultipurpose Infrastructure for Network Application\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIncubating Projects¶\\nThe Apache Incubator is the primary entry path into The Apache Software Foundation for projects and their communities wishing to become part of the Foundation’s efforts. \\n        All code donations from external organisations and existing external projects seeking to join the Apache community enter through the Incubator.\\n\\n\\n\\n\\n\\nMilagro\\n\\nBaremaps\\n\\nTraining\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMilagro\\nMilagro is core security infrastructure and crypto libraries for decentralized networks and distributed systems.\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\nBaremaps\\nApache Baremaps is a toolkit and a set of infrastructure components for creating, publishing, and operating online maps.\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\nTraining\\nThe Training project aims to develop resources which can be used for training purposes in various media formats, languages and for various Apache and non-Apache target projects.\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nApache Project List¶\\n\\n\\n\\n\\n\\nOverview\\nAll Projects\\n\\nBy Category¶\\n\\nAttic\\nBig Data\\nBuild Management\\nCloud\\nContent\\nDatabases\\nFTP\\nGraphics\\nHTTP\\nHTTP-module\\nIncubating\\n\\nJavaEE\\nLibraries\\nMail\\nMobile\\nNetwork-client\\nNetwork-server\\nOSGi\\nRegExp\\nRetired\\nSearch\\nSecurity\\nSQL\\nTesting\\nVirtual-machine\\nWeb-framework\\nXML\\n\\n\\n\\n\\n\\n\\nBy Name¶\\n\\n\\n\\n\\nHTTP Server\\nA\\nAGE\\nAPISIX\\nAccumulo\\nActiveMQ\\nAiravata\\nAirflow\\nAllura\\nAmbari\\nAnt\\nArchiva\\nAries\\nArrow\\nAsterixDB\\nAtlas\\nAttic\\nAvro\\nAxis\\nB\\nBVal\\nBeam\\nBigtop\\nBloodhound\\nBookKeeper\\nBrooklyn\\nBuildStream\\nbRPC\\nC\\nCXF\\nCalcite\\nCamel\\nCarbonData\\nCassandra\\nCauseway\\nCayenne\\nCelix\\nCloudStack\\nCocoon\\n\\n\\n\\n\\nCommons\\nCommunity Development\\nCordova\\nCouchDB\\nCreadur\\nCurator\\ncTAKES\\nD\\nDB\\nDaffodil\\nDataFu\\nDataSketches\\nDeltaSpike\\nDirectory\\nDolphinScheduler\\nDoris\\nDrill\\nDruid\\nDubbo\\nE\\nECharts\\nEmpire-db\\nEventMesh\\nF\\nFelix\\nFineract\\nFlagon\\nFlex\\nFlink\\nFlume\\nFluo\\nFreeMarker\\nG\\nGeode\\nGeronimo\\nGobblin\\nGora\\nGriffin\\nGroovy\\n\\n\\n\\n\\n\\n\\n\\n\\nGuacamole\\nGump\\nH\\nHAWQ\\nHBase\\nHadoop\\nHelix\\nHive\\nHop\\nHttpComponents\\nHudi\\nI\\nIceberg\\nIgnite\\nImpala\\nInLong\\nIncubator\\nIoTDB\\nJ\\nJMeter\\nJSPWiki\\nJackrabbit\\nJames\\nJena\\nJohnzon\\nJuneau\\njclouds\\nK\\nKafka\\nKaraf\\nKibble\\nKnox\\nKudu\\nKvrocks\\nKylin\\nKyuubi\\nL\\nLibcloud\\nLinkis\\n\\n\\n\\n\\nLogging Services\\nLucene\\nLucene.Net\\nM\\nMADlib\\nMINA\\nMahout\\nManifoldCF\\nMaven\\nMesos\\nMnemonic\\nMyFaces\\nMynewt\\nN\\nNetBeans\\nNiFi\\nNutch\\nNuttX\\nO\\nOFBiz\\nORC\\nOlingo\\nOozie\\nOpenDAL\\nOpenJPA\\nOpenMeetings\\nOpenNLP\\nOpenOffice\\nOpenWebBeans\\nOpenWhisk\\nOzone\\nP\\nPDFBox\\nPLC4X\\nPOI\\nParquet\\nPerl\\nPetri\\nPhoenix\\n\\n\\n\\n\\n\\n\\n\\n\\nPig\\nPinot\\nPivot\\nPortable Runtime (APR)\\nPortals\\nPulsar\\nQ\\nQpid\\nR\\nRanger\\nRatis\\nRocketMQ\\nRoller\\nRoyale\\nRya\\nS\\nSINGA\\nSIS\\nSamza\\nSantuario\\nSeaTunnel\\nSedona\\nSerf\\nServiceComb\\nServiceMix\\nShardingSphere\\nShenYu\\nShiro\\nSkyWalking\\nSling\\nSolr\\nSpamAssassin\\nSpark\\nSteve\\nStorm\\nStreamPipes\\nStreams\\nStruts\\nSubmarine\\n\\n\\n\\n\\nSubversion\\nSuperset\\nSynapse\\nSyncope\\nSystemDS\\nT\\nTVM\\nTapestry\\nTcl\\nTez\\nThrift\\nTika\\nTinkerPop\\nTomEE\\nTomcat\\nTraffic Control\\nTraffic Server\\nTsFile\\nTurbine\\nU\\nUIMA\\nUnomi\\nV\\nVCL\\nVelocity\\nW\\nWeb Services\\nWhimsy\\nWicket\\nX\\nXML Graphics\\nXalan\\nXerces\\nY\\nYetus\\nYuniKorn\\nZ\\nZeppelin\\nZooKeeper\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\n\\nContributor Getting Started\\nBecoming a Committer\\nCode of Conduct\\nCommunity Resources\\nCommunity Over Code\\nEvents\\nStore\\n\\n\\n\\nProjects\\n\\nProjects\\nIncubator Projects\\nProjects Directory \\nMailing Lists \\nReport a Vulnerability\\n\\n\\n\\nDownloads\\n\\nDistributions\\nReleases\\nInfrastructure Status\\nInfrastructure Statistics\\n\\n\\n\\nLearn\\n\\nBlog\\nHow the ASF Works\\nThe Apache Way\\nLegal & Trademark\\nLicenses\\nGlossary\\nFAQ\\n\\n\\n\\nResources & Tools\\n\\nDeveloper Information\\nWiki\\nIssues\\nSlack\\nSelf Serve Portal\\nInfrastructure\\nWhimsy\\nBrand Guidelines\\nProject Logos\\n\\n\\n\\nAbout\\n\\nAbout\\nOur Sponsors\\nCorporate Sponsorship\\nIndividual Supporters\\nLeadership\\nMembers\\nDiversity & Inclusion\\nNewsroom\\nContact\\nPrivacy Policy\\n\\n\\n\\n\\n\\n\\nCopyright © 2024 The Apache Software Foundation, Licensed under the Apache License, Version 2.0.\\nApache and the Apache feather logo are trademarks of The Apache Software Foundation. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content=\"Privacy Notice | DatabricksSkip to main contentWhy Databricks DiscoverFor ExecutivesFor Startups Lakehouse Architecture CustomersFeatured StoriesSee All CustomersPartnersCloud ProvidersDatabricks on AWS, Azure, and GCPConsulting & System IntegratorsExperts to build, deploy and migrate to DatabricksTechnology PartnersConnect your existing tools to your LakehouseC&SI Partner ProgramBuild, deploy or migrate to the LakehouseData PartnersAccess the ecosystem of data consumersPartner SolutionsFind custom industry and migration solutionsBuilt on DatabricksBuild, market and grow your businessProduct Databricks PlatformPlatform OverviewA unified platform for data, analytics and AIData ManagementData reliability, security and performanceSharingAn open, secure, zero-copy sharing for all dataData WarehousingETL and orchestration for batch and streaming dataGovernanceUnified governance for all data, analytics and AI assetsReal-Time AnalyticsReal-time analytics, AI and applications made simpleArtificial IntelligenceA data-centric approach to AIData EngineeringETL and orchestration for batch and streaming dataMosaicMLTrain and deploy secure generative AI modelsData ScienceCollaborative data science at scaleIntegrations and DataMarketplaceOpen marketplace for data, analytics and AIIDE IntegrationsBuild on the Lakehouse in your favorite IDEPartner ConnectDiscover and integrate with the Databricks ecosystemPricingDatabricks PricingExplore product pricing, DBUs and moreCost CalculatorEstimate your compute costs on any cloudOpen SourceOpen Source TechnologiesLearn more about the innovations behind the platformSolutions Databricks for IndustriesFinancial ServicesRetailHealthcare & Life SciencesMedia and EntertainmentManufacturingCommunicationsPublic SectorSee All IndustriesCross Industry SolutionsConsumer Data Platform Cyber SecurityMigration & DeploymentData MigrationProfessional ServicesSolution AcceleratorsExplore AcceleratorsMove faster toward outcomes that matterResources Training and CertificationLearning OverviewHub for training, certification, events and moreTraining OverviewDiscover curriculum tailored to your needsLearning PathsGuided learning by role and career pathCertificationGain recognition and differentiationUniversity AllianceWant to teach Databricks? See how.EventsData + AI SummitData + AI World TourEvent CalendarBlog and PodcastsDatabricks BlogExplore news, product announcements, and moreDatabricks Mosaic AI Research BlogDiscover the latest in our Gen AI researchData Brew PodcastLet’s talk data!Champions of Data + AI PodcastInsights from data leaders powering innovationGet HelpCustomer SupportDocumentationCommunityDive DeepResource CenterDemo CenterAbout CompanyWho We AreOur TeamDatabricks VenturesContact UsCareersWorking at DatabricksOpen JobsPressAwards and RecognitionNewsroomSecurity and TrustSecurity and TrustReady to get started?Get a DemoLoginContact UsTry DatabricksSkip to main contentOverviewTermsMaster Cloud Services AgreementAcceptable Use PolicyExternal User TermsUS Public Sector ServicesCommunity Edition Terms of ServicePartner Terms and ConditionsWebsite Terms of UsePrivacyData Processing AddendumAmendment to Data Processing AddendumPrivacy NoticeInternational Data Transfers FAQDatabricks SubprocessorsCookie NoticeApplicant Privacy NoticeSecurityDatabricks SecuritySecurity AddendumTrust CenterCompliance and EthicsCode of ConductThird Party Code of ConductModern Slavery StatementPay Equity Report (France)Subscribe to UpdatesPrivacy NoticeThis Privacy Notice explains how Databricks, Inc. and its affiliates (“Databricks”, “we”, “our”, and “us”) collects, uses, shares and otherwise processes your personal information (also known as personal data) in connection with the use of Databricks websites and applications that link to this Privacy Notice (the “Sites”), our data processing platform products and services (the “Platform Services”) and in the usual course of business, such as in connection with our events, sales, and marketing activities (collectively, “Databricks Services”). It also contains information about your choices and privacy rights.Our ServicesWe provide the Platform Services to our customers and users (collectively, “Customers”) under an agreement with them and solely for their benefit and the benefit of personnel authorized to use the Platform Services (“Authorized Users”). Our processing of such data is governed by our agreement with the relevant Customer. This Privacy Notice does not apply to (i) the data that our Customers upload, submit or otherwise make available to the Platform Services and other data that we process on their behalf, as defined in our agreement with the Customer; (ii) any products, services, websites, or content that are offered by third parties or that have their own privacy notice; or (iii) personal information that we collect and process in connection with our recruitment activities, which is covered under our Applicant Privacy Notice.We recommend that you read this Privacy Notice in full to ensure that you are informed. However, if you only want to access a particular section of this Privacy Notice, you can click on the link below to go to that section.Information We Collect About YouHow We Use Your InformationHow We Share Your InformationInternational TransfersYour Choices and RightsAdditional Information for Certain JurisdictionsOther Important InformationChanges to this NoticeHow to Contact UsInformation We Collect About YouInformation that we collect from or about you includes information you provide, information we collect automatically, and information we receive from other sources.Information you provideWhen using our Databricks Services, we may collect certain information, such as your name, email address, phone number, postal address, job title, and company name. We may also collect other information that you provide through your interactions with us, for example if you request information about our Platform Services, interact with our sales team or contact customer support, complete a survey, provide feedback or post comments, register for an event, or take part in marketing activities.\\xa0We may keep a record of your communications with us and other information you share during the course of the communications.When you create an account, for example, through our Sites or register to use our Platform Services, we may collect your personal information, such as your name and contact information. We may also collect credit card information if chosen by you as a payment method, which may be shared with our third party service providers, including for payment and billing purposes.\\xa0Information we collect automatically\\xa0We use standard automated data collection tools, such as cookies, web beacons, tracking pixels, tags, and similar tools, to collect information about how people use our Sites and interact with our emails.For example, when you visit our Sites we (or an authorized third party) may collect certain information from you or your device. This may include information about your computer or device (such as operating system, device identifier, browser language, and Internet Protocol (IP) address), and information about your activities on our Sites (such as how you came to our Sites, access times, the links you click on, and other statistical information).\\xa0For example, your IP address may be used to derive general location information. We use this information to help us understand how you are using our Sites and how to better provide the Sites to you. We may also use web beacons and pixels in our emails. For example, we may place a pixel in our emails that notifies us when you click on a link in the email. We use these technologies to improve our communications.\\xa0The types of data collection tools we use may change over time as technology evolves. You can learn more about our use of cookies and similar tools, as well as how to opt out of certain data collection, by visiting our\\xa0Cookie Notice.\\xa0When you use our Platform Services, we automatically collect information about how you are using the Platform Services (“Usage Data”). While most Usage Data is not personal information, it may include information about your account (such as User ID, email address, or Internet Protocol (IP) address) and information about your computer or device (such as browser type and operating system). It may also include information about your activities within the Platform Services, such as the pages or features you access or use, the time spent on those pages or features, search terms entered, commands executed, information about the types and size of files analyzed via the Platform Services, and other statistical information relating to your use of the Platform Services. We collect Usage Data to provide, support and operate the Platform Services, for network and information security, and to better understand how our Authorized Users and Customers are using the Platform Services to improve our products and services.\\xa0We may also use the information we collect automatically (for example, IP address, and unique device identifiers) to identify the same unique person across Databricks Services to provide a more seamless and personalized experience to you.\\xa0Information we receive from other sourcesWe may obtain information about you from third party sources, including resellers, distributors, business partners, event sponsors, security and fraud detection services, social media platforms, and publicly available sources. Examples of information that we receive from third parties include marketing and sales information (such as name, email address, phone number and similar contact information), and purchase, support and other information about your interactions with our Sites and Platform Services. We may combine such information with the information we receive and collect from you.How We Use Your InformationWe use your personal information to provide, maintain, improve and update our Databricks Services. Our purposes for collecting your personal information include:to provide, maintain, deliver and update the Databricks Services;to create and maintain your Databricks account;to measure your use and improve Databricks Services, and to develop new products and services;for billing, payment, or account management; for example, to identify your account and correctly identify your usage of our products and services;to provide you with customer service and support;to register and provide you with training and certification programs;to investigate security issues, prevent fraud, or combat the illegal or controlled uses of our products and services;for sales phone calls for training and coaching purposes, quality assurance and administration (in accordance with applicable laws), including to analyze sales calls using analytics tools to gain better insights into our interactions with customers;\\xa0to send you notifications about the Databricks Services, including technical notices, updates, security alerts, administrative messages and invoices;to respond to your questions, comments, and requests, including to keep in contact with you regarding the products and services you use;to tailor and send you newsletters, emails and other content to promote our products and services (you can always unsubscribe from our marketing emails by clicking\\xa0here) and to allow third party partners (like our event sponsors) to send you marketing communications about their services, in accordance with your preferences;to personalize your experience when using our Sites and Platform Services;for advertising purposes; for example, to display and measure advertising on third party websites;to contact you to conduct surveys and for market research purposes;to generate and analyze statistical information about how our Sites and Platform Services are used in the aggregate;for other legitimate interests or lawful business purposes; for example, customer surveys, collecting feedback, and conducting audits;to comply with our obligations under applicable law, legal process, or government regulation; andfor other purposes, where you have given consent.How We Share Your InformationWe may share your personal information with third parties as follows:with our affiliates and subsidiaries for the purposes described in this Privacy Notice;with our service providers who assist us in providing the Databricks Services, such as billing, payment card processing, customer support, sales and marketing, and data analysis, subject to confidentiality obligations and the requirement that those service providers do not sell your personal information;with our service providers who assist us with detecting and preventing fraud, security threats or other illegal or malicious behavior, for example Sift who provides fraud detection services where your personal information is processed by Sift in accordance with its Privacy Notice available at https://sift.com/service-privacy;with third party business partners, such as resellers, distributors, and/or referral partners, who are involved in providing content, products or services to our prospects or Customers. We may also engage with third party partners who are working with us to organize or sponsor an event to which you have registered to enable them to contact you about the event or their services (but only where we have a lawful basis to do so, such as your consent where required by applicable law);with marketing partners, such as advertising providers that tailor online ads to your interests based on information they collect about your online activity (known as interest-based advertising);with the organization that is sponsoring your training or certification program, for example to notify them of your registration and completion of the course;when authorized by law or we deem necessary to comply with a legal process;when required to protect and defend the rights or property of Databricks or our Customers, including the security of our Sites, products and services (including the Platform Services);when necessary to protect the personal safety, property or other rights of the public, Databricks or our Customers;where it has been de-identified, including through aggregation or anonymization;when you instruct us to do so;where you have consented to the sharing of your information with third parties; orin connection with a merger, sale, financing or reorganization of all or part of our business.International TransfersDatabricks may transfer your personal information to countries other than your country of residence. In particular, we may transfer your personal information to the United States and other countries where our affiliates, business partners and services providers are located. These countries may not have equivalent data protection laws to the country where you reside.\\xa0Wherever we process your personal information, we take appropriate steps to ensure it is protected in accordance with this Privacy Notice and applicable data protection laws. These safeguards include implementing the European Commission’s Standard Contractual Clauses for transfers of personal information from the EEA or Switzerland between us and our business partners and service providers, and equivalent measures for transfers of personal information from the United Kingdom. Databricks also offers our Customers the ability to enter into a data processing addendum (DPA) that contains the Standard Contractual Clauses, for transfers between us and our Customers. We also make use of supplementary measures to ensure your information is adequately protected.\\xa0Data Privacy Framework NoticeDatabricks complies with the EU-U.S. Data Privacy Framework (EU-U.S. DPF), the UK Extension to the EU-U.S. DPF, and the Swiss-U.S. Data Privacy Framework (Swiss-U.S. DPF) as set forth by the U.S. Department of Commerce.\\xa0 Databricks has certified to the U.S. Department of Commerce that it adheres to the EU-U.S. Data Privacy Framework Principles (EU-U.S. DPF Principles) with regard to the processing of personal data received from the European Union in reliance on the EU-U.S. DPF and from the United Kingdom (and Gibraltar) in reliance on the UK Extension to the EU-U.S. DPF.\\xa0 Databricks has certified to the U.S. Department of Commerce that it adheres to the Swiss-U.S. Data Privacy Framework Principles (Swiss-U.S. DPF Principles) with regard to the processing of personal data received from Switzerland in reliance on the Swiss-U.S. DPF.\\xa0 If there is any conflict between the terms in this privacy policy and the EU-U.S. DPF Principles and/or the Swiss-U.S. DPF Principles, the Principles shall govern.\\xa0 To learn more about the Data Privacy Framework (DPF) program, and to view our certification, please visit https://www.dataprivacyframework.gov/.\\xa0To learn more, visit our Data Privacy Framework Notice\\xa0here.Your Choices and RightsWe offer you choices regarding the collection, use and sharing of your personal information and we will respect the choices you make in accordance with applicable law. Please note that if you decide not to provide us with certain personal information, you may not be able to access certain features of the Sites or use the Platform Services.Account informationIf you want to correct, update or delete your account information, please log on to your Databricks account and update your profile.Opt out of marketingWe may periodically send you marketing communications that promote our products and services consistent with your choices. You may opt out from receiving such communications, either by following the unsubscribe instructions in the communication you receive or by clicking\\xa0here. Please note that we may still send you important service-related communications regarding our products or services, such as communications about your subscription or account, service announcements or security information.Your privacy rightsDepending upon your place of residence, you may have rights in relation to your personal information. Please review the jurisdiction specific sections below, including the disclosures for California residents. Depending on applicable data protection laws, those rights may include asking us to provide certain information about our collection and processing of your personal information, or requesting access, correction or deletion of your personal information. You also have the right to withdraw your consent, to the extent we rely on consent to process your personal information.\\xa0If you wish to exercise any of your rights under applicable data protection laws, submit a request online by completing the request form here or emailing us at [email\\xa0protected]. We will respond to requests that we receive in accordance with applicable laws. Databricks may take certain steps to verify your request using information available to us, such as your email address or other information associated with your Databricks account, and if needed we may ask you to provide additional information for the purposes of verifying your request. Any information you provide to us for verification purposes will only be used to process and maintain a record of your request.As described above, we may also process personal information that has been submitted by a Customer to our Platform Services. If your personal information has been submitted to the Platform Services by or on behalf of a Databricks Customer and you wish to exercise your privacy rights, please direct your request to the relevant Customer. For other inquiries, please contact us at [email\\xa0protected].Additional Information for Certain JurisdictionsThis section provides additional information about our privacy practices for certain jurisdictions.CaliforniaIf you are a California resident, the California Consumer Privacy Act (“CCPA”) requires us to provide you with additional information regarding your rights with respect to your “personal information. This information is described in our Supplemental Privacy Notice to California Residents.\\xa0\\xa0Other US StatesDepending on applicable laws in your state of residence, you may request to: (1) confirm whether or not we process your personal information; (2) access, correct, or delete personal information we maintain about you; (3) receive a portable copy of such personal information; and/or (4) restrict or opt out of certain processing of your personal information, such as targeted advertising, or profiling in furtherance of decisions that produce legal or similarly significant effects. If we refuse to take action on a request, we will provide instructions on how you may appeal the decision. We will respond to requests consistent with applicable law.European Economic Area, UK and SwitzerlandIf you are located in the European Economic Area, United Kingdom or Switzerland, the controller of your personal information is Databricks, Inc., 160 Spear Street, Suite 1300, San Francisco, CA 94105, United States.\\xa0We only collect your personal information if we have a legal basis for doing so. The legal basis that we rely on depends on the personal information concerned and the specific context in which we collect it. Generally, we collect and process your personal information where:We need it to enter into or perform a contract with you, such as to provide you with the Platform Services, respond to your request, or provide you with customer support;We need to process your personal information to comply with a legal obligation (such as to comply with applicable legal, tax and accounting requirements) or to protect the vital interests of you or other individuals;You give us your consent, such as to receive certain marketing communications; orWhere we have a legitimate interest, such as to respond to your requests and inquiries, to ensure the security of the Sites and Platform Services, to detect and prevent fraud, to maintain, customize and improve the Sites and Platform Services, to promote Databricks and our Platform Services, and to defend our interests and rights.If you have consented to our use of your personal information for a specific purpose, you have the right to change your mind at any time but this will not affect our processing of your information that has already taken place.\\xa0You also have the following rights with respect to your personal information:The right to access, correct, update, or request deletion of your personal information;The right to object to the processing of your personal information or ask that we restrict the processing of your personal information;The right to request portability of your personal information;The right to withdraw your personal information at any time, if we collected and processed your personal information with your consent; andThe right to lodge a complaint with your national data protection authority or equivalent regulatory body.If you wish to exercise any of your rights under data protection laws, please contact us as described under “Your Choices and Rights”.Other Important InformationNotice to Authorized UsersOur Platform Services are intended to be used by organizations. Where the Platform Services are made available to you through an organization (e.g., your employer), that organization is the administrator of the Platform Services and responsible for the accounts and/or services over which it has control. For example, administrators can access and change information in your account or restrict and terminate your access to the Platform Services. We are not responsible for the privacy or security practices of an administrator's organization, which may be different from this Privacy Notice. Please contact your organization or refer to your organization's policies for more information.Data RetentionDatabricks retains the personal information described in this Privacy Notice for as long as you use our Databricks Services, as may be required by law (for example, to comply with applicable legal tax or accounting requirements), as necessary for other legitimate business or commercial purposes described in this Privacy Notice (for example, to resolve disputes or enforce our agreements), or as otherwise communicated to you.SecurityWe are committed to protecting your information. We use a variety of technical, physical, and organizational security measures designed to protect against unauthorized access, alteration, disclosure, or destruction of information. However, no security measures are perfect or impenetrable. As such, we cannot guarantee the security of your information.Third Party ServicesOur Databricks Services may contain links to third party websites, applications, services, or social networks (including co-branded websites or products that are maintained by one of our business partners). We may also make available certain features that allow you to sign into our Sites using third party login credentials (such as LinkedIn, Facebook, Twitter and Google+) or access third party services from our Platform Services (such as Github). Any information that you choose to submit to third party services is not covered by this Privacy Notice. We encourage you to read the terms of use and privacy notices of use of such third party services before sharing your information with them to understand how your information may be collected and used.Children's DataThe Sites and Platform Services are not directed to children under 18 years of age and Databricks does not knowingly collect personal information from children under 18. If we learn that we have collected any personal information from children under 18, we will promptly take steps to delete such information. If you are aware that a child has submitted us such information, please contact us using the details provided below.Changes to this NoticeDatabricks may change this Privacy Notice from time to time. We will post any changes on this page and, if we make material changes, provide a more prominent notice (for example, by adding a statement to the website landing page, providing notice through the Platform Services login screen, or by emailing you). You can see the date on which the latest version of this Privacy Notice was posted below. If you disagree with any changes to this Privacy Notice, you should stop using the Databricks Services and deactivate your Databricks account.\\xa0How to Contact UsPlease contact us at\\xa0[email\\xa0protected]\\xa0if you have any questions about our privacy practices or this Privacy Notice. You can also write to us at Databricks Inc., 160 Spear Street, Suite 1300, San Francisco, CA 94105 Attn: Privacy.If you interact with Databricks through or on behalf of your organization, then your personal information may also be subject to your organization’s privacy practices and you should direct any questions to that organization.Last updated: August 15th, 2023Why DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsWhy DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustDatabricks Inc.\\n160 Spear Street, 13th Floor\\nSan Francisco, CA 94105\\n1-866-330-0121See Careers\\nat Databricks© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the\\xa0Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights\\n\\n\", metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='Terms of Use | DatabricksSkip to main contentWhy Databricks DiscoverFor ExecutivesFor Startups Lakehouse Architecture CustomersFeatured StoriesSee All CustomersPartnersCloud ProvidersDatabricks on AWS, Azure, and GCPConsulting & System IntegratorsExperts to build, deploy and migrate to DatabricksTechnology PartnersConnect your existing tools to your LakehouseC&SI Partner ProgramBuild, deploy or migrate to the LakehouseData PartnersAccess the ecosystem of data consumersPartner SolutionsFind custom industry and migration solutionsBuilt on DatabricksBuild, market and grow your businessProduct Databricks PlatformPlatform OverviewA unified platform for data, analytics and AIData ManagementData reliability, security and performanceSharingAn open, secure, zero-copy sharing for all dataData WarehousingETL and orchestration for batch and streaming dataGovernanceUnified governance for all data, analytics and AI assetsReal-Time AnalyticsReal-time analytics, AI and applications made simpleArtificial IntelligenceA data-centric approach to AIData EngineeringETL and orchestration for batch and streaming dataMosaicMLTrain and deploy secure generative AI modelsData ScienceCollaborative data science at scaleIntegrations and DataMarketplaceOpen marketplace for data, analytics and AIIDE IntegrationsBuild on the Lakehouse in your favorite IDEPartner ConnectDiscover and integrate with the Databricks ecosystemPricingDatabricks PricingExplore product pricing, DBUs and moreCost CalculatorEstimate your compute costs on any cloudOpen SourceOpen Source TechnologiesLearn more about the innovations behind the platformSolutions Databricks for IndustriesFinancial ServicesRetailHealthcare & Life SciencesMedia and EntertainmentManufacturingCommunicationsPublic SectorSee All IndustriesCross Industry SolutionsConsumer Data Platform Cyber SecurityMigration & DeploymentData MigrationProfessional ServicesSolution AcceleratorsExplore AcceleratorsMove faster toward outcomes that matterResources Training and CertificationLearning OverviewHub for training, certification, events and moreTraining OverviewDiscover curriculum tailored to your needsLearning PathsGuided learning by role and career pathCertificationGain recognition and differentiationUniversity AllianceWant to teach Databricks? See how.EventsData + AI SummitData + AI World TourEvent CalendarBlog and PodcastsDatabricks BlogExplore news, product announcements, and moreDatabricks Mosaic AI Research BlogDiscover the latest in our Gen AI researchData Brew PodcastLet’s talk data!Champions of Data + AI PodcastInsights from data leaders powering innovationGet HelpCustomer SupportDocumentationCommunityDive DeepResource CenterDemo CenterAbout CompanyWho We AreOur TeamDatabricks VenturesContact UsCareersWorking at DatabricksOpen JobsPressAwards and RecognitionNewsroomSecurity and TrustSecurity and TrustReady to get started?Get a DemoLoginContact UsTry DatabricksSkip to main contentOverviewTermsMaster Cloud Services AgreementAcceptable Use PolicyExternal User TermsUS Public Sector ServicesCommunity Edition Terms of ServicePartner Terms and ConditionsWebsite Terms of UsePrivacyData Processing AddendumAmendment to Data Processing AddendumPrivacy NoticeInternational Data Transfers FAQDatabricks SubprocessorsCookie NoticeApplicant Privacy NoticeSecurityDatabricks SecuritySecurity AddendumTrust CenterCompliance and EthicsCode of ConductThird Party Code of ConductModern Slavery StatementPay Equity Report (France)Subscribe to UpdatesTerms of UseWebsite Terms of UseThese terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”). These Terms expressly do not govern your access to or use of the Databricks Platform Services (known as Databricks and Databricks Community Edition, each located at *.cloud.databricks.com, or the related website at help.databricks.com and platform support services, together the “Platform Services”), which are subject to the\\xa0Databricks Terms of Service\\xa0(or with respect to the Community Edition, the\\xa0Community Edition Terms of Service) or other written agreement in place between Databricks, Inc. (“Databricks”) and our subscribers (“Subscribers”) (any such agreement, a “Services Agreement”).PLEASE READ CAREFULLY THESE TERMS AND THE\\xa0DATABRICKS PRIVACY POLICY\\xa0(“PRIVACY POLICY”) WHICH IS INCORPORATED BY REFERENCE INTO THESE TERMS. BY ACCESSING OR USING ANY OF THE SITES, YOU REPRESENT THAT YOU ARE AT LEAST 18 YEARS OLD, YOU ACKNOWLEDGE AND AGREE THAT YOU HAVE READ AND UNDERSTOOD THESE TERMS, AND YOU AGREE TO BE LEGALLY BOUND BY ALL OF THESE TERMS. IF YOU DO NOT AGREE TO ALL OF THESE TERMS, DO NOT ACCESS OR USE THE SITES. WE SUGGEST YOU PRINT A COPY OF THESE TERMS FOR YOUR RECORDS.Throughout the Terms, “we,” “us,” “our” and “ours” refer to Databricks, and “you,” “your” or “yours” refer to you personally (i.e., the individual who reads and agrees to be bound by these Terms) and, if you access the Sites on behalf of a legal entity, to that entity. If you are using the Sites on behalf of any entity you represent and warrant that you are authorized to accept these Terms on such entity’s behalf and, by accepting these Terms, you are hereby binding such entity to the Terms.Subject to your compliance with these Terms, solely for so long as you are permitted by Databricks to access and use the Sites, and provided that you keep intact all copyright and other proprietary notices, you may view Content and you may download and print the materials that Databricks specifically makes available for downloading from the Sites (such as white papers or user documentation), in each case solely for informational purposes and solely for personal or internal business use.ACCEPTANCE OF TERMS\\n\\tDatabricks provides the Sites to you conditioned upon your accepting all of the Terms, without modification. Your use of the Sites constitutes your agreement with such Terms. We reserve the right to change, modify, add to, or remove portions of these Terms in our sole discretion at any time and we will, at our sole discretion, either post the modification on\\xa0https://www.databricks.com/terms-of-use\\xa0or provide you with email notice of the modification. You should check these Terms periodically for changes and you can determine when these Terms were last revised by referring to the “Last Updated” reference at the top of these Terms. Any modification shall be effective immediately upon the uploading of modified Terms. You indicate your agreement to comply with, and be bound by, any such modification by continuing to use or access the Sites after modified Terms are posted. If the modified Terms are not acceptable to you, your sole recourse is to discontinue your use of the Sites.\\n\\n\\t\\xa0If you have registered for and opened an account through the Sites (an “Account”), you are entirely responsible for maintaining the confidentiality of your Account information, including your password, and for any and all activity that occurs under your Account. You agree to notify Databricks immediately of any unauthorized use of your Account or password, or any other breach of security. However, you will remain responsible for losses incurred by Databricks or by any other party due to your knowingly or inadvertently permitting unauthorized use of your Account or your Account information. You may not use anyone else’s ID, password or account at any time unless we expressly pre-approve such use, or unless expressly permitted under a Services Agreement. Databricks cannot and will not be liable for any loss or damage arising from your failure to comply with these obligations. Registration for any account is void where the user lacks the requisite eligibility for registration or if such registration is otherwise prohibited.Software (“Software”) or the Platform Services may be made available to you through the Sites. Your rights to access and use the Platform Services, including any Software will be subject to your agreement to the applicable Services Agreement governing your use of the Platform Services and to any terms and conditions of any applicable third party software license agreement (“Software License”) identified in the Software or on the web page providing access to the Software. You may not use any Software unless you agree to be bound by all terms and conditions of any applicable Software License. If there is a conflict between any Services Agreement and any Software License, the conflicting term of the Software License shall control but only to the extent necessary to eliminate the conflict.LICENSE GRANT AND PROPRIETARY RIGHTS\\n\\tProvided that you fully comply at all times with these Terms and any other policies or restrictions posted on or transmitted through the Sites, Databricks grants you a limited, non-exclusive, non-transferable, revocable license to access and use the Sites. Except as otherwise specifically noted in these Terms or on the Sites, the Software, Submissions (as later defined), and all other information, content, user interfaces, graphics, registered or unregistered trademarks, logos, images, artwork, videos, and documents, and the design, structure, selection, coordination, expression, “look and feel” and arrangement of such materials, made available through the Sites (collectively, the “Content”), regardless of its source or creation, is owned, controlled or licensed by or to Databricks, and is protected by trade dress, copyright, patent and trademark laws, and various other intellectual property rights and unfair competition laws, and Databricks reserves and retains all rights in and to such Content. Any reproduction, redistribution or other use or exploitation of Software in violation of any applicable Software License or in violation of any license granted under these Terms or, if applicable, under a Services Agreement, is expressly prohibited by law, and may result in civil and criminal penalties.\\n\\t\\xa0“Apache” and “Spark” are trademarks of the Apache Software Foundation. Any other third party trademarks, service marks, logos, trade names or other proprietary designations, that are or may become present within the Sites, including within any Content, are the registered or unregistered trademarks of the respective parties.Except solely as necessary for you to access the Sites for the intended purpose pursuant to these Terms, you may not copy, collect, modify, create derivative works or uses of, translate, distribute, transmit, publish, re-publish, perform, display, post, download, upload, sublicense, transfer, dispose of, resell or sell the Content or any other part of the Services. Except as expressly set forth in these Terms, these Terms do not grant to you any license to any intellectual property rights or other proprietary rights, including any implied licenses or licenses granted by estoppel or otherwise.INFORMATION SUBMITTED THROUGH OR TO OUR SITES\\n\\tAt our sole discretion, you may be permitted to provide Submissions (as defined in the next sentence) to the Sites (e.g., through our forums). “Submissions” are defined to include: any messages, emails, text, graphics, code, questions, suggestions, comments, feedback, ideas, plans, notes, drawings, sample data, sound, images, video, original or creative materials, and other items or materials that you may provide to discussion forums, blogs, or other interactive features or areas of the Services where you or other users can create, post, transmit or store Content. Unless otherwise specifically agreed to by you and Databricks, by uploading, e-mailing, posting, publishing or otherwise transmitting any Submission, you hereby acknowledge that such Submission is non-confidential and you automatically grant (or warrant that the owner of such rights has expressly granted) to Databricks a perpetual, irrevocable, worldwide, non-exclusive, sub-licensable, fully paid-up and royalty-free license to use, make, have made, offer for sale, sell, copy, distribute, perform, display (whether publicly or otherwise), modify, adapt, publish, transmit and otherwise exploit such Submission, by means of any form, medium, or technology now known or later developed, and to grant to others rights to do any of the foregoing. In addition, you warrant that all so-called moral rights in such Submission have been waived.\\n\\t\\xa0For each Submission, you represent and warrant that you have all rights necessary for you to grant the license granted in the prior paragraph, and that such Submission, and your provision thereof to and through the Sites, does not violate any privacy, publicity, contractual, intellectual property, or other right or rights of any person or entity or otherwise violate any applicable laws, rules or regulations. You acknowledge that Databricks may have ideas or materials already under consideration or development that are or may be similar to your Submissions and that you are not entitled to any form of compensation or reimbursement from Databricks in connection with your Submissions. You agree to be fully responsible for, and to pay any and all royalties, fees, damages, and any other monies owing any person or entity by reason of, any Submission you provide to the Sites. We reserve the right to terminate access to all or any part of the Sites for anyone we suspect to be an infringer of our or any third party’s intellectual property rights of any kind whatsoever.You agree that you will not, and will not allow or authorize any third party to, post Submissions containing:Anything that is or may be (a) threatening, harassing, degrading, abusive or hateful; (b) an incitement to violence, terrorism or other wrongdoing; (c) defamatory or libelous; (d) invasive of privacy rights; (e) fraudulent, deceptive, impersonating of any person or entity, or misrepresentative of your affiliation with any person or entity; (f) obscene, pornographic, indecent, grotesque or otherwise objectionable; or (g) protected by copyright, trademark, confidentiality obligations, or other proprietary or privacy right without the express prior written consent of the owner of such right.Any material, the posting or usage of which would give rise to criminal or civil liability, or cause violation of any rules or regulations, or that encourages conduct that constitutes a criminal offense.Any virus, worm, Trojan horse or other computer code, file, data or program that is harmful, disruptive, corrupted, or invasive, or may be or is intended to damage or hijack the operation of any hardware or software.Any information identifiable to a particular individual, including but not limited to addresses, phone numbers, email addresses, birthdates, Social Security numbers and other government-issued identification numbers, payment card and other financial account numbers or login credentials, and health information.Any unsolicited or unauthorized advertising, promotional materials, junk mail, spam, chain letter, pyramid scheme, political campaign message, offering of an investment opportunity, or any other form of solicitation.Any material with respect to which you do not have all rights, power and authority necessary for its collection, use and processing, or where your use and provision to the Sites would breach any agreement between you and any third party.Databricks generally does not pre-screen or monitor Submissions (but reserves the right to do so) and does not control Submissions. Therefore, Databricks does not guarantee the accuracy, quality or appropriateness of Submissions and disclaims any responsibility for Submissions, including any liability for errors or omissions, or for any loss or damage of any kind incurred as a result of their use. However, Databricks reserves the right at its sole discretion to refuse, delete, screen or edit Submissions, provided that even if we do remove or alter any Submission, we shall have no obligation to stop our other uses of such Submission or any other Submission as permitted above. We have no obligation to store any of your Submissions. We have no responsibility or liability for the deletion or failure to store, transmit or receive your Submissions, nor do we have any responsibility for the security, privacy, storage or transmission of other communications originating with or involving your use of the Sites, except as may be expressly stated in these Terms or in the Privacy Policy. You are solely responsible for creating backup copies of and replacing any Submissions at your sole cost and expense. Our Privacy Policy governs your Submissions.By accepting these Terms, you agree to our collection, use, and disclosure of your information as described in the Privacy Policy. No one under age 18 may register for an Account or provide any personal information to Databricks or to the Sites. If we learn that we have collected personal information from or about anyone under age 18, we will delete that information as quickly as possible. If you believe that we might have any information from or about a child under 18, please contact us at\\xa0[email\\xa0protected]\\xa0with the subject “Child Data“.Databricks reserves the right to disclose any Submissions, and the circumstances surrounding their transmission, to any third party to operate the Sites, to protect Databricks or its suppliers or representatives, to protect users of the Sites, to comply with legal or regulatory obligations, to enforce these Terms, or for any other reason. Databricks is not responsible or liable for the conduct of, or your interactions with, any other users of the Sites (whether online or offline), or for any associated loss, damage, injury or harm. By using the Site, you may be exposed to Submissions that are offensive, indecent or objectionable and you agree that Databricks bears no liability for such exposure.REQUIRED CONDUCT WHILE USING OUR SITES\\n\\tWhile using the Sites you will comply with all applicable laws, rules and regulations. In addition, Databricks expects users of the Sites to respect the rights and dignity of others. Your use of the Sites is conditioned on your compliance with the rules of conduct set forth in this Section; any failure to comply may also result in termination of your access to the Sites pursuant to Section 9 (Suspension or Termination of Access to Our Sites). In using the Sites, you agree that you will not, and will not allow or authorize any third party to:\\n\\t\\xa0Use the Sites or any Content for any purpose that is illegal, fraudulent, deceptive or unauthorized by these Terms, or would give rise to civil liability, or to solicit the performance of any illegal activity or other activity which infringes the rights of Databricks or others, or to encourage or promote any such activity.Engage in or promote any conduct that is offensive, harassing, predatory, stalking, violent, threatening, discriminatory, racist, hateful, or otherwise harmful, against any individual or group.Harvest or collect information about any third parties, including their email addresses or other personally identifiable information.Send, by email or other means, any unsolicited or unauthorized advertising, promotional materials, junk mail, spam, chain letter, pyramid scheme, political campaign message, offering of an investment opportunity, or any other form of solicitation, or conceal or forge headers of emails or other messages, or otherwise misrepresent the identity of senders, for the purpose of sending spam or other unsolicited messages.Impersonate or post on behalf of, or express or imply the endorsement of, any individual or entity, including Databricks or any of its representatives, or otherwise misrepresent your affiliation with a person or entity.Use the Sites in any manner, whether deliberate or otherwise, including without limitation a denial of service attack, that could in any way (a) interfere with, damage, disable, overburden or impair the functioning of the Sites, or Databricks’ systems or networks, or any systems or networks connected to the Sites, or (b) violate any requirements, procedures, policies or regulations of such systems or networks.Operate non-permissioned network services, including open proxies, mail relays or recursive domain name servers, or use any means to bypass user limitations relating to the Sites.Use any robot, spider, crawler, scraper, deep-link, page-scrape, site search/retrieval application or other manual or automated device, program, algorithm or methodology or interface not provided by us to access, acquire, copy, retrieve, index, scrape, data mine, in any way reproduce or circumvent the navigational structure or presentation of the Sites or monitor any portion of the Sites or to extract data, or to sell, resell, frame, mirror or otherwise exploit for any commercial purpose, any portion of, use of, or access to the Sites (including any Content, Software and other materials available through the Sites), or attempt to circumvent any content filtering techniques we may employ.Remove any copyright, trademark or other proprietary rights notice from the Sites or from Content or other materials contained on or originating from the Sites.Create a database of any type by systematically downloading and storing any Content unless expressly permitted by Databricks to do so.Attempt to gain unauthorized access to any portion or feature of the Sites, or any other systems or networks connected to the Sites or to any Databricks server, or to any of the services offered on or through the Sites, by hacking, password mining or any other illegitimate means.Use or attempt to use any account you are not authorized to use.Probe, scan, monitor or test the vulnerability of the Sites or any network connected to the Sites, or breach the security or authentication measures on the Sites or any network connected to the Sites.Modify, adapt, create derivative works of, translate, reverse engineer, decompile or disassemble any portion of the Sites (including any Content or other materials available through the Sites), or do anything that might discover source code or bypass or circumvent measures employed to prevent or limit access to any area, Content or code within the Sites except as, and solely to the extent, expressly authorized under applicable law overriding any of these restrictions.Develop any third-party applications that interact with the Sites or Content without our prior written consent.Use or apply the Sites in any manner directly or indirectly competitive with any business of Databricks.\\xa0LINKS\\n\\tWe may from time-to-time at our discretion host or provide links to services, products, web pages, websites or other content of third parties (“Third-Party Content”). The inclusion of any link to, or the hosting of, any Third Party Content is provided solely as a convenience to our users, including you, and does not imply affiliation, endorsement, approval, control or adoption by us of the Third-Party Content. We make no claims or representations regarding, and accept no responsibility or liability for, Third-Party Content including without limitation its quality, accuracy, nature, ownership or reliability. Your use of Third-Party Content is at your own risk. When you leave the Sites to access Third Party Content via a link, you should be aware that our policies, including the Privacy Policy, no longer govern. You should review the applicable terms and policies, including privacy and data gathering policies, of any website to which you navigate from the Sites.DISCLAIMER OF WARRANTIES\\n\\tYOU EXPRESSLY AGREE THAT YOUR USE OF THE SITES, INCLUDING ANY CONTENT, IS AT YOUR SOLE RISK. ALL OF THE SITES AND CONTENT ARE PROVIDED TO YOU ON AN “AS IS” AND “AS AVAILABLE” BASIS, AND DATABRICKS MAKES NO RELATED REPRESENTATIONS, AND DISCLAIMS ALL POSSIBLE WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. WE DO NOT WARRANT THAT THE SITES OR CONTENT ARE ACCURATE, CONTINUOUSLY AVAILABLE, COMPLETE, RELIABLE, SECURE, CURRENT, ERROR-FREE, OR FREE OF VIRUSES OR OTHER HARMFUL COMPONENTS. DATABRICKS CANNOT AND DOES NOT GUARANTEE THAT ANY DEFECTS, ERRORS OR OMISSIONS WILL BE CORRECTED, REGARDLESS OF WHETHER DATABRICKS IS AWARE OF SUCH DEFECTS, ERRORS OR OMISSIONS.\\n\\t\\xa0TO THE EXTENT APPLICABLE STATE LAW DOES NOT ALLOW THE EXCLUSIONS AND DISCLAIMERS OF WARRANTIES AS SET FORTH IN THIS SECTION 6, SOME OR ALL OF THE ABOVE EXCLUSIONS AND DISCLAIMERS MAY NOT APPLY TO YOU, IN WHICH CASE SUCH EXCLUSIONS AND DISCLAIMERS WILL APPLY TO THE FULLEST EXTENT PERMITTED BY APPLICABLE LAW. YOU ACKNOWLEDGE THAT THE DISCLAIMERS, LIMITATIONS, AND WAIVERS OF LIABILITY SET FORTH IN THIS SECTION 6 SHALL SURVIVE ANY EXPIRATION OR TERMINATION OF THESE TERMS OR YOUR USE OF THE SITES.LIMITATION OF LIABILITY\\n\\tYOU ACKNOWLEDGE AND AGREE THAT, TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE ENTIRE RISK ARISING OUT OF YOUR ACCESS TO AND USE OF THE SITES AND CONTENT REMAINS WITH YOU. IN NO EVENT WILL DATABRICKS OR ANY OF ITS DIRECTORS, EMPLOYEES, AGENTS OR SUPPLIERS BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL, EXEMPLARY, CONSEQUENTIAL OR PUNITIVE DAMAGES OF ANY KIND (INCLUDING, BUT NOT LIMITED TO, LOSS OF USE, LOSS OF BUSINESS, LOSS OF PROFITS, LOSS OF DATA, LOSS OF GOODWILL, SERVICE INTERRUPTION, COMPUTER DAMAGE, SYSTEM FAILURE OR THE COST OF SUBSTITUTE PRODUCTS OR SERVICES) ARISING OUT OF OR IN CONNECTION WITH THE SITES, AND ANY CONTENT, SERVICES OR PRODUCTS INCLUDED ON OR OTHERWISE MADE AVAILABLE THROUGH THE SITES, REGARDLESS OF THE FORM OF ACTION (WHETHER IN CONTRACT, TORT, STRICT LIABILITY, EQUITY OR OTHERWISE) AND EVEN IF WE ARE AWARE OF OR HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\\n\\t\\xa0IN NO EVENT WILL OUR TOTAL CUMULATIVE LIABILITY TO YOU ARISING OUT OF OR IN CONNECTION WITH THESE TERMS, OR FROM THE USE OF OR INABILITY TO USE THE SITES, INCLUDING ANY CONTENT, OR FROM THE USE OF OR EXPOSURE TO ANY SUBMISSIONS, EXCEED ONE HUNDRED DOLLARS ($100.00). MULTIPLE CLAIMS WILL NOT EXPAND THIS LIMITATION.THE FOREGOING LIMITATIONS AND EXCLUSIONS SHALL NOT APPLY WITH RESPECT TO ANY LIABILITY ARISING UNDER FRAUD, FRAUDULENT MISREPRESENTATION, GROSS NEGLIGENCE, OR ANY OTHER LIABILITY THAT CANNOT BE LIMITED OR EXCLUDED BY LAW. ADDITIONALLY, TO THE EXTENT APPLICABLE STATE OR OTHER LAW DOES NOT ALLOW THE EXCLUSIONS AND LIMITATIONS OF DAMAGES AS SET FORTH IN THIS SECTION 7, SOME OR ALL OF THE ABOVE EXCLUSIONS AND LIMITATIONS MAY NOT APPLY TO YOU, IN WHICH CASE DATABRICKS’ LIABILITY TO YOU WILL BE LIMITED BY THIS SECTION TO THE FULLEST EXTENT PERMITTED BY APPLICABLE LAW.THIS SECTION WILL BE GIVEN FULL EFFECT EVEN IF ANY REMEDY SPECIFIED IN THESE TERMS IS DEEMED TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THESE LIMITATIONS OF LIABILITY FORM AN ESSENTIAL BASIS OF THE BARGAIN BETWEEN THE PARTIES. YOU ACKNOWLEDGE THAT THE LIMITATIONS OF LIABILITY SET FORTH IN THIS SECTION 7 SHALL SURVIVE ANY TERMINATION OR EXPIRATION OF THESE TERMS OR YOUR USE OF THE SITES.INDEMNIFICATION\\n\\tTo the fullest extent permitted by law, you agree to indemnify, defend and hold harmless Databricks, its officers, directors, shareholders, successors in interest, employees, agents, subsidiaries and affiliates, from and against any and all actual or threatened third party claims (groundless or otherwise), demands, losses, damages, costs and liability, proceedings (at law or in equity) and expenses (including reasonable attorneys’ and expert fees and costs of investigation) arising out of or in connection with (a) your use of the Sites, including without limitation any of your Submissions, (b) your breach of these Terms, including your breach of any covenant, representation, warranty, term, or condition set forth herein, including, without limitation, the obligations set forth in Section 3 (Information Submitted Through Our Sites) and Section 4 (Required Conduct While Using Our Sites), (c) your violation of any law or regulation or of any third party rights, including infringement, libel, misappropriation, or other violation of any third party’s intellectual property or other legal rights or (d) the disclosure, solicitation or use of any personal information by you, whether with or without your knowledge or consent. Databricks reserves the right, however, to assume the exclusive defense and control of any matter otherwise subject to indemnification by you and, in such case, you agree to cooperate with Databricks’ defense of such claim, and in no event may you agree to any settlement affecting Databricks without Databricks’ prior written consent.SUSPENSION OR TERMINATION OF ACCESS TO OUR SITES\\n\\tNotwithstanding any provision to the contrary in these Terms, you agree that Databricks may, in its sole discretion and with or without prior notice, for any or no reason, suspend or terminate your access to any or all of the Sites and/or block your future access to any or all of the Sites, including without limitation for any of the following reasons: (a) if we determine that you have violated any provision, or the spirit, of these Terms, (b) in response to a request by a law enforcement or other government agency, (c) due to discontinuance or material modification of any of the Sites, or (d) due to unexpected technical issues or problems. Databricks shall not be liable to you or any third party for any termination of your access to any part of the Sites. The rights and obligations of these Terms which by their nature should survive, shall so survive any termination of your use of the Sites.CONTACT\\n\\tQuestions or comments about the Terms or about the Sites may be directed to Databricks at the email address [email\\xa0protected]. You may also email us at that address if you would like to report what you believe to be a violation of these Terms. However, please note that we do not accept any responsibility to maintain the confidentiality of any report of a violation you may submit to us, including your identity, nor do we commit to providing a personal reply to any report you submit, nor are we obligated to take action in response to your report.CLAIMS OF COPYRIGHT INFRINGEMENT\\n\\tDatabricks respects the intellectual property rights of others and we request that the people who use the Sites do the same. The Digital Millennium Copyright Act of 1998 (the “DMCA”) provides recourse for copyright owners who believe that material appearing on the Internet infringes their rights under U.S. copyright law. If you believe in good faith that materials available on the Sites infringe your copyright, you (or your agent) may send Databricks a notice requesting that we remove the material or block access to it. If you believe in good faith that someone has wrongly filed a notice of copyright infringement against you, you may send a counter-notice to Databricks under applicable provisions of the DMCA. Please note that substantial penalties under U.S. copyright law may be levied against any filer of a false counter-notice. Notices and counter-notices must meet the then-current statutory requirements imposed by the DMCA. See 17 U.S.C. § 512(c)(3), available at https://www.copyright.gov/title17/92chap5.html for details. Notices and counter-notices should be sent to:\\n\\t\\xa0Attn: Legal Department/DMCA Copyright Agent\\n\\tDatabricks, Inc.\\n\\t160 Spear Street, Suite 1300\\n\\tSan Francisco, CA 94105\\n[email\\xa0protected]\\n\\t(866) 330-0121You should note that if you knowingly misrepresent in your notification that the material or activity is infringing, you will be liable for any damages, including costs and attorneys’ fees, incurred by us or the alleged infringer as the result of our relying upon such misrepresentation in removing or disabling access to the material or activity claimed to be infringing. We encourage you to consult your legal advisor before filing a notice or counter-notice.In accordance with the DMCA and other applicable law, Databricks may at our discretion limit access to the Sites and/or terminate the accounts of any users who infringe any intellectual property rights of others, whether or not there is any repeat infringement.GENERAL\\n\\tThe Terms and the relationship between each user and Databricks shall be governed by the laws of the State of California without regard to its conflict of law provisions and each party shall submit to the personal and exclusive jurisdiction of the courts located in San Francisco, California. The application of the United Nations Convention on Contracts for the International Sale of Goods is expressly excluded. Except to the extent a Services Agreement applies, these Terms, along with the Privacy Policy, constitute the entire agreement between you and Databricks with respect to your use of the Sites and supersede all prior or contemporaneous communications and proposals, whether electronic, oral or written, between you and Databricks with respect to the Sites. If any provision of the Terms is found by a court of competent jurisdiction to be invalid, the parties nevertheless agree that the court should endeavor to give effect to the parties’ intentions as reflected in the provision, and the other provisions of the Terms remain in full force and effect. A party may only waive its rights under these Terms by a written document executed by both parties. Databricks’ failure to insist on or enforce strict performance of these Terms shall not be construed as a waiver by Databricks of any provision or any right it has to enforce these Terms, nor shall any course of conduct between Databricks and you or any other party be deemed to modify any provision of these Terms. The headings of the sections of these Terms are for convenience of reference only and are not intended to restrict, affect or be of any weight in the interpretation or construction of the provisions of such sections.\\n\\t\\xa0None of your rights or duties under these Terms may be transferred, assigned or delegated by you without our prior written consent, and any attempted transfer, assignment or delegation without such consent will be void and without effect. We may freely transfer, assign or delegate any of our rights or duties under these Terms. Subject to the foregoing, these Terms will be binding upon and will inure to the benefit of the parties and their respective representatives, heirs, administrators, successors and permitted assigns. No provision of these Terms is intended for the benefit of any third party, and the parties do not intend that any provision should be enforceable by a third party. Our relationship is an independent contractor relationship, and neither these Terms nor any actions by either party may be interpreted as creating an agency or partnership relationship. Nothing in these Terms shall be construed to obligate Databricks to enter into or engage with you on any commercial transaction.If you are provided access to any Software, you acknowledge that such Software may be subject to regulation by local laws and United States government agencies which prohibit export or diversion of certain products or information about products to certain countries and certain persons. You represent and warrant that you will not export or re-export such Software in violation of these regulations.You acknowledge that your breach of any of the provisions of these Terms may cause immediate and irreparable harm to Databricks for which we may not have an adequate remedy in money damages. We will therefore be entitled to obtain an injunction against such breach from any court of competent jurisdiction immediately upon request and will be entitled to recover from you the costs incurred in seeking such an injunction. The availability or exercise of our right to obtain injunctive relief will not limit our right to seek or obtain any other remedy.You agree that we will not be liable for delays, failures, or inadequate performance of the Sites resulting from conditions outside of our reasonable control, including but not limited to natural disasters or other acts of God, failure of telecommunications networks or any other network or utility, threatened or actual acts of terrorism or war, riots, labor strikes, or governmental acts or orders.Last Updated May 25, 2018Why DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsWhy DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustDatabricks Inc.\\n160 Spear Street, 13th Floor\\nSan Francisco, CA 94105\\n1-866-330-0121See Careers\\nat Databricks© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the\\xa0Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights\\n\\n', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'})]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "96NEs-jKgj7E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyJWxX6Tgn3-",
        "outputId": "e18ea28f-7d97-4b60-9942-f6ed2353f11a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='The Data and AI Company — DatabricksSkip to main contentWhy Databricks DiscoverFor ExecutivesFor Startups Lakehouse Architecture CustomersFeatured StoriesSee All CustomersPartnersCloud ProvidersDatabricks on AWS, Azure, and GCPConsulting & System IntegratorsExperts to build, deploy and migrate to DatabricksTechnology PartnersConnect your existing tools to your LakehouseC&SI Partner ProgramBuild, deploy or migrate to the LakehouseData PartnersAccess the ecosystem of data consumersPartner SolutionsFind custom industry and migration solutionsBuilt on DatabricksBuild, market and grow your businessProduct Databricks PlatformPlatform OverviewA unified platform for data, analytics and AIData ManagementData reliability, security and performanceSharingAn open, secure, zero-copy sharing for all dataData WarehousingETL and orchestration for batch and streaming dataGovernanceUnified governance for all data, analytics and AI assetsReal-Time AnalyticsReal-time analytics, AI and applications made', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='applications made simpleArtificial IntelligenceA data-centric approach to AIData EngineeringETL and orchestration for batch and streaming dataMosaicMLTrain and deploy secure generative AI modelsData ScienceCollaborative data science at scaleIntegrations and DataMarketplaceOpen marketplace for data, analytics and AIIDE IntegrationsBuild on the Lakehouse in your favorite IDEPartner ConnectDiscover and integrate with the Databricks ecosystemPricingDatabricks PricingExplore product pricing, DBUs and moreCost CalculatorEstimate your compute costs on any cloudOpen SourceOpen Source TechnologiesLearn more about the innovations behind the platformSolutions Databricks for IndustriesFinancial ServicesRetailHealthcare & Life SciencesMedia and EntertainmentManufacturingCommunicationsPublic SectorSee All IndustriesCross Industry SolutionsConsumer Data Platform Cyber SecurityMigration & DeploymentData MigrationProfessional ServicesSolution AcceleratorsExplore AcceleratorsMove faster toward outcomes', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='toward outcomes that matterResources Training and CertificationLearning OverviewHub for training, certification, events and moreTraining OverviewDiscover curriculum tailored to your needsLearning PathsGuided learning by role and career pathCertificationGain recognition and differentiationUniversity AllianceWant to teach Databricks? See how.EventsData + AI SummitData + AI World TourEvent CalendarBlog and PodcastsDatabricks BlogExplore news, product announcements, and moreDatabricks Mosaic AI Research BlogDiscover the latest in our Gen AI researchData Brew PodcastLet’s talk data!Champions of Data + AI PodcastInsights from data leaders powering innovationGet HelpCustomer SupportDocumentationCommunityDive DeepResource CenterDemo CenterAbout CompanyWho We AreOur TeamDatabricks VenturesContact UsCareersWorking at DatabricksOpen JobsPressAwards and RecognitionNewsroomSecurity and TrustSecurity and TrustReady to get started?Get a DemoLoginContact UsTry DatabricksYour data. Your AI.Your', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='data. Your AI.Your future.Own them all on the new data intelligence platformExplore demosLearn moreOn-demand WebinarImplement generative AI faster, from RAG to pre-training, with a data-centric approachWatch nowPLATFORMThe DatabricksData Intelligence PlatformDatabricks brings AI to your data to help you bring AI to the world.Succeed with AIDevelop generative AI applications on your data without sacrificing data privacy or control.Democratize insightsEmpower everyone in your organization to discover insights from your data using natural language.Drive down costsGain efficiency and simplify complexity by unifying your approach to data, AI and governance.Explore the platformUSE CASESUnify all your data + AIAIGovernanceWarehousingETLData sharingOrchestrationBuild better AI with a data-centric approachGreat models are built with great data. With Databricks, lineage, quality, control and data privacy are maintained across the entire AI workflow, powering a complete set of tools to deliver', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='of tools to deliver any AI use case.✔ Create, tune and deploy your own generative AI models✔\\xa0Automate experiment tracking and governance✔\\xa0Deploy and monitor models at scaleSee howSchedule demoCUSTOMER STORIESIndustry leaders are data + AI companiesOur customers achieve breakthroughs, innovate faster and drive down costs. See how you can too.2YRhigh ROI attainedJetBlue soars on data + AIWith over 40 million customers and 1,000 daily flights, JetBlue is leveraging the power of LLMs and Gen AI to optimize operations, grow new and existing revenue sources, reduce flight delays and enhance efficiency.See the full story$6Min infrastructure cost savingsCondé Nast crafts bespoke content with data + AICondé Nast aims to deliver personalized content to every consumer across\\xa0their\\xa037 brands. Unity Catalog and Databricks SQL drive faster analysis and decision-making, ensuring Condé Nast is providing compelling customer experiences at the right time.See the full story2PBof data governed by Unity', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='governed by Unity CatalogBlock redefines financial services with data + AIWith brands like Square, Cash App and Afterpay, Block is unifying data + AI on Databricks, including LLMs that will provide customers with easier access to financial opportunities for economic growth.See the full storySee more Customer StoriesTOOLS AND INTEGRATIONSPlug into what you already useSpeed up success in data + AIThe Databricks Data Intelligence Platform integrates with your current tools for ETL, data ingestion, business intelligence, AI and governance. Adopt what’s next without throwing away what works.Browse integrationsRESOURCESMore than meets the AIGet helpEverything you need to succeed on lakehouseSupportTrainingCommunitySee what’s newOur latest announcements, expert analyses and eventsBlogNewsEventsIn the spotlight', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='REPORTStrategies to transition to the new stage of data + AIRead nowREPORTDatabricks named a Leader in the 2023 Gartner®️ Magic Quadrant™️ for CDBMSRead nowEBOOKData-centric MLOps and LLMOpsRead nowEBOOKBig Book of Data Engineering: 2nd EditionRead nowSee more resourcesReady to become a data + AI company?Take the first steps in your transformationBrowse demosTry it freeWhy DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsWhy DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='ServicesSolution AcceleratorsResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustDatabricks Inc.', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='160 Spear Street, 13th Floor\\nSan Francisco, CA 94105\\n1-866-330-0121See Careers\\nat Databricks© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the\\xa0Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights', metadata={'source': 'https://www.databricks.com/', 'title': 'The Data and AI Company — Databricks', 'description': 'The Databricks Platform is the world’s first data intelligence platform powered by generative AI. Infuse AI into every facet of your business.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks Community\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoadingÃ—Sorry to interruptCSS ErrorRefresh', metadata={'source': 'https://help.databricks.com', 'title': 'Databricks Community', 'language': 'en-US'}),\n",
              " Document(page_content='Try Databricks | Databricks\\n\\nTry Databricks free Test-drive the full Databricks platform free for 14 days on your choice of AWS, Microsoft Azure or Google Cloud. Sign-up with your work email to elevate your trial experience.Simplify data ingestion and automate ETLIngest data from hundreds of sources. Use a simple declarative approach to build data pipelines.Collaborate in your preferred languageCode in Python, R, Scala and SQL with coauthoring, automatic versioning, Git integrations\\xa0and RBAC.12x better price/performance than cloud data warehousesSee why over 7,000 customers worldwide rely on Databricks for all their workloads from BI to AI.Create your Databricks account1/2Sign up with your work email to elevate your trial with expert assistance and more.First nameLast nameEmailCompanyTitlePhone (Optional)SelectCountryContinuePrivacy Notice (Updated)Terms of UseYour Privacy ChoicesYour California Privacy Rights', metadata={'source': 'https://databricks.com/try-databricks', 'title': 'Try Databricks | Databricks', 'description': 'Discover why businesses are turning to Databricks to accelerate innovation. Try Databricks’ Full Platform Trial free for 14 days!', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks Community\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLoadingÃ—Sorry to interruptCSS ErrorRefresh', metadata={'source': 'https://help.databricks.com/s/', 'title': 'Databricks Community', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks documentation | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive', metadata={'source': 'https://docs.databricks.com', 'title': 'Databricks documentation | Databricks on AWS', 'description': 'How-to guidance and reference information for data analysts, data scientists, and data engineers working in the Databricks Data Science & Engineering, Databricks Machine Learning, and Databricks SQL environments.', 'language': 'en-US'}),\n",
              " Document(page_content='Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks documentation\\n\\n\\n\\n\\n\\n\\n\\nDatabricks documentation \\nDatabricks documentation provides how-to guidance and reference information for data analysts, data scientists, and data engineers solving problems in analytics and AI. The Databricks Data Intelligence Platform enables data teams to collaborate on data stored in the lakehouse. See What is a data lakehouse?.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTry Databricks \\n\\nGet a free trial & set up\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild a simple lakehouse analytics pipeline\\nFree training\\n\\n\\n\\nWhat do you want to do? \\n\\nData science & engineering\\nMachine learning\\nSQL queries & visualizations\\n\\n\\n\\nManage Databricks \\n\\nAccount & workspace administration\\nSecurity & compliance\\nData governance\\n\\n\\n\\nReference Guides \\n\\nAPI reference\\nSQL language reference\\nError handling and error messages\\n\\n\\n\\n\\nResources \\n\\nRelease notes\\nOther resources', metadata={'source': 'https://docs.databricks.com', 'title': 'Databricks documentation | Databricks on AWS', 'description': 'How-to guidance and reference information for data analysts, data scientists, and data engineers working in the Databricks Data Science & Engineering, Databricks Machine Learning, and Databricks SQL environments.', 'language': 'en-US'}),\n",
              " Document(page_content='© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'https://docs.databricks.com', 'title': 'Databricks documentation | Databricks on AWS', 'description': 'How-to guidance and reference information for data analysts, data scientists, and data engineers working in the Databricks Data Science & Engineering, Databricks Machine Learning, and Databricks SQL environments.', 'language': 'en-US'}),\n",
              " Document(page_content=\"Databricks Knowledge Base\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks Knowledge Base\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMain Navigation\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\nCommunity\\nTraining\\nFeedback\\n\\n\\n\\n\\n\\n\\nWelcome to the Databricks Knowledge Base\\n\\n\\n\\nAll Categories\\nAWS\\nAzure\\nGCP\\nAll articles\\nTraining\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContact Us\\nIf you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible.\\n\\n\\nYour email address\\n\\nSubject\\n\\nDescription\\n\\nPlease enter the details of your request. A member of our support staff will respond as soon as possible.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStart Here\\nSearch & Browse the Databricks Knowledge Base.\\n\\n\\n\\n\\nAWS\\nTopics on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\nAzure\\nTopics on Azure\\n\\n\\n\\n\\n\\n\\n\\n\\nGCP\\nTopics on GCP\\n\\n\\n\\n\\n\\n\\n\\nHelp Topics\\nWhether it's a very specific question, or a vague one, we have it covered in our knowledge base.\\n\\n\\n\\n\\n\\n\\nAWS\\n\\n\\n\\n\\n\\n\\n\\nAzure\\n\\n\\n\\n\\n\\n\\n\\nGCP\\n\\n\\n\\n\\n\\n\\n\\nAll articles\\n\\n\\n\\n\\n\\n\\n\\nTraining\\n15 Articles by 1 Authors\", metadata={'source': 'https://kb.databricks.com/', 'title': 'Databricks Knowledge Base', 'description': 'Databricks Support Center helps you to find FAQ, how-to guides and step-by-step tutorials.', 'language': ''}),\n",
              " Document(page_content='Popular Articles\\n\\n\\nUnexpected cluster termination\\nSometimes a cluster is terminated unexpectedly, not as a result of a manual termi...\\n\\n\\nSpark job fails with Driver is temporarily unavailable\\nProblem When running notebooks or jobs on a cluster, they run successfully multip...\\n\\n\\nApache Spark job fails with Parquet column cannot be converted error\\nProblem You are reading data in Parquet format and writing to a Delta table when ...\\n\\n\\nHow to improve performance of Delta Lake MERGE INTO queries using partition pruning\\nThis article explains how to trigger partition pruning in Delta Lake MERGE INTO (...\\n\\n\\nCreate a DataFrame from a JSON string or Python dictionary\\nIn this article we are going to review how you can create an Apache Spark DataFra...\\n\\n\\nAppend to a DataFrame\\nTo append to a DataFrame, use the union method. %scala val firstDF = spark.range(...', metadata={'source': 'https://kb.databricks.com/', 'title': 'Databricks Knowledge Base', 'description': 'Databricks Support Center helps you to find FAQ, how-to guides and step-by-step tutorials.', 'language': ''}),\n",
              " Document(page_content=\"Get and set Apache Spark configuration properties in a notebook\\nIn most cases, you set the Spark config (AWS | Azure ) at the cluster level. Howe...\\n\\n\\nPrevent duplicated columns when joining two DataFrames\\nIf you perform a join in Spark and don’t specify your join correctly you’ll end u...\\n\\n\\nGenerate unique increasing numeric values\\nThis article shows you how to use Apache Spark functions to generate unique incre...\\n\\n\\nForbidden error while accessing S3 data\\nProblem While trying to access S3 data using DBFS mount or directly in Spark APIs...\\n\\n\\n\\n\\nCan't Find What you're Looking for?\\nFeel free to suggest an article topic by clicking on the button below.\\n\\nOffer feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n            © Databricks 2022-2023. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\r\\n        \\n\\nSend us feedback\\r\\n        \\r\\n       | Privacy Notice (Updated) | Terms of Use | Your Privacy Choices | Your California Privacy Rights\", metadata={'source': 'https://kb.databricks.com/', 'title': 'Databricks Knowledge Base', 'description': 'Databricks Support Center helps you to find FAQ, how-to guides and step-by-step tutorials.', 'language': ''}),\n",
              " Document(page_content='Definition by Author\\n\\n\\n\\n0\\n\\n\\n\\n\\n\\n0', metadata={'source': 'https://kb.databricks.com/', 'title': 'Databricks Knowledge Base', 'description': 'Databricks Support Center helps you to find FAQ, how-to guides and step-by-step tutorials.', 'language': ''}),\n",
              " Document(page_content='Get started: Account and workspace setup | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/getting-started/index.html', 'title': 'Get started: Account and workspace setup | Databricks on AWS', 'description': 'Learn how to set up a Databricks free trial and a cloud provider account with AWS.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup\\n\\n\\n\\n\\n\\n\\n\\nGet started: Account and workspace setup \\nIf you’re new to Databricks, you’ve found the place to start. This article walks you through the minimum steps required to create your account and get your first workspace up and running.\\nFor information about online training resources, see Get free Databricks training.\\n\\n\\n\\nRequirements \\nTo use your Databricks account on AWS, you need an existing AWS account. If you don’t have an AWS account, you can sign up for an AWS Free Tier account at https://aws.amazon.com/free/.', metadata={'source': 'http://docs.databricks.com/getting-started/index.html', 'title': 'Get started: Account and workspace setup | Databricks on AWS', 'description': 'Learn how to set up a Databricks free trial and a cloud provider account with AWS.', 'language': 'en-US'}),\n",
              " Document(page_content='Step 1: Sign up for a free trial \\nYou can sign up for your free Databricks trial either on the AWS Marketplace or through the Databricks website. The only difference between the two is where you’ll handle the account billing after the free trial ends.\\nFor detailed instructions on the free trial and billing, see Databricks free trial.\\n\\n\\nStep 2: Create your first Databricks workspace \\nAfter you sign up for the free trial, you’re prompted to set up your first workspace using the AWS Quick Start. This automated template is the recommended method for workspace creation. It creates Databricks-enabled AWS resources for you so you can get your workspace up and running quickly.\\nFor instructions on the AWS Quick Start, see Create a workspace using the AWS Quick Start (Recommended).\\nIf you’re more familiar with AWS and want to create your own AWS resources, see Manually create a workspace (existing Databricks accounts).', metadata={'source': 'http://docs.databricks.com/getting-started/index.html', 'title': 'Get started: Account and workspace setup | Databricks on AWS', 'description': 'Learn how to set up a Databricks free trial and a cloud provider account with AWS.', 'language': 'en-US'}),\n",
              " Document(page_content='Step 3: Explore and use the Databricks platform \\nAt this point, you have a functional Databricks workspace. To learn how to navigate the platform, see Navigate the workspace. To jump in and start querying data, run the Tutorial: Query data with notebooks tutorial.\\n\\n\\nNext steps \\nYour next steps depend on whether you want to continue setting up your account organization and security or want to start building out data pipelines:\\n\\nTo onboard data to your workspace in Databricks SQL, see Load data using streaming tables in Databricks SQL.\\nTo continuing building out your account organization and security, follow the steps in Get started with Databricks administration.', metadata={'source': 'http://docs.databricks.com/getting-started/index.html', 'title': 'Get started: Account and workspace setup | Databricks on AWS', 'description': 'Learn how to set up a Databricks free trial and a cloud provider account with AWS.', 'language': 'en-US'}),\n",
              " Document(page_content='Get help \\nIf you have any questions about setting up Databricks and need live help, please e-mail onboarding-help@databricks.com.\\nIf you have a Databricks support package, you can open and manage support cases with Databricks. See Learn how to use Databricks support.\\nIf your organization does not have a Databricks support subscription, or if you are not an authorized contact for your company’s support subscription, you can get answers to many questions in Databricks Office Hours or from the Databricks Community.\\nIf you need additional help, sign up for a live weekly demo to ask questions and practice alongside Databricks experts. Or, follow this blog series on best practices for managing and maintaining your environments.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/getting-started/index.html', 'title': 'Get started: Account and workspace setup | Databricks on AWS', 'description': 'Learn how to set up a Databricks free trial and a cloud provider account with AWS.', 'language': 'en-US'}),\n",
              " Document(page_content='What is Databricks? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\n  What is a data lakehouse?\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations\\n\\n\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Reference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Databricks?\\n\\n\\n\\n\\n\\n\\n\\nWhat is Databricks? \\nDatabricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. The Databricks Data Intelligence Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='How does a data intelligence platform work? \\nDatabricks uses generative AI with the data lakehouse to understand the unique semantics of your data. Then, it automatically optimizes performance and manages infrastructure to match your business needs.\\nNatural language processing learns your business’s language, so you can search and discover data by asking a question in your own words. Natural language assistance helps you write code, troubleshoot errors, and find answers in documentation.\\nFinally, your data and AI applications can rely on strong governance and security. You can integrate APIs such as OpenAI without compromising data privacy and IP control.\\n\\n\\nWhat is Databricks used for? \\nDatabricks provides tools that help you connect your sources of data to one platform to process, store, share, analyze, model, and monetize datasets with solutions from BI to generative AI.\\nThe Databricks workspace provides a unified interface and tools for most data tasks, including:', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Data processing scheduling and management, in particular ETL\\nGenerating dashboards and visualizations\\nManaging security, governance, high availability, and disaster recovery\\nData discovery, annotation, and exploration\\nMachine learning (ML) modeling, tracking, and model serving\\nGenerative AI solutions\\n\\n\\n\\nManaged integration with open source \\nDatabricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects originally created by Databricks employees:\\n\\nDelta Lake and Delta Sharing\\nMLflow\\nApache Spark and Structured Streaming\\nRedash\\n\\n\\n\\nTools and programmatic access \\nDatabricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:\\n\\nWorkflows\\nUnity Catalog\\nDelta Live Tables\\nDatabricks SQL\\nPhoton compute clusters', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='In addition to the workspace UI, you can interact with Databricks programmatically with the following tools:\\n\\nREST API\\nCLI\\nTerraform\\n\\n\\n\\nHow does Databricks work with AWS? \\nThe Databricks platform architecture comprises two primary parts:\\n\\nThe infrastructure used by Databricks to deploy, configure, and manage the platform and services.\\nThe customer-owned infrastructure managed in collaboration by Databricks and your company.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Unlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.\\nUnity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\\nDatabricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='What are common use cases for Databricks? \\nUse cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.\\n\\n\\nBuild an enterprise data lakehouse \\nThe data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See What is a data lakehouse?.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='ETL and data engineering \\nWhether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.\\nDelta Live Tables simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks provides a number of custom tools for data ingestion, including Auto Loader, an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Machine learning, AI, and data science \\nDatabricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including MLflow and Databricks Runtime for Machine Learning.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Large language models and generative AI \\nDatabricks Runtime for Machine Learning includes libraries like Hugging Face Transformers that allow you to integrate existing pre-trained models or other open-source libraries into your workflow. The Databricks MLflow integration makes it easy to use the MLflow tracking service with transformer pipelines, models, and processing components. In addition, you can integrate OpenAI models or solutions from partners like John Snow Labs in your Databricks workflows.\\nWith Databricks, you can customize a LLM on your data for your specific task. With the support of open source tooling, such as Hugging Face and DeepSpeed, you can efficiently take a foundation LLM and start training with your own data to have more accuracy for your domain and workload.\\nIn addition, Databricks provides AI functions that SQL data analysts can use to access LLM models, including from OpenAI, directly within their data pipelines and workflows. See AI Functions on Databricks.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Data warehousing, analytics, and BI \\nDatabricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as SQL warehouses, allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the SQL query editor or in notebooks. Notebooks support Python, R, and Scala in addition to SQL, and allow users to embed the same visualizations available in dashboards alongside links, images, and commentary written in markdown.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Data governance and secure data sharing \\nUnity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.\\nUnity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See What is Unity Catalog?.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='The lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of Delta Sharing.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='DevOps, CI/CD, and task orchestration \\nThe development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations. Workflows schedule Databricks notebooks, SQL queries, and other arbitrary code. Repos let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see Developer tools and guidance.', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Real-time and streaming analytics \\nDatabricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See Streaming on Databricks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS', 'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US'}),\n",
              " Document(page_content='Best practice articles | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nPlatform administration cheat sheet\\nCompute creation cheat sheet\\nProduction job scheduling cheat sheet\\n\\n\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/getting-started/tutorials/index.html', 'title': 'Best practice articles | Databricks on AWS', 'description': 'Explore best practice articles to help you make the most out of Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nBest practice articles\\n\\n\\n\\n\\n\\n\\n\\nBest practice articles \\nThis article provides a reference of best practice articles you can use to optimize your Databricks activity.\\n\\n\\nThe Databricks documentation includes a number of best practices articles to help you get the best performance at the lowest cost when using and administering Databricks.', metadata={'source': 'http://docs.databricks.com/getting-started/tutorials/index.html', 'title': 'Best practice articles | Databricks on AWS', 'description': 'Explore best practice articles to help you make the most out of Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Cheat sheets \\nCheat sheets provide you with a high-level view of practices you should be implementing in your Databricks account and workflows. Each cheat sheet includes a table of best practices, their impact, and helpful resources. Available cheat sheets include the following:\\n\\nPlatform administration cheat sheet\\nCompute creation cheat sheet\\nProduction job scheduling cheat sheet\\n\\n\\n\\nBest practice articles \\nThe following articles provide you with best practice guidance for various Databricks features.\\n\\nDelta Lake best practices\\nHyperparameter tuning with Hyperopt\\nDeep learning in Databricks\\nRecommendations for MLOps\\nUnity Catalog best practices\\nCluster configuration best practices\\nInstance pool configuration best practices\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/getting-started/tutorials/index.html', 'title': 'Best practice articles | Databricks on AWS', 'description': 'Explore best practice articles to help you make the most out of Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks release notes | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\nDatabricks platform release notes\\nDatabricks Runtime release notes versions and compatibility\\nDatabricks SQL release notes\\nDatabricks developer tools and SDKs release notes\\nDatabricks Connect release notes\\nDelta Live Tables release notes\\nDatabricks preview releases\\n\\n\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/release-notes/index.html', 'title': 'Databricks release notes | Databricks on AWS', 'description': 'Learn about Databricks releases for the Databricks platform, the Databricks Runtime, Databricks SQL, Delta Live Tables, and more.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks release notes\\n\\n\\n\\n\\n\\n\\n\\nDatabricks release notes \\nDatabricks release notes are organized by release vehicle:', metadata={'source': 'http://docs.databricks.com/release-notes/index.html', 'title': 'Databricks release notes | Databricks on AWS', 'description': 'Learn about Databricks releases for the Databricks platform, the Databricks Runtime, Databricks SQL, Delta Live Tables, and more.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks platform release notes cover the features that we develop for the Databricks environment.\\nDatabricks Runtime release notes versions and compatibility cover the features that we develop for Databricks Runtime. This includes proprietary features and optimizations. A Databricks Runtime version includes the set of core components that run on the clusters managed by Databricks. Each new verion provides updates that substantially improve the usability, performance, and security of big data analytics.\\nDatabricks SQL release notes cover the features that we develop for the Databricks SQL user interface and SQL warehouses. You can also find the available Databricks SQL versions, the rollout schedule, and features related to each version.\\nDatabricks developer tools and SDKs release notes cover the features for our IDE extensions and plugins, command-line interfaces, and SDKs.', metadata={'source': 'http://docs.databricks.com/release-notes/index.html', 'title': 'Databricks release notes | Databricks on AWS', 'description': 'Learn about Databricks releases for the Databricks platform, the Databricks Runtime, Databricks SQL, Delta Live Tables, and more.', 'language': 'en-US'}),\n",
              " Document(page_content='Delta Live Tables release notes cover the features, bug fixes, and runtime upgrade process for Delta Live Tables.', metadata={'source': 'http://docs.databricks.com/release-notes/index.html', 'title': 'Databricks release notes | Databricks on AWS', 'description': 'Learn about Databricks releases for the Databricks platform, the Databricks Runtime, Databricks SQL, Delta Live Tables, and more.', 'language': 'en-US'}),\n",
              " Document(page_content='Learn about the kinds of preview releases and how Databricks supports them.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/release-notes/index.html', 'title': 'Databricks release notes | Databricks on AWS', 'description': 'Learn about Databricks releases for the Databricks platform, the Databricks Runtime, Databricks SQL, Delta Live Tables, and more.', 'language': 'en-US'}),\n",
              " Document(page_content='Load data into a Databricks lakehouse | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nOnboard data from S3\\nAdd data UI\\nCOPY INTO\\nAuto Loader\\n\\n\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources', metadata={'source': 'http://docs.databricks.com/ingestion/index.html', 'title': 'Load data into a Databricks lakehouse | Databricks on AWS', 'description': 'Learn about the different ways to ingest data into a lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Reference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nLoad data into a Databricks lakehouse\\n\\n\\n\\n\\n\\n\\n\\nLoad data into a Databricks lakehouse \\nDatabricks offers a variety of ways to help you load data into a lakehouse backed by Delta Lake. Databricks recommends using Auto Loader for incremental data ingestion from cloud object storage. The add data UI provides a number of options for quickly uploading local files or connecting to external data sources.\\n\\n\\n\\nRun your first ETL workload \\nIf you haven’t used Auto Loader on Databricks, start with a tutorial. See Run your first ETL workload on Databricks.', metadata={'source': 'http://docs.databricks.com/ingestion/index.html', 'title': 'Load data into a Databricks lakehouse | Databricks on AWS', 'description': 'Learn about the different ways to ingest data into a lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Auto Loader \\nAuto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without additional setup. Auto Loader provides a Structured Streaming source called cloudFiles. Given an input directory path on the cloud file storage, the cloudFiles source automatically processes new files as they arrive, with the option of also processing existing files in that directory.\\n\\n\\nAutomate ETL with Delta Live Tables and Auto Loader \\nYou can simplify deployment of scalable, incremental ingestion infrastructure with Auto Loader and Delta Live Tables. Note that Delta Live Tables does not use the standard interactive execution found in notebooks, instead emphasizing deployment of infrastructure ready for production.\\n\\nTutorial: Run your first ETL workload on Databricks\\nLoad data using streaming tables (Python/SQL notebook)\\n\\n\\nLoad data using streaming tables in Databricks SQL', metadata={'source': 'http://docs.databricks.com/ingestion/index.html', 'title': 'Load data into a Databricks lakehouse | Databricks on AWS', 'description': 'Learn about the different ways to ingest data into a lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Upload local data files or connect external data sources \\nYou can securely upload local data files or ingest data from external sources to create tables. See Load data using the add data UI.\\n\\n\\nLoad data into Databricks using third-party tools \\nDatabricks validates technology partner integrations that enable you to load data into Databricks. These integrations enable low-code, scalable data ingestion from a variety of sources into Databricks. See Technology partners. Some technology partners are featured in Databricks Partner Connect, which provides a UI that simplifies connecting third-party tools to your lakehouse data.\\n\\n\\nCOPY INTO \\nCOPY INTO allows SQL users to idempotently and incrementally load data from cloud object storage into Delta tables. It can be used in Databricks SQL, notebooks, and Databricks Jobs.\\n\\n\\nWhen to use COPY INTO and when to use Auto Loader \\nHere are a few things to consider when choosing between Auto Loader and COPY INTO:', metadata={'source': 'http://docs.databricks.com/ingestion/index.html', 'title': 'Load data into a Databricks lakehouse | Databricks on AWS', 'description': 'Learn about the different ways to ingest data into a lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='If you’re going to ingest files in the order of thousands, you can use COPY INTO. If you are expecting files in the order of millions or more over time, use Auto Loader. Auto Loader requires fewer total operations to discover files compared to COPY INTO and can split the processing into multiple batches, meaning that Auto Loader is less expensive and more efficient at scale.\\nIf your data schema is going to evolve frequently, Auto Loader provides better primitives around schema inference and evolution. See Configure schema inference and evolution in Auto Loader for more details.\\nLoading a subset of re-uploaded files can be a bit easier to manage with COPY INTO. With Auto Loader, it’s harder to reprocess a select subset of files. However, you can use COPY INTO to reload the subset of files while an Auto Loader stream is running simultaneously.', metadata={'source': 'http://docs.databricks.com/ingestion/index.html', 'title': 'Load data into a Databricks lakehouse | Databricks on AWS', 'description': 'Learn about the different ways to ingest data into a lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='For an even more scalable and robust file ingestion experience, Auto Loader enables SQL users to leverage streaming tables. See Load data using streaming tables in Databricks SQL.\\n\\nFor a brief overview and demonstration of Auto Loader, as well as COPY INTO, watch the following YouTube video (2 minutes).\\n\\n\\xa0\\n\\n\\nReview file metadata captured during data ingestion \\nApache Spark automatically captures data about source files during data loading. Databricks lets you access this data with the File metadata column.\\n\\n\\nUpload spreadsheet exports to Databricks \\nUse the Create or modify table from file upload page to upload CSV, TSV, or JSON files. See Create or modify a table using file upload.\\n\\n\\nMigrate data applications to Databricks \\nMigrate existing data applications to Databricks so you can work with data from many source systems on a single platform. See Migrate data applications to Databricks.', metadata={'source': 'http://docs.databricks.com/ingestion/index.html', 'title': 'Load data into a Databricks lakehouse | Databricks on AWS', 'description': 'Learn about the different ways to ingest data into a lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/ingestion/index.html', 'title': 'Load data into a Databricks lakehouse | Databricks on AWS', 'description': 'Learn about the different ways to ingest data into a lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Exploratory data analysis on Databricks: Tools and techniques | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nExplore storage and find data files\\nExplore database objects\\nView frequent queries and users of a table\\nEDA\\nVisualizations in Databricks SQL\\nVisualizations in notebooks\\nVisualization types\\nPreview chart visualizations\\nNo-code EDA with bamboolib\\n\\n\\nSample datasets\\n\\n\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/exploratory-data-analysis/index.html', 'title': 'Exploratory data analysis on Databricks: Tools and techniques | Databricks on AWS', 'description': 'Learn about tools and techniques for doing exploratory data analysis (EDA) on Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDiscover data \\nExploratory data analysis on Databricks: Tools and techniques\\n\\n\\n\\n\\n\\n\\n\\nExploratory data analysis on Databricks: Tools and techniques \\nThis article describes tools and techniques for exploratory data analysis (EDA) on Databricks.', metadata={'source': 'http://docs.databricks.com/exploratory-data-analysis/index.html', 'title': 'Exploratory data analysis on Databricks: Tools and techniques | Databricks on AWS', 'description': 'Learn about tools and techniques for doing exploratory data analysis (EDA) on Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='What is EDA and why is it useful? \\nExploratory data analysis (EDA) includes methods for exploring data sets to summarize their main characteristics and identify any problems with the data. Using statistical methods and visualizations, you can learn about a data set to determine its readiness for analysis and inform what techniques to apply for data preparation. EDA can also influence which algorithms you choose to apply for training ML models.\\n\\n\\nWhat are the EDA tools in Databricks? \\nDatabricks has built-in analysis and visualization tools in both Databricks SQL and in Databricks Runtime. For an illustrated list of the types of visualizations available in Databricks, see Visualization types.\\n\\nEDA in Databricks SQL \\nHere are some helpful articles about data visualization and exploration tools in Databricks SQL:\\n\\nVisualize queries and create a dashboard in Databricks SQL\\nCreate data visualizations in Databricks SQL', metadata={'source': 'http://docs.databricks.com/exploratory-data-analysis/index.html', 'title': 'Exploratory data analysis on Databricks: Tools and techniques | Databricks on AWS', 'description': 'Learn about tools and techniques for doing exploratory data analysis (EDA) on Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='EDA in Databricks Runtime \\nDatabricks Runtime provides a pre-built environment that has popular data exploration libraries already installed. You can see the list of the built-in libraries in the release notes.\\nIn addition, the following articles show examples of visualization tools in Databricks Runtime:\\n\\nCreate data visualizations in Databricks notebooks\\nDo no-code EDA with bamboolib\\n\\nIn a Databricks Python notebook, you can combine SQL and Python to explore data. When you run code in a SQL language cell in a Python notebook, the table results are automatically made available as a Python DataFrame. For details, see Explore SQL cell results in Python notebooks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/exploratory-data-analysis/index.html', 'title': 'Exploratory data analysis on Databricks: Tools and techniques | Databricks on AWS', 'description': 'Learn about tools and techniques for doing exploratory data analysis (EDA) on Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Introduction to data preparation in Databricks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'}),\n",
              " Document(page_content='Reference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nIntroduction to data preparation in Databricks\\n\\n\\n\\n\\n\\n\\n\\nIntroduction to data preparation in Databricks \\nThis article describes how Databricks can help you with data preparation for analytics and machine learning. Data preparation is typically the most time-consuming component of an analytics and machine learning project, and good data is important to ensure accurate and useful results.\\n\\nData preparation tasks \\nData preparation includes the following tasks:\\n\\nCleaning and formatting data. This includes tasks such as handling missing values or outliers, ensuring data is in the correct format, and removing unneeded columns.\\nPreprocessing data. This includes tasks like numerical transformations, aggregating data, encoding text or image data, and creating new features.\\nCombining data. This includes tasks like joining tables or merging datasets.', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'}),\n",
              " Document(page_content='Data preparation resources and information \\nThe Databricks platform provides a unified platform for data ingestion, preparation, analytics and machine learning, and monitoring.', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'}),\n",
              " Document(page_content='The medallion lakehouse architecture guides you in data preparation by specifying a set of data layers of increasing quality. The architecture maintains ACID guarantees as data passes through multiple layers of validations and transformations before being stored in a layout optimized for efficient analytics.\\nDelta Live Tables is a framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data, and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.\\nDatabricks Partner Connect lets you connect your Databricks workspace directly to third-party data preparation and transformation partners. Partner Connect provisions the required Databricks resources on your behalf, then passes resource details to the partner.', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks Runtime and Databricks Runtime ML provide pre-built environments that come with many of the most widely used data preparation libraries already installed. A list of all built-in libraries is available in the release notes.\\nFeature engineering for machine learning is the process of converting raw data into features that can be used to develop machine learning models. For ML applications, Databricks Feature Store helps your team discover and re-use features, track feature lineage, and publish features to online stores for realtime serving and automatic lookup.', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'}),\n",
              " Document(page_content='© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/data-preparation/index.html', 'title': 'Introduction to data preparation in Databricks | Databricks on AWS', 'description': 'This article provides an introduction to tools and techniques for data preparation in Databricks. Learn about tools and processes to streamline data preparation.', 'language': 'en-US'}),\n",
              " Document(page_content='Share data securely using Delta Sharing | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Load & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nSet up Delta Sharing for your account (for providers)\\nDatabricks-to-Databricks sharing\\nOpen sharing\\nCreate and manage recipients\\nCreate and manage shares\\nGrant access to shares\\nAccess data shared with you\\nRead shared data (Databricks-to-Databricks)\\nRead shared data (open sharing)\\nAudit data sharing (for recipients)\\nAudit data sharing (for providers)\\nManage providers (for recipients)\\nUse IP access lists to restrict access\\nTroubleshoot common sharing errors\\n\\n\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nShare data securely using Delta Sharing\\n\\n\\n\\n\\n\\n\\n\\nShare data securely using Delta Sharing \\nThis article introduces Delta Sharing in Databricks, the secure data sharing platform that lets you share data in Databricks with users outside your organization, whether those users use Databricks or not.\\n\\nImportant\\nThe Delta Sharing articles on this site focus on sharing Databricks data and notebooks. Delta Sharing is also available as an open-source project that you can use to share Delta tables from other platforms. Delta Sharing also provides the backbone for Databricks Marketplace, an open forum for exchanging data products.\\n\\n\\nNote\\nIf you are a data recipient who has been granted access to shared data through Delta Sharing, and you just want to learn how to access that data, see Access data shared with you using Delta Sharing (for recipients).', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='What is Delta Sharing? \\nDelta Sharing is an open protocol developed by Databricks for secure data sharing with other organizations regardless of the computing platforms they use.\\nThere are three ways to share data using Delta Sharing:\\n\\nThe Databricks-to-Databricks sharing protocol, which lets you share data from your Unity Catalog-enabled workspace with users who also have access to a Unity Catalog-enabled Databricks workspace.\\nThis approach uses the Delta Sharing server that is built into Databricks. It supports some Delta Sharing features that are not suppported in the other protocols, including notebook sharing, Unity Catalog volume sharing, Unity Catalog model sharing, Unity Catalog data governance, auditing, and usage tracking for both providers and recipients. The integration with Unity Catalog simplifies setup and governance for both providers and recipients and improves performance.\\nSee Share data using the Delta Sharing Databricks-to-Databricks protocol (for providers).', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='The Databricks open sharing protocol, which lets you share tabular data that you manage in a Unity Catalog-enabled Databricks workspace with users on any computing platform.\\nThis approach uses the Delta Sharing server that is built into Databricks and is useful when you manage data using Unity Catalog and want to share it with users who don’t use Databricks or don’t have access to a Unity Catalog-enabled Databricks workspace. The integration with Unity Catalog on the provider side simplifies setup and governance for providers.\\nSee Share data using the Delta Sharing open sharing protocol (for providers).\\n\\nA customer-managed implementation of the open-source Delta Sharing server, which lets you share from any platform to any platform, whether Databricks or not.\\nThe Databricks documentation does not cover instructions for setting up your own Delta Sharing server. See github.com/delta-io/delta-sharing.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Shares, providers, and recipients \\nThe primary concepts underlying Delta Sharing in Databricks are shares, providers, and recipients.\\n\\n\\nWhat is a share? \\nIn Delta Sharing, a share is a read-only collection of tables and table partitions that a provider wants to share with one or more recipients. If your recipient uses a Unity Catalog-enabled Databricks workspace, you can also include notebook files, views (including dynamic views that restrict access at the row and column level), Unity Catalog volumes, and Unity Catalog models in a share.\\nYou can add or remove tables, views, volumes, models, and notebook files from a share at any time, and you can assign or revoke data recipient access to a share at any time.\\nIn a Unity Catalog-enabled Databricks workspace, a share is a securable object registered in Unity Catalog. If you remove a share from your Unity Catalog metastore, all recipients of that share lose the ability to access it.\\nSee Create and manage shares for Delta Sharing.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='What is a provider? \\nA provider is an entity that shares data with a recipient. If you are a provider and you want to take advantage of the built-in Databricks Delta Sharing server and manage shares and recipients using Unity Catalog, you need at least one Databricks workspace that is enabled for Unity Catalog. You do not need to migrate all of your existing workspaces to Unity Catalog. You can simply create a new Unity Catalog-enabled workspace for your Delta Sharing needs.\\nIf a recipient is on a Unity Catalog-enabled Databricks workspace, the provider is also a Unity Catalog securable object that represents the provider organization and associates that organization with a set of shares.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='What is a recipient? \\nA recipient is an entity that receives shares from a provider. In Unity Catalog, a share is a securable object that represents an organization and associates it with a credential or secure sharing identifier that allows that organization to access one or more shares.\\nAs a data provider (sharer), you can define multiple recipients for any given Unity Catalog metastore, but if you want to share data from multiple metastores with a particular user or group of users, you must define the recipient separately for each metastore. A recipient can have access to multiple shares.\\nIf a provider deletes a recipient from their Unity Catalog metastore, that recipient loses access to all shares it could previously access.\\nSee Create and manage data recipients for Delta Sharing.\\n\\n\\n\\nOpen sharing versus Databricks-to-Databricks sharing \\nThis section describes the two protocols for sharing from a Databricks workspace that is enabled for Unity Catalog.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Note\\nThis section assumes that the provider is on a Unity Catalog-enabled Databricks workspace. To learn about setting up an open-source Delta Sharing server to share from a non-Databricks platform or non-Unity Catalog workspace, see github.com/delta-io/delta-sharing.\\n\\nThe way a provider uses Delta Sharing in Databricks depends on who they are sharing data with:\\n\\nOpen sharing lets you share data with any user, whether or not they have access to Databricks.\\nDatabricks-to-Databricks sharing lets you share data with Databricks users whose workspace is attached to a Unity Catalog metastore that is different from yours. Databricks-to-Databricks also supports notebook, volume, and model sharing, which is not available in open sharing.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='What is open Delta Sharing? \\nIf you want to share data with users outside of your Databricks workspace, regardless of whether they use Databricks, you can use open Delta Sharing to share your data securely. As a data provider, you generate a token and share it securely with the recipient. They use the token to authenticate and get read access to the tables you’ve included in the shares you’ve given them access to.\\nRecipients can access the shared data using many computing tools and platforms, including:\\n\\nDatabricks\\nApache Spark\\nPandas\\nPower BI\\n\\nFor a full list of Delta Sharing connectors and information about how to use them, see the Delta Sharing documentation.\\nSee also Share data using the Delta Sharing open sharing protocol (for providers).', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='What is Databricks-to-Databricks Delta Sharing? \\nIf you want to share data with users who have a Databricks workspace that is enabled for Unity Catalog, you can use Databricks-to-Databricks Delta Sharing. Databricks-to-Databricks sharing lets you share data with users in other Databricks accounts, whether they’re on AWS, Azure, or GCP. It’s also a great way to securely share data across different Unity Catalog metastores in your own Databricks account. Note that there is no need to use Delta Sharing to share data between workspaces attached to the same Unity Catalog metastore, because in that scenario you can use Unity Catalog itself to manage access to data across workspaces.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='One advantage of Databricks-to-Databricks sharing is that the share recipient doesn’t need a token to access the share, and the provider doesn’t need to manage recipient tokens. The security of the sharing connection—including all identity verification, authentication, and auditing—is managed entirely through Delta Sharing and the Databricks platform. Another advantage is the ability to share Databricks notebook files, views, Unity Catalog volumes, and Unity Catalog models.\\nSee also Share data using the Delta Sharing Databricks-to-Databricks protocol (for providers).', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='How do provider admins set up Delta Sharing? \\nThis section gives an overview of how providers can enable Delta Sharing and initiate sharing from a Unity Catalog-enabled Databricks workspace. For open-source Delta Sharing, see github.com/delta-io/delta-sharing.\\nDatabricks-to-Databricks sharing between Unity Catalog metastores in the same account is always enabled. If you are a provider who wants to enable Delta Sharing to share data with Databricks workspaces in other accounts or non-Databricks clients, a Databricks account admin or metastore admin performs the following setup steps (at a high level):\\n\\nEnable Delta Sharing for the Unity Catalog metastore that manages the data you want to share.\\n\\nNote\\nYou do not need to enable Delta Sharing on your metastore if you intend to use Delta Sharing to share data only with users on other Unity Catalog metastores in your account. Metastore-to-metastore sharing within a single Databricks account is enabled by default.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='See Enable Delta Sharing on a metastore.\\n\\nCreate a share that includes data assets registered in the Unity Catalog metastore.\\nIf you are sharing with a non-Databricks recipient (known as open sharing) you can include tables in the Delta or Parquet format. If you plan to use Databricks-to-Databricks sharing, you can also add views, Unity Catalog volumes, Unity Catalog models, and notebook files to a share.\\nSee Create and manage shares for Delta Sharing.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Create a recipient.\\nSee Create and manage data recipients for Delta Sharing.\\nIf your recipient is not a Databricks user, or does not have access to a Databricks workspace that is enabled for Unity Catalog, you must use open sharing. A set of token-based credentials is generated for that recipient.\\nIf your recipient has access to a Databricks workspace that is enabled for Unity Catalog, you can use Databricks-to-Databricks sharing, and no token-based credentials are required. You request a sharing identifier from the recipient and use it to establish the secure connection.\\n\\nTip\\nUse yourself as a test recipient to try out the setup process.\\n\\n\\nGrant the recipient access to one or more shares.\\nSee Grant and manage access to Delta Sharing data shares (for providers).\\n\\nNote\\nThis step can also be performed by a non-admin user with the USE SHARE, USE RECIPIENT and SET SHARE PERMISSION privileges. See Unity Catalog privileges and securable objects.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Send the recipient the information they need to connect to the share (open sharing only).\\nSee Send the recipient their connection information.\\nFor open sharing, use a secure channel to send the recipient an activation link that allows them to download their token-based credentials.\\nFor Databricks-to-Databricks sharing, the data included in the share becomes available in the recipient’s Databricks workspace as soon as you grant them access to the share.\\n\\n\\nThe recipient can now access the shared data.\\n\\n\\nHow do recipients access the shared data? \\nRecipients access shared data assets in read-only format. Shared notebook files are read-only, but they can be cloned and then modified and run in the recipient workspace just like any other notebook.\\nSecure access depends on the sharing model:', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Open sharing (recipient does not have a Databricks workspace enabled for Unity Catalog): The recipient provides the credential whenever they access the data in their tool of choice, including Apache Spark, pandas, Power BI, Databricks, and many more.  See Read data shared using Delta Sharing open sharing (for recipients).\\nDatabricks-to-Databricks (recipient workspace is enabled for Unity Catalog): The recipient accesses the data using Databricks. They can use Unity Catalog to grant and deny access to other users in their Databricks account. See Read data shared using Databricks-to-Databricks Delta Sharing (for recipients).\\n\\nWhenever the data provider updates data tables or volumes in their own Databricks account, the updates appear in near real time in the recipient’s system.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='How do you keep track of who is sharing and accessing shared data? \\nData providers on Unity Catalog-enabled Databricks workspaces can use Databricks audit logging and system tables to monitor the creation and modification of shares and recipients, and can monitor recipient activity on shares. See Audit and monitor data sharing using Delta Sharing (for providers).\\nData recipients who use shared data in a Databricks workspace can use Databricks audit logging and system tables to understand who is accessing which data. See Audit and monitor data access using Delta Sharing (for recipients).\\n\\n\\nSharing volumes \\nYou can share volumes using the Databricks-to-Databricks sharing flow. See Add volumes to a share (for providers) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients) (for recipients).', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Sharing models \\nYou can share models using the Databricks-to-Databricks sharing flow. See Add models to a share (for providers) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients) (for recipients).\\n\\n\\nSharing notebooks \\nYou can use Delta Sharing to share notebook files using the Databricks-to-Databricks sharing flow. See Add notebook files to a share (for providers) and Read shared notebooks (for recipients).\\n\\n\\nRestricting access at the row and column level \\nYou can share dynamic views that restrict access to certain table data based on recipient properties. Dynamic view sharing requires the Databricks-to-Databricks sharing flow. See Add dynamic views to a share to filter rows and columns.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Delta Sharing and streaming \\nDelta Sharing supports Spark Structured Streaming. A provider can share a table with history so that a recipient can use it as a Structured Streaming source, processing shared data incrementally with low latency. Recipients can also perform Delta Lake time travel queries on tables shared with history.\\nTo learn how to share tables with history, see Add tables to a share. To learn how to use shared tables as streaming sources, see Query a table using Apache Spark Structured Streaming (for recipients of Databricks-to-Databricks sharing) or Access a shared table using Spark Structured Streaming (for recipients of open sharing data).\\nSee also Streaming on Databricks.\\n\\n\\nDelta Sharing FAQs \\nThe following are frequently asked questions about Delta Sharing.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Do I need Unity Catalog to use Delta Sharing? \\nNo, you do not need Unity Catalog to share (as a provider) or consume shared data (as a recipient). However, Unity Catalog provides benefits such as support for non-tabular asset sharing, out-of-the-box governance, simplicity, and query performance.\\nProviders can share data in two ways:\\n\\nPut the assets to share under Unity Catalog management and share them using the built-in Databricks Delta Sharing server.\\nYou do do not need to migrate all assets to Unity Catalog. You need only one Databricks workspace that is enabled for Unity Catalog to manage assets that you want to share. In some accounts, new workspaces are enabled for Unity Catalog automatically. See Automatic enablement of Unity Catalog.\\n\\nImplement the open Delta Sharing server to share data, without necessarily using your Databricks account.\\n\\nRecipients can consume data in two ways:', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Without a Databricks workspace. Use open source Delta Sharing connectors that are available for many data platforms, including Power BI, pandas, and open source Apache Spark. See Read data shared using Delta Sharing open sharing (for recipients) and the Delta Sharing open source project.\\nIn a Databricks workspace. Recipient workspaces don’t need to be enabled for Unity Catalog, but there are advantages of governance, simplicity, and performance if they are.\\nRecipient organizations who want these advantages don’t need to migrate all assets to Unity Catalog. You need only one Databricks workspace that is enabled for Unity Catalog to manage assets that are shared with you. In some accounts, new workspaces are enabled for Unity Catalog automatically. See Automatic enablement of Unity Catalog.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Recipient organizations who want these advantages don’t need to migrate all assets to Unity Catalog. You need only one Databricks workspace that is enabled for Unity Catalog to manage assets that are shared with you.\\nSee Read data shared using Delta Sharing open sharing (for recipients) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients).', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Do I need to be a Databricks customer to use Delta Sharing? \\nNo, Delta Sharing is an open protocol. You can share non-Databricks data with recipients on any data platform. Providers can configure an open Delta Sharing server to share from any computing platform. Recipients can consume shared data using open source Delta Sharing connectors for many data products, including Power BI, pandas, and open source Spark.\\nHowever, using Delta Sharing on Databricks, especially sharing from a Unity Catalog-enabled workspace, has many advantages.\\nFor details, see the first question in this FAQ.\\n\\n\\nAre egress costs high? \\n\\nDelta Sharing within a region incurs no egress cost.\\nDelta Sharing is the only solution that enables cross-cloud and cross-region sharing without data replication, which is why it can incur egress charges.\\nTo learn about how to reduce egress charges, talk to your Databricks account team.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Can providers revoke recipient access? \\nYes, recipient access can be revoked on-demand and at specified levels of granularity. You can deny recipient access to specific shares and specific IP addresses, filter tabular data for a recipient, revoke recipient tokens, and delete recipients entirely.  See Revoke recipient access to a share and Create and manage data recipients for Delta Sharing.\\n\\n\\nIsn’t it insecure to use pre-signed URLs? \\nDelta Sharing uses pre-signed URLs to provide temporary access to a file in object storage. They are only given to recipients that already have access to the shared data. They are secure because they are short-lived and don’t expand the level of access beyond what recipients have already been granted.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Are the tokens used in the Delta Sharing open sharing protocol secure? \\nBecause Delta Sharing enables cross-platform sharing—unlike other available data sharing platforms—the sharing protocol requires an open token. Providers can ensure token security by configuring the token lifetime, setting networking controls, and revoking access on demand. In addition, the token does not expand the level of access beyond what recipients have already been granted. See Security considerations for tokens.\\nIf you prefer not to use tokens to manage access to recipient shares, you should use Databricks-to-Databricks sharing or contact your Databricks account team for alternatives.\\n\\n\\nDoes Delta Sharing support view sharing? \\nYes, Delta Sharing supports view sharing. See Add views to a share.\\nTo learn about planned enhancements to viewing sharing, contact your Databricks account team.\\n\\n\\n\\nLimitations', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Tabular data must be in the Delta table format. You can easily convert Parquet tables to Delta—and back again. See CONVERT TO DELTA.\\nView sharing is supported only in Databricks-to-Databricks sharing. Shareable views must be defined on Delta tables or other shareable views. See (for providers)Add views to a share and (for consumers) Read shared views.\\nVolume sharing is supported only in Databricks-to-Databricks sharing. See Add volumes to a share (for providers) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients) (for recipients).\\nModel sharing is supported only in Databricks-to-Databricks sharing. See Add models to a share (for providers) and Read data shared using Databricks-to-Databricks Delta Sharing (for recipients) (for recipients).\\nThere are limits on the number of files in metadata allowed for a shared table. To learn more, see Resource limit exceeded errors.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Schemas named information_schema cannot be imported into a Unity Catalog metastore, because that schema name is reserved in Unity Catalog.\\nTable constraints (primary and foreign key constraints) are not available in shared tables.', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='Resource quotas \\nThe values below indicate the quotas for Delta Sharing resources. Quota values below are expressed relative to the parent object in Unity Catalog.\\n\\n\\n\\n\\n\\n\\n\\nObject\\nParent\\nValue\\n\\n\\n\\n provider\\n metastore\\n 1000\\n\\n recipients\\n metastore\\n 5000\\n\\n shares\\n metastore\\n 1000\\n\\n tables\\n share\\n 1000\\n\\n volumes\\n share\\n 1000\\n\\n models\\n share\\n 1000\\n\\n schemas\\n share\\n 500\\n\\n notebooks\\n share\\n 100\\n\\n\\n\\nIf you expect to exceed these resource limits, contact your Databricks account team.\\n\\n\\nNext steps \\n\\nEnable your Databricks account for Delta Sharing\\nCreate shares\\nCreate recipients\\nLearn more about the open sharing and Databricks-to-Databricks sharing models\\nLearn how recipients access shared data\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/data-sharing/index.html', 'title': 'Share data securely using Delta Sharing | Databricks on AWS', 'description': 'Learn how to use Delta Sharing for secure data sharing with users outside your organization or on different metastores within your Databricks account.', 'language': 'en-US'}),\n",
              " Document(page_content='What is Databricks Marketplace? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\nRequest and access data products using Unity Catalog (consumer)\\nRequest and access data products using external platforms (consumer)\\nManage requests and installed products (consumer)\\nList data products (provider)\\nManage listings (provider)\\nManage consumer requests (provider)\\nCreate private exchanges (provider)\\nProvider policies\\n\\n\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/marketplace/index.html', 'title': 'What is Databricks Marketplace? | Databricks on AWS', 'description': 'Learn how to use the Databricks Marketplace to provide and consume shared data securely.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Databricks Marketplace?\\n\\n\\n\\n\\n\\n\\n\\nWhat is Databricks Marketplace? \\nThis article introduces Databricks Marketplace, an open forum for exchanging data products. Databricks Marketplace takes advantage of Delta Sharing to give data providers the tools to share data products securely and data consumers the power to explore and expand their access to the data and data services they need.', metadata={'source': 'http://docs.databricks.com/marketplace/index.html', 'title': 'What is Databricks Marketplace? | Databricks on AWS', 'description': 'Learn how to use the Databricks Marketplace to provide and consume shared data securely.', 'language': 'en-US'}),\n",
              " Document(page_content='What kinds of data assets are shared on Databricks Marketplace? \\nMarketplace assets include datasets, Databricks notebooks, Databricks Solution Accelerators, and machine learning (AI) models. Datasets are typically made available as catalogs of tabular data, although non-tabular data, in the form of Databricks volumes, is also supported. Solution Accelerators are available as clonable Git repos.\\n\\n\\nHow do consumers get access to data in Databricks Marketplace? \\nTo find a data product you want on the Databricks Marketplace, simply browse or search provider listings.\\nYou can browse:\\n\\nThe Open Marketplace, which does not require access to a Databricks workspace.\\nThe Databricks Marketplace on your Databricks workspace. Just click  Marketplace.', metadata={'source': 'http://docs.databricks.com/marketplace/index.html', 'title': 'What is Databricks Marketplace? | Databricks on AWS', 'description': 'Learn how to use the Databricks Marketplace to provide and consume shared data securely.', 'language': 'en-US'}),\n",
              " Document(page_content='To request access to data products in the Marketplace, you must use the Marketplace on a Databricks workspace. You do not need a Databricks workspace to access and work with data once it is shared, although using a Databricks workspace with Unity Catalog enabled lets you take advantage of the deep integration of Unity Catalog with Delta Sharing.\\nSome data products are available to everyone in the public marketplace, and others are available as part of a private exchange, in which a provider shares their listings only with member consumers. Whether public or private, some data products are available instantly, as soon as you request them and agree to the terms. Others might require provider approval and transaction completion using provider interfaces. In either case, the Delta Sharing protocol that powers the Marketplace ensures that you can access shared data securely.\\n\\nGet started accessing data products \\nTo learn how to get started as a data consumer:', metadata={'source': 'http://docs.databricks.com/marketplace/index.html', 'title': 'What is Databricks Marketplace? | Databricks on AWS', 'description': 'Learn how to use the Databricks Marketplace to provide and consume shared data securely.', 'language': 'en-US'}),\n",
              " Document(page_content='Using a Databricks workspace that is enabled for Unity Catalog, see Request and access data products in Databricks Marketplace (Unity Catalog-enabled workspaces).\\nUsing third-party platforms like Power BI, pandas, or Apache Spark, along with Databricks workspaces that are not enabled for Unity Catalog, see Request and access data products in Databricks Marketplace using external platforms.', metadata={'source': 'http://docs.databricks.com/marketplace/index.html', 'title': 'What is Databricks Marketplace? | Databricks on AWS', 'description': 'Learn how to use the Databricks Marketplace to provide and consume shared data securely.', 'language': 'en-US'}),\n",
              " Document(page_content='How do providers list data products in Databricks Marketplace? \\nDatabricks Marketplace gives data providers a secure platform for sharing data products that data scientists and analysts can use to help their organizations succeed. Databricks Marketplace uses Delta Sharing to provide security and control over your shared data. You can share public data, free sample data, and commercialized data offerings. You can share data products in public listings or as part of private exchanges that you create, making listings discoverable only by member consumers. In addition to datasets, you can also share Databricks notebooks and other content to demonstrate use cases and show customers how to take full advantage of your data products.\\n\\nGet started listing data products \\nTo list your data products on Databricks Marketplace, you must:', metadata={'source': 'http://docs.databricks.com/marketplace/index.html', 'title': 'What is Databricks Marketplace? | Databricks on AWS', 'description': 'Learn how to use the Databricks Marketplace to provide and consume shared data securely.', 'language': 'en-US'}),\n",
              " Document(page_content='Have a Databricks account and premium workspace that is enabled for Unity Catalog. You do not need to enable all of your workspaces for Unity Catalog. You can create one specifically for managing Marketplace listings.\\nApply to be a provider through the Databricks Data Partner Program.\\nReview the Marketplace provider policies.\\n\\nTo learn how to get started, see List your data product in Databricks Marketplace.\\n\\n\\n\\nView a demo \\nThis video introduces Databricks Marketplace, shows how consumers access listings, and demonstrates how providers create them.\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/marketplace/index.html', 'title': 'What is Databricks Marketplace? | Databricks on AWS', 'description': 'Learn how to use the Databricks Marketplace to provide and consume shared data securely.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks data engineering | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nDelta Live Tables\\nStructured Streaming\\nApache Spark\\nCompute\\nNotebooks\\nWorkflows\\nLibraries\\nInit scripts\\nRepos\\nDBFS\\nFiles\\nMigration\\nOptimization & performance\\n\\n\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration', metadata={'source': 'http://docs.databricks.com/workspace-index.html', 'title': 'Databricks data engineering | Databricks on AWS', 'description': 'Learn how get started with the Databricks data engineering tools and features.', 'language': 'en-US'}),\n",
              " Document(page_content='Administration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks data engineering\\n\\n\\n\\n\\n\\n\\n\\nDatabricks data engineering \\nDatabricks data engineering features are a robust environment for collaboration among data scientists, data engineers, and data analysts. Data engineering tasks are also the backbone of  Databricks machine learning solutions.\\n\\nNote\\nIf you are a data analyst who works primarily with SQL queries and BI tools, you might prefer Databricks SQL.\\n\\nThe data engineering documentation provides how-to guidance to help you get the most out of the Databricks collaborative analytics platform. For getting started tutorials and introductory information, see Get started: Account and workspace setup and What is Databricks?.', metadata={'source': 'http://docs.databricks.com/workspace-index.html', 'title': 'Databricks data engineering | Databricks on AWS', 'description': 'Learn how get started with the Databricks data engineering tools and features.', 'language': 'en-US'}),\n",
              " Document(page_content='Delta Live TablesLearn how to build data pipelines for ingestion and transformation with Databricks Delta Live Tables.\\n\\n\\n\\nStructured StreamingLearn about streaming, incremental, and real-time workloads powered by Structured Streaming on Databricks.\\n\\n\\n\\nApache SparkLearn how Apache Spark works on Databricks and the Databricks platform.\\n\\n\\n\\nComputeLearn about Databricks clusters and how to create and manage them.\\n\\n\\n\\nNotebooksLearn what a Databricks notebook is, and how to use and manage notebooks to process, analyze, and visualize your data.\\n\\n\\n\\nWorkflowsLearn how to orchestrate data processing, machine learning, and data analysis workflows on the Databricks Data Intelligence Platform.\\n\\n\\n\\nLibrariesLearn how to make third-party or custom code available in Databricks using libraries. Learn about the different modes for installing libraries on Databricks.', metadata={'source': 'http://docs.databricks.com/workspace-index.html', 'title': 'Databricks data engineering | Databricks on AWS', 'description': 'Learn how get started with the Databricks data engineering tools and features.', 'language': 'en-US'}),\n",
              " Document(page_content='Init scriptsLearn how to use initialization (init) scripts to install packages and libraries, set system properties and environment variables, modify Apache Spark config parameters, and set other configurations on Databricks clusters.\\n\\n\\n\\nReposLearn how to use Git to version control your notebooks and other files for development in Databricks.\\n\\n\\n\\nDBFSLearn about Databricks File System (DBFS), a distributed file system mounted into a Databricks workspace and available on Databricks clusters\\n\\n\\n\\nFilesLearn about options for working with files on Databricks.\\n\\n\\n\\nMigrationLearn how to migrate data applications such as ETL jobs, enterprise data warehouses, ML, data science, and analytics to Databricks.\\n\\n\\n\\nOptimization & performanceLearn about optimizations and performance recommendations on Databricks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.', metadata={'source': 'http://docs.databricks.com/workspace-index.html', 'title': 'Databricks data engineering | Databricks on AWS', 'description': 'Learn how get started with the Databricks data engineering tools and features.', 'language': 'en-US'}),\n",
              " Document(page_content='Send us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/workspace-index.html', 'title': 'Databricks data engineering | Databricks on AWS', 'description': 'Learn how get started with the Databricks data engineering tools and features.', 'language': 'en-US'}),\n",
              " Document(page_content='AI and Machine Learning on Databricks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\n10-minute tutorials\\nStep-by-step: AI and ML on Databricks\\nPrepare data and environment\\nFeature engineering\\nTrain machine learning models\\nTrain deep learning models\\nAutoML\\nUse Ray on Databricks\\nManage the ML lifecycle with MLflow\\nBatch inference\\nReference solutions\\nMLOps\\nGraphFrames\\n\\n\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nAI and Machine Learning on Databricks', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='AI and Machine Learning on Databricks \\nThis article describes the tools that Databricks provides to help you build and monitor AI and ML workflows. The diagram shows how these components work together to help you implement your model development and deployment process.\\n\\n\\n\\n\\nWhy use Databricks for machine learning and deep learning? \\nWith Databricks, you can implement the full ML lifecycle on a single platform with end-to-end governance throughout the ML pipeline. Databricks includes the following built-in tools to support ML workflows:\\n\\nUnity Catalog for governance, discovery, versioning, and access control for data, features, models, and functions.\\nLakehouse Monitoring for data monitoring.\\nFeature engineering and serving.\\nSupport for the model lifecycle:', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks AutoML for automated model training.\\nMLflow for model development tracking.\\nUnity Catalog for model management.\\nDatabricks Model Serving for high-availability, low-latency model serving. This includes deploying LLMs using:\\n\\nFoundation Model APIs which allow you to access and query state-of-the-art open models from a serving endpoint.\\nExternal models which allow you to access models hosted outside of Databricks.\\n\\n\\nLakehouse Monitoring to track model prediction quality and drift.\\n\\n\\nDatabricks Workflows for automated workflows and production-ready ETL pipelines.\\nDatabricks Repos for code management and Git integration.', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='Deep learning applications on Databricks \\nConfiguring infrastructure for deep learning applications can be difficult.\\nDatabricks Runtime for Machine Learning takes care of that for you, with clusters that have built-in compatible versions of the most common deep learning libraries like TensorFlow, PyTorch, and Keras, and supporting libraries such as Petastorm, Hyperopt, and Horovod. Databricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries.\\nFor machine learning applications, Databricks recommends using a cluster running Databricks Runtime for Machine Learning. See Create a cluster using Databricks Runtime ML.', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='Use Databricks for deep learning applications \\nDatabricks Machine Learning provides pre-built deep learning infrastructure, enabling development across the entire deep learning lifecycle. Databricks Model Serving and Databricks Runtime for Machine Learning include built-in, pre-configured GPU support with drivers and supporting libraries. Databricks Model Serving enables creation of scalable GPU endpoints for deep learning models with no extra configuration. Databricks Runtime for Machine Learning includes the most common deep learning libraries like TensorFlow, PyTorch, and Keras and supporting libraries like Petastorm, Hyperopt, and Horovod.\\nTo get started with deep learning on Databricks, see:\\n\\nBest practices for deep learning on Databricks\\nDeep learning on Databricks\\nReference solutions for deep learning', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='Large language models (LLMs) and generative AI on Databricks \\nDatabricks Runtime for Machine Learning includes libraries like Hugging Face Transformers and LangChain that allow you to integrate existing pre-trained models or other open-source libraries into your workflow. The Databricks MLflow integration makes it easy to use the MLflow tracking service with transformer pipelines, models, and processing components. In addition, you can integrate OpenAI models or solutions from partners like John Snow Labs in your Databricks workflows.\\nWith Databricks, you can customize a LLM on your data for your specific task. With the support of open source tooling, such as Hugging Face and DeepSpeed, you can efficiently take a foundation LLM and train it with your own data to improve its accuracy for your specific domain and workload. You can then leverage the custom LLM in your generative AI applications.', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='In addition, Databricks provides Foundation Model APIs and external models which allows you to access and query state-of-the-art open models from a serving endpoint. Using Foundation Model APIs, developers can quickly and easily build applications that leverage a high-quality generative AI model without maintaining their own model deployment.\\nFor SQL users, Databricks provides AI functions that SQL data analysts can use to access LLM models, including from OpenAI, directly within their data pipelines and workflows. See AI Functions on Databricks.', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks Runtime for Machine Learning \\nDatabricks Runtime for Machine Learning (Databricks Runtime ML) automates the creation of a cluster with pre-built machine learning and deep learning infrastructure including the most common ML and DL libraries. For the full list of libraries in each version of Databricks Runtime ML, see the release notes.\\nTo access data in Unity Catalog for machine learning workflows, the access mode for the cluster must be single user (assigned). Shared clusters are not compatible with Databricks Runtime for Machine Learning. In addition, Databricks Runtime ML is not supported on TableACLs clusters or clusters with spark.databricks.pyspark.enableProcessIsolation config set to true.\\n\\nCreate a cluster using Databricks Runtime ML \\nWhen you create a cluster, select a Databricks Runtime ML version from the Databricks runtime version drop-down menu. Both CPU and GPU-enabled ML runtimes are available.', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='If you select a cluster from the drop-down menu in the notebook, the Databricks Runtime version appears at the right of the cluster name:\\n\\n\\n\\nIf you select a GPU-enabled ML runtime, you are prompted to select a compatible Driver type and Worker type. Incompatible instance types are grayed out in the drop-down menu. GPU-enabled instance types are listed under the GPU accelerated label.\\n\\nNote\\nTo access data in Unity Catalog for machine learning workflows, the access mode for the cluster must be single user (assigned). Shared clusters are not compatible with Databricks Runtime for Machine Learning.', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='Libraries included in Databricks Runtime ML \\nDatabricks Runtime ML includes a variety of popular ML libraries. The libraries are updated with each release to include new features and fixes.\\nDatabricks has designated a subset of the supported libraries as top-tier libraries. For these libraries, Databricks provides a faster update cadence, updating to the latest package releases with each runtime release (barring dependency conflicts). Databricks also provides advanced support, testing, and embedded optimizations for top-tier libraries.\\nFor a full list of top-tier and other provided libraries, see the release notes for Databricks Runtime ML.\\n\\n\\n\\nNext steps \\nTo get started, see:\\n\\nTutorials: Get started with ML\\n\\nFor a recommended MLOps workflow on Databricks Machine Learning, see:\\n\\nMLOps workflows on Databricks\\n\\nTo learn about key Databricks Machine Learning features, see:', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='What is AutoML?\\nWhat is a feature store?\\nModel serving with Databricks\\nLakehouse Monitoring\\nManage model lifecycle\\nMLflow experiment tracking\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/machine-learning/index.html', 'title': 'AI and Machine Learning on Databricks | Databricks on AWS', 'description': 'AI and Machine Learning on Databricks, an integrated environment to simplify and standardize ML, DL, LLM, and AI development. Tutorials and user guides for common tasks and scenarios.', 'language': 'en-US'}),\n",
              " Document(page_content='What is data warehousing on Databricks? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\n   Get started\\n   What are SQL Warehouses\\n   SQL editor\\n   Queries\\n   Lakeview dashboards\\n   DBSQL dashboards\\n   Alerts\\n   Use materialized views\\n   Load data using streaming tables\\n\\n\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Administration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is data warehousing on Databricks?\\n\\n\\n\\n\\n\\n\\n\\nWhat is data warehousing on Databricks? \\nData warehousing refers to collecting and storing data from multiple sources so it can be quickly accessed for business insights and reporting. This article contains key concepts for building a data warehouse in your data lakehouse.\\n\\nData warehousing in your lakehouse \\nThe lakehouse architecture and Databricks SQL bring cloud data warehousing capabilities to your data lakes. Using familiar data structures, relations, and management tools, you can model a highly-performant, cost-effective data warehouse that runs directly on your data lake. For more information, see What is a data lakehouse?', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='As with a traditional data warehouse, you model data according to business requirements and then serve it to your end users for analytics and reports. Unlike a traditional data warehouse, you can avoid siloing your business analytics data or creating redundant copies that quickly become stale.\\nBuilding a data warehouse inside your lakehouse lets you bring all your data into a single system and lets you take advantage of features such as Unity Catalog and Delta Lake.\\nUnity Catalog adds a unified governance model so that you can secure and audit data access and provide lineage information on downstream tables. Delta Lake adds ACID transactions and schema evolution, among other powerful tools for keeping your data reliable, scalable, and high-quality.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='What is Databricks SQL? \\nDatabricks SQL is the collection of services that bring data warehousing capabilities and performance to your existing data lakes. Databricks SQL supports open formats and standard ANSI SQL. An in-platform SQL editor and dashboarding tools allow team members to collaborate with other Databricks users directly in the workspace. Databricks SQL also integrates with a variety of tools so that analysts can author queries and dashboards in their favorite environments without adjusting to a new platform.\\nDatabricks SQL provides general compute resources that are executed against the tables in the lakehouse. Databricks SQL is powered by SQL warehouses, offering scalable SQL compute resources decoupled from storage.\\nSee What are SQL Warehouses? for more information on SQL Warehouse defaults and options.\\nDatabricks SQL integrates with Unity Catalog so that you can discover, audit, and govern data assets from one place. To learn more, see What is Unity Catalog?', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Data modeling on Databricks \\nA lakehouse supports a variety of modeling styles. The following image shows how data is curated and modeled as it moves through different layers of a lakehouse.\\n\\n\\n\\n\\nMedallion architecture \\nThe medallion architecture is a data design pattern that describes a series of incrementally refined data layers that provide a basic structure in the lakehouse. The bronze, silver, and gold layers signify increasing data quality at each level, with gold representing the highest quality. For more information, see What is the medallion lakehouse architecture?.\\nInside a lakehouse, each layer can contain one or more tables. The data warehouse is modeled at the silver layer and feeds specialized data marts in the gold layer.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Bronze layer \\nData can enter your lakehouse in any format and through any combination of batch or steaming transactions. The bronze layer provides the landing space for all of your raw data in its original format. That data is converted to Delta tables.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Silver layer \\nThe silver layer brings the data from different sources together. For the part of the business that focuses on data science and machine learning applications, this is where you start to curate meaningful data assets. This process is often marked by a focus on speed and agility.\\nThe silver layer is also where you can carefully integrate data from disparate sources to build a data warehouse in alignment with your existing business processes. Often, this data follows a Third Normal Form (3NF) or Data Vault model. Specifying primary and foreign key constraints allows end users to understand table relationships when using Unity Catalog.  Your data warehouse should serve as the single source of truth for your data marts.\\nThe data warehouse itself is schema-on-write and atomic. It is optimized for change, so you can quickly modify the data warehouse to match your current needs when your business processes change or evolve.', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Gold layer \\nThe gold layer is the presentation layer, which can contain one or more data marts. Frequently, data marts are dimensional models in the form of a set of related tables that capture a specific business perspective.\\nThe gold layer also houses departmental and data science sandboxes to enable self-service analytics and data science across the enterprise. Providing these sandboxes and their own separate compute clusters prevents the Business teams from creating copies of data outside the lakehouse.\\n\\n\\n\\nNext step \\nTo learn more about the principles and best practices for implementing and operating a lakehouse using Databricks, see Data lakehouse architecture: Databricks well-architected framework.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/sql/index.html', 'title': 'What is data warehousing on Databricks? | Databricks on AWS', 'description': 'Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='What is Delta Lake? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDelta tutorial\\nMerge data\\nSelective overwrite\\nDrop or replace\\nHistory and data retention\\nVacuum\\nLiquid clustering\\nData skipping\\nOptimize\\nChange data feed\\nColumn mapping\\nTable constraints\\nView table details\\nUser-defined metadata\\nGenerated columns\\nIdempotent writes\\nDeletion vectors\\nSchema validation\\nUpdate schema\\nTune file size\\nBest practices\\nTable properties reference\\nManage feature compatibility\\nDrop table features\\nGenerate manifest file\\nPartitioning tables\\nDeep and shallow clone\\nClone with Unity Catalog\\nClone Parquet and Iceberg tables\\nConvert to Delta\\nUniversal Format\\n\\n\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Documentation \\nWhat is Delta Lake?', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='What is Delta Lake? \\nDelta Lake is the optimized storage layer that provides the foundation for storing data and tables in the Databricks lakehouse. Delta Lake is open source software that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Delta Lake is the default storage format for all operations on Databricks. Unless otherwise specified, all tables on Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project. Many of the optimizations and products in the Databricks platform build upon the guarantees provided by Apache Spark and Delta Lake. For information on optimizations on Databricks, see Optimization recommendations on Databricks.\\nFor reference information on Delta Lake SQL commands, see Delta Lake statements.\\nThe Delta Lake transaction log has a well-defined open protocol that can be used by any system to read the log. See Delta Transaction Log Protocol.', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Getting started with Delta Lake \\nAll tables on Databricks are Delta tables by default. Whether you’re using Apache Spark DataFrames or SQL, you get all the benefits of Delta Lake just by saving your data to the lakehouse with default settings.\\nFor examples of basic Delta Lake operations such as creating tables, reading, writing, and updating data, see Tutorial: Delta Lake.\\nDatabricks has many recommendations for best practices for Delta Lake.\\n\\n\\nConverting and ingesting data to Delta Lake \\nDatabricks provides a number of products to accelerate and simplify loading data to your lakehouse.\\n\\nDelta Live Tables:\\n\\nTutorial: Run your first ETL workload on Databricks\\nLoad data using streaming tables (Python/SQL notebook)\\n\\n\\nLoad data using streaming tables in Databricks SQL\\n\\n\\nCOPY INTO\\nAuto Loader\\nAdd data UI\\nIncrementally convert Parquet or Iceberg data to Delta Lake\\nOne-time conversion of Parquet or Iceberg data to Delta Lake\\nThird-party partners', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='For a full list of ingestion options, see Load data into a Databricks lakehouse.\\n\\n\\nUpdating and modifying Delta Lake tables \\nAtomic transactions with Delta Lake provide many options for updating data and metadata. Databricks recommends you avoid interacting directly with data and transaction log files in Delta Lake file directories to avoid corrupting your tables.\\n\\nDelta Lake supports upserts using the merge operation.\\nDelta Lake provides numerous options for selective overwrites based on filters and partitions.\\nYou can manually or automatically update your table schema without rewriting data.\\nColumn mapping enables columns to be renamed or deleted without rewriting data.\\n\\n\\n\\nIncremental and streaming workloads on Delta Lake \\nDelta Lake is optimized for Structured Streaming on Databricks. Delta Live Tables extends native capabilities with simplified infrastructure deployment, enhanced scaling, and managed data dependencies.', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Delta table streaming reads and writes\\nUse Delta Lake change data feed on Databricks\\nEnable idempotent writes across jobs\\n\\n\\n\\nQuerying previous versions of a table \\nEach write to a Delta table creates a new table version. You can use the transaction log to review modifications to your table and query previous table versions. See Work with Delta Lake table history.\\n\\n\\nDelta Lake schema enhancements \\nDelta Lake validates schema on write, ensuring that all data written to a table matches the requirements you’ve set.\\n\\nDelta Lake schema validation\\nConstraints on Databricks\\nUse Delta Lake generated columns\\nEnrich Delta Lake tables with custom metadata\\n\\n\\n\\nManaging files and indexing data with Delta Lake \\nDatabricks sets many default parameters for Delta Lake that impact the size of data files and number of table versions that are retained in history. Delta Lake uses a combination of metadata parsing and physical data layout to reduce the number of files scanned to fulfill any query.', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Use liquid clustering for Delta tables\\nData skipping for Delta Lake\\nCompact data files with optimize on Delta Lake\\nRemove unused data files with vacuum\\nConfigure Delta Lake to control data file size\\n\\n\\n\\nConfiguring and reviewing Delta Lake settings \\nDatabricks stores all data and metadata for Delta Lake tables in cloud object storage. Many configurations can be set at either the table level or within the Spark session. You can review the details of the Delta table to discover what options are configured.\\n\\nReview Delta Lake table details with describe detail\\nDelta table properties reference\\n\\n\\n\\nData pipelines using Delta Lake and Delta Live Tables \\nDatabricks encourages users to leverage a medallion architecture to process data through a series of tables as data is cleaned and enriched. Delta Live Tables simplifies ETL workloads through optimized execution and automated infrastructure deployment and scaling.', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Troubleshooting Delta Lake features \\nNot all Delta Lake features are in all versions of Databricks Runtime. You can find information about Delta Lake versioning and answers to frequent questions in the following articles:\\n\\nHow does Databricks manage Delta Lake feature compatibility?\\nGenerate a manifest file\\n\\n\\n\\nDelta Lake API documentation \\nFor most read and write operations on Delta tables, you can use Spark SQL or Apache Spark DataFrame APIs.\\nFor Delta Lake-spefic SQL statements, see Delta Lake statements.\\nDatabricks ensures binary compatibility with Delta Lake APIs in Databricks Runtime. To view the Delta Lake API version packaged in each Databricks Runtime version, see the System environment section on the relevant article in the Databricks Runtime release notes. Delta Lake APIs exist for Python, Scala, and Java:\\n\\nPython API docs\\nScala API docs\\nJava API docs', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/delta/index.html', 'title': 'What is Delta Lake? | Databricks on AWS', 'description': 'Learn about the Delta Lake storage protocol used to power the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Developer tools and guidance | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nAuthentication\\nIDEs\\nSDKs\\nSQL connectors/drivers\\nCLIs\\nUtilities\\nREST API reference\\nIaC\\nCI/CD\\nSQL tools\\nService principals\\n\\n\\nTechnology partners\\n\\nAdministration', metadata={'source': 'http://docs.databricks.com/dev-tools/index.html', 'title': 'Developer tools and guidance | Databricks on AWS', 'description': 'Learn how to use Databricks developer tools and follow Databricks developer guidance.', 'language': 'en-US'}),\n",
              " Document(page_content='Administration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDeveloper tools and guidance\\n\\n\\n\\n\\n\\n\\n\\nDeveloper tools and guidance \\nLearn about tools and guidance you can use to work with Databricks resources and data and to develop Databricks applications.\\n\\n\\n\\n\\n\\n\\nSection\\nUse this section when you want to…\\n\\n\\n\\nAuthentication\\nAuthenticate with Databricks from your tools, scripts, and apps. You must authenticate with Databricks\\nbefore you can work with Databricks resources and data.\\n\\nIDEs\\nConnect to Databricks by using popular integrated development environments (IDEs) such as\\nVisual Studio Code, PyCharm, IntelliJ IDEA, Eclipse, and RStudio, as well as automate Databricks by using IDE\\nplugins.', metadata={'source': 'http://docs.databricks.com/dev-tools/index.html', 'title': 'Developer tools and guidance | Databricks on AWS', 'description': 'Learn how to use Databricks developer tools and follow Databricks developer guidance.', 'language': 'en-US'}),\n",
              " Document(page_content='SDKs\\nAutomate Databricks from code libraries written for popular languages such as Python, Java, Go, and R.\\n\\nSQL connectors/drivers\\nRun SQL commands on Databricks from code written in popular languages such as Python, Go, JavaScript, and TypeScript.\\nConnect tools and clients to Databricks through ODBC and JDBC connections.\\n\\nCLIs\\nAutomate Databricks by using the Databricks command-line interface (CLI).\\nQuery data warehouses from the command line by using the Databricks SQL CLI.\\n\\nUtilities\\nUse Databricks Utilities from within notebooks to do things such as work with object storage efficiently,\\nchain and parameterize notebooks, and work with sensitive credential information.\\n\\nREST API Reference\\nLook up reference information for the Databricks REST APIs.\\n\\nIaC\\nAutomate the provision and maintenance of Databricks infrastructure and resources by using popular\\ninfrastructure-as-code (IaC) products such as Terraform, the Cloud Development Kit for Terraform, and Pulumi.', metadata={'source': 'http://docs.databricks.com/dev-tools/index.html', 'title': 'Developer tools and guidance | Databricks on AWS', 'description': 'Learn how to use Databricks developer tools and follow Databricks developer guidance.', 'language': 'en-US'}),\n",
              " Document(page_content='CI/CD\\nImplement industry-standard continuous integration and continuous delivery (CI/CD) practices for Databricks\\nby using Databricks Asset Bundles, and popular systems and frameworks such as GitHub Actions, DevOps pipelines,\\nJenkins, and Apache Airflow.\\n\\nSQL tools\\nRun SQL commands and scripts in Databricks by using the Databricks SQL CLI, the Databricks Driver for SQLTools, and\\npopular tools such as DataGrip, DBeaver, and SQL Workbench/J.\\n\\nService principals\\nDatabricks recommends that you use service principals instead of users to authenticate automated scripts, tools,\\napps, and systems with Databricks workspaces and resources.\\n\\n\\n\\n\\n\\n\\nTip\\nYou can also connect many additional popular third-party tools to clusters and SQL warehouses to access data in Databricks. See the Technology partners.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.', metadata={'source': 'http://docs.databricks.com/dev-tools/index.html', 'title': 'Developer tools and guidance | Databricks on AWS', 'description': 'Learn how to use Databricks developer tools and follow Databricks developer guidance.', 'language': 'en-US'}),\n",
              " Document(page_content='Send us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/dev-tools/index.html', 'title': 'Developer tools and guidance | Databricks on AWS', 'description': 'Learn how to use Databricks developer tools and follow Databricks developer guidance.', 'language': 'en-US'}),\n",
              " Document(page_content='Technology partners | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Databricks lakehouse data.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\nDatabricks ODBC and JDBC Drivers\\nGet compute resource connection details\\nDatabricks sign-on from partner solutions\\nPartner Connect\\nIngestion\\nData preparation and transformation\\nMachine Learning\\nBI and visualization\\nReverse ETL\\nSemantic layer\\nData governance\\nData security\\n\\n\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nTechnology partners', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Databricks lakehouse data.', 'language': 'en-US'}),\n",
              " Document(page_content='Technology partners \\nDatabricks has validated integrations with various third-party solutions that allow you to work with data through Databricks clusters and SQL warehouses, in many cases with low-code and no-code experiences. These solutions enable common scenarios such as data ingestion, data preparation and transformation, business intelligence (BI), and machine learning.\\nDatabricks also includes Partner Connect, a user interface that allows some of these validated solutions to integrate more quickly and easily with your Databricks clusters and SQL warehouses.\\n\\n\\nBuild an integration with Databricks \\nThis section provides instructions and best practices for technology partners to build and maintain their integrations with Databricks.\\n\\nBest practices for ingestion partners using Unity Catalog volumes as staging locations for data', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Databricks lakehouse data.', 'language': 'en-US'}),\n",
              " Document(page_content='All Databricks technology partners \\nFor a list of all Databricks partner solutions, see Databricks Technology Partners. Some of these partner solutions are featured in Databricks Partner Connect.\\n\\n\\n\\nDatabricks Partner Connect partners \\n\\nThis section lists the partner solutions that are featured in Partner Connect.\\n\\nData ingestion \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Fivetran using Partner Connect\\n\\n\\nNo\\nConnect to Hevo Data using Partner Connect\\n\\n\\nNo\\nConnect to Rivery using Partner Connect\\n\\n\\nYes\\nConnect to RudderStack using Partner Connect\\n\\n\\nYes\\nConnect to Snowplow using Partner Connect\\n\\n\\n\\n\\n\\nData preparation and transformation \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to dbt Cloud using Partner Connect\\n\\n\\nYes\\nConnect to Matillion using Partner Connect\\n\\n\\nN/A\\nConnect to Prophecy using Partner Connect\\n\\n\\n\\n\\n\\nMachine learning \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Databricks lakehouse data.', 'language': 'en-US'}),\n",
              " Document(page_content='Yes\\nConnect to Dataiku using Partner Connect\\n\\n\\nN/A\\nConnect to John Snow Labs using Partner Connect\\n\\n\\nN/A\\nConnect to Labelbox using Partner Connect\\n\\n\\n\\n\\n\\nBI and visualization \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Hex using Partner Connect\\n\\n\\nYes\\nConnect Power BI Desktop to Databricks using Partner Connect\\n\\n\\nNo\\nConnect to Preset using Partner Connect\\n\\n\\n\\nPartner Connect: No\\nManual connection: Yes\\n\\n\\nConnect to Qlik Sense using Partner Connect\\n\\n\\nYes\\nConnect to Sigma using Partner Connect\\n\\n\\nYes\\nConnect to Tableau Desktop using Partner Connect\\n\\n\\nYes\\nConnect to ThoughtSpot using Partner Connect\\n\\n\\n\\n\\n\\nReverse ETL \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Census using Partner Connect\\n\\n\\n\\nPartner Connect: No\\nManual connection: Yes\\n\\n\\nConnect to Hightouch using Partner Connect\\n\\n\\n\\n\\nSecurity \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Hunters using Partner Connect', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Databricks lakehouse data.', 'language': 'en-US'}),\n",
              " Document(page_content='Yes\\nConnect to Privacera using Partner Connect\\n\\n\\n\\n\\n\\n\\nData governance \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to Alation using Partner Connect\\n\\n\\nYes\\nConnect to Anomalo using Partner Connect\\n\\n\\nNo\\nConnect to erwin Data Modeler using Partner Connect\\n\\n\\n\\nPartner Connect: No\\nManual connection: Yes\\n\\n\\nConnect to Lightup using Partner Connect\\n\\n\\n\\nPartner Connect: Yes\\nManual connection: Yes\\n\\n\\nConnect to Monte Carlo using Partner Connect\\n\\n\\n\\n\\n\\nSemantic layer \\n\\n\\n\\n\\n\\n\\n\\nPartner\\nUnity Catalog support\\nSteps to connect\\n\\n\\n\\n\\nYes\\nConnect to AtScale using Partner Connect\\n\\n\\nYes\\nConnect to Stardog using Partner Connect\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/integrations/index.html', 'title': 'Technology partners | Databricks on AWS', 'description': 'Learn how you can connect technology partners to your Databricks workspace so you can use third-party tools with your Databricks lakehouse data.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks administration introduction | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Administration\\n\\nAccount and workspace administration\\nGet started with Databricks administration\\nAccount administration\\nWorkspace deployment\\nManage a workspace\\nIdentity management\\nCompute policies\\nAudit logs\\nSystem tables\\n\\n\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks administration introduction\\n\\n\\n\\n\\n\\n\\n\\nDatabricks administration introduction \\n\\n\\nThis article provides an introduction to Databricks administrator privileges and responsibilities.\\n\\nNote\\nTo fully administer your Databricks instance, you will also need administrative access to your AWS account.\\n\\n\\nDatabricks admin types \\nThere are two main levels of admin privileges available on the Databricks platform:\\n\\nAccount admins: Manage the Databricks account, including workspace creation, user management, cloud resources, and account usage monitoring.', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Workspace admins: Manage workspace identities, access control, settings, and features for individual workspaces in the account.\\n\\nAdditionally, users can be assigned these feature-specific admin roles, which have narrower sets of privileges:\\n\\nMarketplace admins: Manage their account’s Databricks Marketplace provider profile, including creating and managing Marketplace listings.\\nMetastore admins: Manage privileges and ownership for all securable objects within a Unity Catalog metastore, such as who can create catalogs or query a table.\\n\\n\\n\\nWhat are account admins? \\nAccount admins have privileges over the entire Databricks account. As an account admin, you can create workspaces, configure cloud resources, view usage data, and manage account identities, settings, and subscriptions.\\nAccount admins can also delegate the account admin and workspace admin roles to any other user.\\n\\nAccess the account console \\nThe account console is where account admins manage their Databricks account.', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Account admins can access the account console at https://accounts.cloud.databricks.com or by clicking their email address at the top of the workspace UI and selecting Manage Account.\\n\\n\\n\\nAccount admin responsibilities \\nAs an account admin, your responsibilities include:\\n\\nCreating and managing workspaces\\nEnabling Unity Catalog\\nManaging identities\\nMonitoring account usage logs\\nManaging the account subscription\\n\\n\\nCreate and manage workspaces \\nOnly account admins can create new workspaces. There are a few different methods you can use to create workspaces. See Create and manage workspaces for instructions on each method. You can also use the Workspaces section of the account console to view and manage all the workspaces in your account.\\n\\n\\nEnable Unity Catalog \\n\\nNote\\nIf your Databricks account was created after November 8, 2023, your workspaces might have Unity Catalog enabled by default. For more information, see Automatic enablement of Unity Catalog.', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='An account admin is needed to enable Unity Catalog in your account. The process involves creating a Unity Catalog metastore, which can only be done by an account admin.\\nFor instructions on enabling Unity Catalog, see Get started using Unity Catalog.', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Manage identities \\nAccount admins should sync their identity provider with Databricks if applicable. See Sync users and groups from your identity provider.\\nIf you’ve enabled Unity Catalog for at least one workspace in your account, identities (users, groups, and service principals) should be managed in the account console. Account admins can grant permissions and assign workspaces to these identities.\\nFor more information, see Manage users and groups.\\nSingle sign-on (SSO) enables you to authenticate your users using your organization’s identity provider. Databricks recommends configuring SSO in your account for greater security and improved user experience. Once SSO is configured, you can enable multi-factor authentication via your identity provider. For instructions, see Setting up SSO.', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Monitor account usage logs \\nOnly account admins can configure audit logs for their account. For information on audit logs, see Audit log reference.\\nAccount admins can also view and download billable usage logs from the account console. For more information, see View billable usage using the account console.\\n\\n\\nManage account subscription \\nAccount admins can manage aspects of their Databricks subscription from the account console. For more information, see Manage subscription and billing.\\n\\n\\n\\nWhat are workspace admins? \\nWorkspace admins have admin privileges within a single workspace. They can manage workspace-level identities, regulate compute use, and enable and delegate role-based access control (Premium plan or above only).', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Access the admin settings \\nWorkspace admins are the only users who have access to the workspace’s admin settings page. As a workspace admin, you can access admin settings by clicking your username in the top bar of the Databricks workspace and selecting Admin Settings.\\n\\n\\n\\n\\n\\n\\nWorkspace admin responsibilities \\nAs a workspace admin, your responsibilities include:\\n\\nManaging identities in your workspace\\nCreating and managing compute resources\\nManaging workspace features and settings\\n\\n\\nManage identities in your workspace \\nIf your workspace is enabled for Unity Catalog, identities should be added at the account level. Workspace admins can then assign users, groups, and service principals to their workspace. For more information on adding and removing identities in a workspace, see Manage users, service principals, and groups.', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Note\\nDatabricks Academy has a free course on Identity Administration. Before you can access the course, you first need to register for Databricks Academy if you haven’t already.\\n\\n\\n\\nCreate and manage compute resources \\nWorkspace admins can create SQL warehouses (a compute resource that lets you run SQL commands on data objects within Databricks SQL) and clusters for their workspace users. For instructions on creating SQL warehouses, see Create a SQL warehouse.\\nIt is also the workspace admin’s job to regulate how compute resources are used in their workspace. Workspace admins have the following tools:\\n\\nLimit workspace users’ cluster creation options with cluster policies.\\n\\nDatabricks recommends managing all init scripts as cluster-scoped init scripts. Instead of using global init scripts, manage init scipts using cluster policies.\\n\\n\\nLearn which compute resources have Unity Catalog access.\\n\\n\\nGrant S3 bucket access through clusters using instance profiles.', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Note\\nDatabricks Academy has a free course on Compute Resources Administration.\\n\\n\\n\\nManage workspaces features and settings \\nWorkspace admins are responsible for managing select workspace behavior and settings. For information on other available workspace settings, see Managing workspace settings.\\n\\nNote\\nDatabricks Academy has a free course on Databricks Workspace Administration and Security.\\n\\n\\n\\n\\nAdditional resources \\nDatabricks Academy has a free self-paced learning path for platform administrators. Before you can access the course, you first need to register for Databricks Academy if you haven’t already.\\nYou can also sign up to attend a live platform administration training.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/administration-guide/index.html', 'title': 'Databricks administration introduction | Databricks on AWS', 'description': 'This article provides an introduction to Databricks administration. It includes lists of workspace and account admin capabilities.', 'language': 'en-US'}),\n",
              " Document(page_content='Security and compliance guide | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='Administration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nAuthentication and access control\\nNetwork connectivity\\nData security and encryption\\nSecret management\\nAuditing, privacy, and compliance\\n\\n\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nSecurity and compliance guide\\n\\n\\n\\n\\n\\n\\n\\nSecurity and compliance guide \\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their Databricks environment according to their risk profile and governance policy.\\nThis guide does not cover information about securing your data. For that information, see Data governance guide.', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='Note\\nThis article focuses on the most recent (E2) version of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='Authentication and access control \\nIn Databricks, a workspace is a Databricks deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their Databricks assets. Your organization can choose to have multiple workspaces or just one, depending on your needs. A Databricks account represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and Unity Catalog metastores.\\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage Databricks users, service principals, and groups, as well as authentication settings and access control.', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks provides security features, such as single sign-on, to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\\nAccess control lists determine who can view and perform operations on objects in Databricks workspaces, such as notebooks and SQL warehouses.\\nTo learn more about authentication and access control in Databricks, see Authentication and access control.', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='Network access \\nDatabricks provides network protections that enable you to secure Databricks workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of Databricks users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see Network connectivity.\\n\\n\\nData security and encryption \\nSecurity-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. Databricks has an extremely strong security program which manages the risk of such an incident. See the Security and Trust Center for an overview on the program. That said, no company can completely eliminate all risk, and Databricks provides encryption features for additional control of your data. See Data security and encryption.', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='Secret management \\nSometimes accessing data requires that you authenticate to external data sources. Databricks recommends that you  use Databricks secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see Secret management.\\n\\n\\nAuditing, privacy, and compliance \\nDatabricks provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\\nDatabricks also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\\nFor more information, see Auditing, privacy, and compliance.\\n\\nSecurity Analysis Tool', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='Experimental\\nThe Security Analysis Tool (SAT) is a productivity tool in an Experimental state. It’s not meant to be used as a certification of your deployments. The SAT project is regularly updated to improve correctness of checks, add new checks, and fix bugs.\\n\\nYou can use the Security Analysis Tool (SAT) to analyze your Databricks account and workspace security configurations. SAT provides recommendations that help you follow Databricks security best practices. SAT is typically run daily as an automated workflow. The details of these check results are persisted in Delta tables in your storage so that trends can be analyzed over time. These results are displayed in a centralized Databricks dashboard.\\nFor more information, see the Security Analysis Tool GitHub repo.\\n\\n\\n\\n\\n\\n\\nLearn more \\nHere are some resources to help you build a comprehensive security solution that meets your organization’s needs:', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='The Databricks Security and Trust Center, which provides information about the ways in which security is built into every layer of the Databricks platform.\\nSecurity Best Practices, which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/security/index.html', 'title': 'Security and compliance guide | Databricks on AWS', 'description': 'Learn about how Databricks secures your data and privacy and how you can secure your Databricks account and data.', 'language': 'en-US'}),\n",
              " Document(page_content='Data governance guide | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nUnity Catalog\\nWhat is Catalog Explorer?\\nHive metastore table access control (legacy)\\n\\n\\nLakehouse architecture', metadata={'source': 'http://docs.databricks.com/data-governance/index.html', 'title': 'Data governance guide | Databricks on AWS', 'description': 'Learn about data governance in Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Reference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nData governance guide\\n\\n\\n\\n\\n\\n\\n\\nData governance guide \\nThis guide shows how to manage data and data access in Databricks. For information on Databricks security, see the Security and compliance guide. Databricks provides centralized governance for data and AI with Unity Catalog and Delta Sharing.\\n\\n\\n\\nCentralize access control using Unity Catalog \\nUnity Catalog is a fine-grained governance solution for data and AI on the Databricks platform. It helps simplify security and governance of your data and AI assets by providing a central place to administer and audit access to data and AI assets.\\nFor best practices on adopting Unity Catalog, see Unity Catalog best practices.', metadata={'source': 'http://docs.databricks.com/data-governance/index.html', 'title': 'Data governance guide | Databricks on AWS', 'description': 'Learn about data governance in Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Track data lineage using Unity Catalog \\nYou can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows, and dashboards related to the query. To learn more, see Capture and view data lineage with Unity Catalog.\\n\\n\\n\\nDiscover data using Catalog Explorer \\nDatabricks Catalog Explorer provides a UI to explore and manage data and AI assets, including schemas (databases), tables, volumes (non-tabular data), and registered ML models, along with asset permissions, data owners, external locations, and credentials. You can use the Insights tab in Catalog Explorer to view the most frequent recent queries and users of any table registered in Unity Catalog.', metadata={'source': 'http://docs.databricks.com/data-governance/index.html', 'title': 'Data governance guide | Databricks on AWS', 'description': 'Learn about data governance in Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Share data using Delta Sharing \\nDelta Sharing is an open protocol developed by Databricks for secure data and AI asset sharing with other organizations, or with other teams within your organization, regardless of which computing platforms they use.\\n\\n\\nConfigure audit logging \\nDatabricks provides access to audit logs of activities performed by Databricks users, allowing your enterprise to monitor detailed Databricks usage patterns.\\nUnity Catalog lets you easily access and query your account’s operational data, including audit logs, billable usage, and lineage using system tables (Public Preview).\\n\\n\\nConfigure identity \\nEvery good data governance story starts with a strong identity foundation. To learn how to best configure identity in Databricks, see Identity best practices.\\n\\n\\nLegacy data governance solutions', metadata={'source': 'http://docs.databricks.com/data-governance/index.html', 'title': 'Data governance guide | Databricks on AWS', 'description': 'Learn about data governance in Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Table access control is a legacy data governance model that lets you programmatically grant and revoke access to objects managed by your workspace’s built-in Hive metastore. Databricks recommends that you use Unity Catalog instead of table access control. Unity Catalog simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account.\\n\\n\\nIAM role credential passthrough is also a legacy data governance feature that allows users to authenticate automatically to S3 buckets from Databricks clusters using the identity that they use to log in to Databricks. Databricks recommends that you use Unity Catalog instead.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/data-governance/index.html', 'title': 'Data governance guide | Databricks on AWS', 'description': 'Learn about data governance in Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Administration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\nData governance\\nInteroperability & usability\\nOperational excellence\\nSecurity, compliance & privacy\\nReliability\\nPerformance efficiency\\nCost optimization\\n\\n\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nData lakehouse architecture: Databricks well-architected framework\\n\\n\\n\\n\\n\\n\\n\\nData lakehouse architecture: Databricks well-architected framework \\nThis set of data lakehouse architecture articles provides principles and best practices for the implementation and operation of a lakehouse using Databricks.\\n\\nDatabricks well-architected framework for the lakehouse \\n\\n\\n\\nThe well-architected lakehouse consists of 7 pillars which describe different areas of concern for the implementation of a data lakehouse in the cloud:', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Data governance\\nThe oversight to ensure that data brings value and supports your business strategy.\\n\\nInteroperability and usability\\nThe ability of the lakehouse to interact with users and other systems.\\n\\nOperational excellence\\nAll operations processes that keep the lakehouse running in production.\\n\\nSecurity, privacy, compliance\\nProtect the Databricks application, customer workloads and customer data from threats.\\n\\nReliability\\nThe ability of a system to recover from failures and continue to function.\\n\\nPerformance efficiency\\nThe ability of a system to adapt to changes in load.\\n\\nCost optimization\\nManaging costs to maximize the value delivered.', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='The well-architected lakehouse extends the AWS Well-Architected Framework to the Databricks platform and shares the pillars “Operational Excellence”, “Security” (as “Security, privacy, compliance”), “Reliability”, “Performance Efficiency” and “Cost Optimization”.\\nFor these five pillars, the principles and best practices of the cloud framework still apply to the lakehouse. The well-architected lakehouse extends these with principles and best practices that are specific for the lakehouse and important to build an effective and efficient lakehouse.', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Data Governance and Interoperability & Usability in lakehouse architectures \\nThe pillars “Data Governance” and “Interoperability and Usability” cover concerns specific for the lakehouse.\\nData governance encapsulates the policies and practices implemented to securely manage the data assets within an organization. One of the fundamental aspects of a lakehouse is centralized data governance: The lakehouse unifies data warehousing and AI uses cases on a single platform. This simplifies the modern data stack by eliminating the data silos that traditionally separate and complicate data engineering, analytics, BI, data science, and machine learning. To simplify data governance, the lakehouse offers a unified governance solution for data, analytics and AI. By minimizing the copies of your data and moving to a single data processing layer where all your data governance controls can run together, you improve your chances of staying in compliance and detecting a data breach.', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Another important tenet of the lakehouse is to provide a great user experience for all the personas that work with it, and to be able to interact with a wide ecosystem of external systems. AWS already has a variety of data tools that perform most tasks a data-driven enterprise might need. However, these tools must be properly assembled to provide all the functionality, with each service offering a different user experience. This approach can lead to high implementation costs and typically does not provide the same user experience as a native lakehouse platform: Users are limited by inconsistencies between tools and a lack of collaboration capabilities, and often have to go through complex processes to gain access to the system and thus to the data.', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='An integrated lakehouse on the other side provides a consistent user experience across all workloads and therefore increases usability. This reduces training and onboarding costs and improves collaboration between functions. In addition, new features are automatically added over time - to further improve the user experience - without the need to invest internal resources and budgets.\\nA multicloud approach can be a deliberate strategy of a company or the result of mergers and acquisitions or independent business units selecting different cloud providers. In this case, using a multicloud lakehouse results in a unified user experience across all clouds. This reduces the proliferation of systems across the enterprise, which in turn reduces the skill and training requirements of employees involved in data-driven tasks.', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Finally, in a networked world with cross-company business processes, systems must work together as seamlessly as possible. The degree of interoperability is a crucial criterion here, and the most recent data, as a core asset of any business, must flow securely between internal and external partners systems.', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Principles and best practices \\n\\n\\nData governance\\nInteroperability & usability\\nOperational excellence\\nSecurity, compliance & privacy\\nReliability\\nPerformance efficiency\\nCost optimization\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/lakehouse-architecture/index.html', 'title': 'Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS', 'description': 'Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks reference documentation | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources', metadata={'source': 'http://docs.databricks.com/reference/api.html', 'title': 'Databricks reference documentation | Databricks on AWS', 'description': 'Reference documentation for Databricks APIs, SQL language, command-line interfaces, and more. Databricks reference docs cover tasks from automation to data queries.', 'language': 'en-US'}),\n",
              " Document(page_content='Reference\\nPython, SparkR & Scala intros\\nREST API reference\\nMLFlow API\\nFeature Store Python API\\nApache Spark API\\nDelta Lake API\\nDelta Live Tables API\\nSQL language reference\\n\\n\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks reference documentation\\n\\n\\n\\n\\n\\n\\n\\nDatabricks reference documentation \\nThis article contains links to Databricks API reference documentation and guidance.\\n\\n\\n\\n\\nCommon error codes in Databricks \\n\\nError messages in Databricks\\n\\n\\n\\nPython, SparkR, and Scala on Databricks \\n\\nDatabricks for Python developers\\nDatabricks for R developers\\nDatabricks for Scala developers\\n\\n\\n\\nAPI reference documentation \\nDatabricks provides the following API reference documentation:\\n\\nREST API reference\\nMLflow API\\nFeature Store Python API\\nApache Spark API\\nDelta Lake API\\nDelta Live Tables API\\n\\n\\n\\nSQL language reference documentation \\n\\nSQL language reference\\nDelta Live Tables SQL language reference', metadata={'source': 'http://docs.databricks.com/reference/api.html', 'title': 'Databricks reference documentation | Databricks on AWS', 'description': 'Reference documentation for Databricks APIs, SQL language, command-line interfaces, and more. Databricks reference docs cover tasks from automation to data queries.', 'language': 'en-US'}),\n",
              " Document(page_content='CLI reference documentation \\n\\nDatabricks CLI\\nDatabricks SQL CLI\\n\\n\\n\\nSDK reference documentation \\n\\nDatabricks SDK for Python\\nDatabricks SDK for R\\nDatabricks SDK for Java\\nDatabricks SDK for Go\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/reference/api.html', 'title': 'Databricks reference documentation | Databricks on AWS', 'description': 'Reference documentation for Databricks APIs, SQL language, command-line interfaces, and more. Databricks reference docs cover tasks from automation to data queries.', 'language': 'en-US'}),\n",
              " Document(page_content='Resources | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources', metadata={'source': 'http://docs.databricks.com/resources/index.html', 'title': 'Resources | Databricks on AWS', 'description': 'Learn how to submit support tickets, manage your support contract, submit product feedback, and monitor Databricks system status.', 'language': 'en-US'}),\n",
              " Document(page_content='Reference\\nResources\\nStatus Page\\nLimits\\nDatabricks clouds and regions\\nSupported browsers\\nConfigure domain name firewall rules\\nPlatform release process\\nSubmit product feedback\\nSupport\\nError messages\\n\\n\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nResources\\n\\n\\n\\n\\n\\n\\n\\nResources \\nThis section provides information on limits, the Databricks release process, support plans, how to give product feedback, and how to monitor system status.\\n\\n\\nStatus Page\\nLimits\\nDatabricks clouds and regions\\nSupported browsers\\nConfigure domain name firewall rules\\nPlatform release process\\nSubmit product feedback\\nSupport\\nError messages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/resources/index.html', 'title': 'Resources | Databricks on AWS', 'description': 'Learn how to submit support tickets, manage your support contract, submit product feedback, and monitor Databricks system status.', 'language': 'en-US'}),\n",
              " Document(page_content='What’s coming? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive', metadata={'source': 'http://docs.databricks.com/whats-coming.html', 'title': 'What’s coming? | Databricks on AWS', 'description': 'Learn about upcoming Databricks releases.', 'language': 'en-US'}),\n",
              " Document(page_content='Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat’s coming?\\n\\n\\n\\n\\n\\n\\n\\nWhat’s coming? \\nLearn about upcoming Databricks releases.\\n\\nLegacy Git integration is EOL on January 31 \\nAfter January 31st, 2024, Databricks will remove legacy notebook Git integrations. This feature has been in legacy status for more than two years, and a deprecation notice has been displayed in the product UI since November 2023.\\n\\n\\nChanges to query, dashboard, and alert listing pages \\nDatabricks plans to remove the Admin view tab from listing pages for queries, dashboards, and alerts. Workspace admins automatically have “Can Manage” permissions on workspace objects, so all queries, dashboards, and alerts can be accessed from the All tab on each listing page.', metadata={'source': 'http://docs.databricks.com/whats-coming.html', 'title': 'What’s coming? | Databricks on AWS', 'description': 'Learn about upcoming Databricks releases.', 'language': 'en-US'}),\n",
              " Document(page_content='External support ticket submission will be deprecated \\nDatabricks plans to transition the support ticket submission experience from help.databricks.com to the help menu in the Databricks workspace. Support ticket submission via help.databricks.com will be deprecated. You’ll still view and triage your tickets at help.databricks.com.\\nThe in-product experience, which is available if your organization has a Databricks Support contract, integrates with Databricks Assistant to help address your issues quickly without having to submit a ticket.\\nTo access the in-product experience, click the help icon , and then click Create Support Ticket or type “I need help” into the assistant.\\nThe Contact support modal opens.\\n\\n\\n\\nIf the in-product experience is down, send requests for support with detailed information about your issue to help@databricks.com. For more information, see Get help.', metadata={'source': 'http://docs.databricks.com/whats-coming.html', 'title': 'What’s coming? | Databricks on AWS', 'description': 'Learn about upcoming Databricks releases.', 'language': 'en-US'}),\n",
              " Document(page_content='Google Data Studio (Looker Studio) integration \\nUsage of the Databricks Connector for Data Studio and the transfer of information received from Google APIs to any other app will adhere to the Google API Services User Data Policy, including the Limited Use requirements.\\n\\n\\nJDK8 and JDK11 will be unsupported \\nDatabricks plans to remove JDK 8 support with the next major Databricks Runtime version, when Spark 4.0 releases. Databricks plans to remove JDK 11 support with the next LTS version of Databricks Runtime 14.x.\\n\\n\\nAutomatic enablement of Unity Catalog for new workspaces \\nDatabricks will soon start to enable Unity Catalog automatically for new workspaces. This removes the need for account admins to configure Unity Catalog after a workspace is created. Rollout will proceed gradually across accounts.', metadata={'source': 'http://docs.databricks.com/whats-coming.html', 'title': 'What’s coming? | Databricks on AWS', 'description': 'Learn about upcoming Databricks releases.', 'language': 'en-US'}),\n",
              " Document(page_content='New charts and chart improvements \\nDatabricks plans to add new charts to the SQL editor, SQL dashboards, and notebooks. This change will bring faster chart rendering performance, improved colors, and faster interactivity. See New chart visualizations in Databricks\\n\\n\\nFavorites functionality \\nDatabricks plans to enable favorites functionality in the workspace. You’ll be able to save content such as notebooks, dashboards, experiments, and queries to a list of favorites, and then access your favorites from the homepage.\\n\\n\\nsqlite-jdbc upgrade \\nDatabricks Runtime plans to upgrade the sqlite-jdbc version from 3.8.11.2 to 3.42.0.0 in all Databricks Runtime maintenance releases. The APIs of version 3.42.0.0 are not fully compatible with 3.8.11.2. Confirm your methods and return type use version 3.42.0.0.\\nIf you are using sqlite-jdbc in your code, check the sqlite-jdbc compatibility report.', metadata={'source': 'http://docs.databricks.com/whats-coming.html', 'title': 'What’s coming? | Databricks on AWS', 'description': 'Learn about upcoming Databricks releases.', 'language': 'en-US'}),\n",
              " Document(page_content='© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/whats-coming.html', 'title': 'What’s coming? | Databricks on AWS', 'description': 'Learn about upcoming Databricks releases.', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks documentation archive | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}),\n",
              " Document(page_content='Reference\\nResources\\nWhat’s coming?\\nDocumentation archive\\nAdministration guide (legacy)\\nCreate cluster UI (legacy)\\nCluster UI preview\\nInstall a library with an init script\\n(Legacy) Cluster-named init scripts\\n(Legacy) Global init scripts\\nCompute policies best practices\\n(Legacy) Manage libraries with %conda commands\\nExplore and create tables in DBFS\\nTransactional writes to cloud storage with DBIO\\nKoalas\\nDatabricks CLI (legacy)\\nWhat is dbx by Databricks Labs?\\ndbutils.library\\nMigrate to Spark 3.x\\nVScode with Repos\\nExternal metastores (legacy)\\nCredential passthrough (legacy)\\nShare feature tables across workspaces (legacy)\\nGenomics guide\\nMLeap ML model export\\nTrain a PySpark model and save in MLeap format\\nDeploy MLeap model on SageMaker\\nModel serving (legacy)\\nServerless Real-Time Inference (preview)\\nDatabricks light\\nDatabricks runtime release notes (unsupported)\\nSpark SQL 2.x reference\\nUnity Catalog GA release note\\nAudit log schemas for security monitoring', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}),\n",
              " Document(page_content='Amazon S3 source with Amazon SQS (legacy)\\nAzure Blob storage file source with Azure Queue Storage (legacy)\\nConnecting Databricks and Azure Synapse with PolyBase (legacy)\\nNeo4j\\nAccessing Azure Data Lake Storage Gen1 from Databricks\\nConfigure Delta storage credentials\\nConnect to Azure Blob Storage with WASB (legacy)', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}),\n",
              " Document(page_content='Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks documentation archive\\n\\n\\n\\n\\n\\n\\n\\nDatabricks documentation archive \\n\\nImportant\\nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.\\n\\nIn this archive, you can find earlier versions of documentation for Databricks products, features, APIs, and workflows.\\n\\nAdministration \\n\\n\\nAdministration guide (legacy)\\n\\n\\n\\n\\nCompute \\n\\n\\nCreate cluster UI (legacy)\\nCluster UI preview\\nInstall a library with an init script\\n(Legacy) Cluster-named init scripts\\n(Legacy) Global init scripts\\nCompute policies best practices\\n\\n\\n\\n\\nDev tools \\n\\n\\n(Legacy) Manage libraries with %conda commands\\nExplore and create tables in DBFS\\nTransactional writes to cloud storage with DBIO\\nKoalas\\nDatabricks CLI (legacy)\\nWhat is dbx by Databricks Labs?\\ndbutils.library\\nMigrate to Spark 3.x\\nVScode with Repos\\n\\n\\n\\n\\nGovernance', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}),\n",
              " Document(page_content='Governance \\n\\n\\nExternal metastores (legacy)\\nCredential passthrough (legacy)\\n\\n\\n\\n\\nMachine learning and AI \\n\\n\\nShare feature tables across workspaces (legacy)\\nGenomics guide\\nMLeap ML model export\\nTrain a PySpark model and save in MLeap format\\nDeploy MLeap model on SageMaker\\nModel serving (legacy)\\nServerless Real-Time Inference (preview)\\n\\n\\n\\n\\nRelease notes \\n\\n\\nDatabricks light\\nDatabricks runtime release notes (unsupported)\\nSpark SQL 2.x reference\\nUnity Catalog GA release note\\n\\n\\n\\n\\nSecurity \\n\\n\\nAudit log schemas for security monitoring\\n\\n\\n\\n\\nStorage \\n\\n\\nAmazon S3 source with Amazon SQS (legacy)\\nAzure Blob storage file source with Azure Queue Storage (legacy)\\nConnecting Databricks and Azure Synapse with PolyBase (legacy)\\nNeo4j\\nAccessing Azure Data Lake Storage Gen1 from Databricks\\nConfigure Delta storage credentials\\nConnect to Azure Blob Storage with WASB (legacy)', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}),\n",
              " Document(page_content='© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/archive/index.html', 'title': 'Databricks documentation archive | Databricks on AWS', 'description': 'The docs in this archive have been retired and might not be updated. The products, services, or technologies mentioned in this content are no longer supported.', 'language': 'en-US'}),\n",
              " Document(page_content='What is a data lakehouse? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\n  What is a data lakehouse?\\nWhat are ACID guarantees on Databricks?\\nWhat is the medallion lakehouse architecture?\\nWhat does it mean to build a single source of truth?\\nData discovery and collaboration in the lakehouse\\nData objects in the Databricks lakehouse\\n\\n\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations\\n\\n\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Databricks? \\nWhat is a data lakehouse?\\n\\n\\n\\n\\n\\n\\n\\nWhat is a data lakehouse? \\nA data lakehouse is a data management system that combines the benefits of data lakes and data warehouses. This article describes the lakehouse architectural pattern and what you can do with it on Databricks.', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='What is a data lakehouse used for? \\nA data lakehouse provides scalable storage and processing capabilities for modern organizations that want to avoid isolated systems for processing different workloads, like machine learning (ML) and business intelligence (BI). A data lakehouse can help establish a single source of truth, eliminate redundant costs, and ensure data freshness.\\nData lakehouses often use a data design pattern that incrementally improves, enriches, and refines data as it moves through layers of staging and transformation. Each layer of the lakehouse can include one or more layers. This pattern is frequently referred to as a medallion architecture. For more information, see What is the medallion lakehouse architecture?', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='How does the Databricks lakehouse work? \\nDatabricks is built on Apache Spark. Apache Spark enables a massively scalable engine that runs on compute resources decoupled from storage. For more information, see Apache Spark on Databricks\\nThe Databricks lakehouse uses two additional key technologies:\\n\\nDelta Lake: an optimized storage layer that supports ACID transactions and schema enforcement.\\nUnity Catalog: a unified, fine-grained governance solution for data and AI.', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Data ingestion \\nAt the ingestion layer, batch or streaming data arrives from a variety of sources and in a variety of formats. This first logical layer provides a place for that data to land in its raw format. As you convert those files to Delta tables, you can use the schema enforcement capabilities of Delta Lake to check for missing or unexpected data. You can use Unity Catalog to register tables according to your data governance model and required data isolation boundaries. Unity Catalog allows you to track the lineage of your data as it is transformed and refined, as well as apply a unified governance model to keep sensitive data private and secure.', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Data processing, curation, and integration \\nOnce verified, you can start curating and refining your data. Data scientists and machine learning practitioners frequently work with data at this stage to start combining or creating new features and complete data cleansing. Once your data has been thoroughly cleansed, it can be integrated and reorganized into tables designed to meet your particular business needs.\\nA schema-on-write approach, combined with Delta schema evolution capabilities, means that you can make changes to this layer without necessarily having to rewrite the downstream logic that serves data to your end users.', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Data serving \\nThe final layer serves clean, enriched data to end users. The final tables should be designed to serve data for all your use cases. A unified governance model means you can track data lineage back to your single source of truth. Data layouts, optimized for different tasks, allow end users to access data for machine learning applications, data engineering, and business intelligence and reporting.\\nTo learn more about Delta Lake, see What is Delta Lake?\\nTo learn more about Unity Catalog, see What is Unity Catalog?\\n\\n\\n\\nCapabilities of a Databricks lakehouse \\nA lakehouse built on Databricks replaces the current dependency on data lakes and data warehouses for modern data companies. Some key tasks you can perform include:', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Real-time data processing: Process streaming data in real-time for immediate analysis and action.\\nData integration: Unify your data in a single system to enable collaboration and establish a single source of truth for your organization.\\nSchema evolution: Modify data schema over time to adapt to changing business needs without disrupting existing data pipelines.\\nData transformations: Using Apache Spark and Delta Lake brings speed, scalability, and reliability to your data.\\nData analysis and reporting: Run complex analytical queries with an engine optimized for data warehousing workloads.\\nMachine learning and AI: Apply advanced analytics techniques to all of your data. Use ML to enrich your data and support other workloads.\\nData versioning and lineage: Maintain version history for datasets and track lineage to ensure data provenance and traceability.\\nData governance: Use a single, unified system to control access to your data and perform audits.', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Data sharing: Facilitate collaboration by allowing the sharing of curated data sets, reports, and insights across teams.\\nOperational analytics: Monitor data quality metrics, model quality metrics, and drift by applying machine learning to lakehouse monitoring data.', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Lakehouse vs Data Lake vs Data Warehouse \\nData warehouses have powered business intelligence (BI) decisions for about 30 years, having evolved as a set of design guidelines for systems controlling the flow of data. Enterprise data warehouses optimize queries for BI reports, but can take minutes or even hours to generate results. Designed for data that is unlikely to change with high frequency, data warehouses seek to prevent conflicts between concurrently running queries. Many data warehouses rely on proprietary formats, which often limit support for machine learning. Data warehousing on Databricks leverages the capabilities of a Databricks lakehouse and Databricks SQL. For more information, see What is data warehousing on Databricks?.', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Powered by technological advances in data storage and driven by exponential increases in the types and volume of data, data lakes have come into widespread use over the last decade. Data lakes store and process data cheaply and efficiently. Data lakes are often defined in opposition to data warehouses: A data warehouse delivers clean, structured data for BI analytics, while a data lake permanently and cheaply stores data of any nature in any format. Many organizations use data lakes for data science and machine learning, but not for BI reporting due to its unvalidated nature.\\nThe data lakehouse combines the benefits of data lakes and data warehouses and provides:', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Open, direct access to data stored in standard data formats.\\nIndexing protocols optimized for machine learning and data science.\\nLow query latency and high reliability for BI and advanced analytics.\\n\\nBy combining an optimized metadata layer with validated data stored in standard formats in cloud object storage, the data lakehouse allows data scientists and ML engineers to build models from the same data-driven BI reports.\\n\\n\\nNext step \\nTo learn more about the principles and best practices for implementing and operating a lakehouse using Databricks, see Data lakehouse architecture: Databricks well-architected framework\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/lakehouse/index.html', 'title': 'What is a data lakehouse? | Databricks on AWS', 'description': 'Use Databricks in a data lakehouse paradigm for generative AI, ACID transactions, data governance, ETL, BI, and machine learning.', 'language': 'en-US'}),\n",
              " Document(page_content='Tutorial: Query data with notebooks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nTutorial: Query data with notebooks\\n\\n\\n\\n\\n\\n\\n\\nTutorial: Query data with notebooks \\nThis tutorial walks you through using the Databricks notebooks user interface to create a cluster and a notebook, create a table from a dataset, query the table, and display the query results.\\n\\nTip\\nYou can also use the Databricks Terraform provider to create this article’s resources. See Create clusters, notebooks, and jobs with Terraform.\\n\\n\\n\\nRequirements \\n\\nYou are logged into a Databricks workspace.\\nYou have permission to create a cluster.', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='Note\\nIf you do not have cluster control privileges, you can still complete most of the steps below as long as you have access to a cluster.\\n\\nFrom the left sidebar on the landing page, you access fundamental workspace entities: the Workspace, Catalog, Workflows, and Compute. The workspace is the special root folder that stores your Databricks assets, such as notebooks and libraries.\\nFor guidance about how to navigate a Databricks notebook, see Databricks notebook interface and controls.\\n\\n\\nStep 1: Create a cluster \\nA cluster is a collection of Databricks computation resources. To create a cluster:\\n\\nIn the sidebar, click  Compute.\\nOn the Compute page, click Create Compute.\\nOn the New Compute page, select 12.2 LTS (Scala 2.12, Spark 3.3.2) or higher from the Databricks Runtime version dropdown.\\nClick Create Cluster.', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='Step 2: Create a notebook \\nA notebook is a collection of cells that run computations on an Apache Spark cluster. For more information on using notebooks, see Introduction to Databricks notebooks. To create a notebook in the workspace:\\n\\nIn the sidebar, click  Workspace.\\nIn your Home  folder, click the blue  Add button > Notebook.\\nReplace the default name of your notebook with your own title and select SQL in the language drop-down. This selection determines the default language of the notebook.\\n\\n\\n\\n\\nAttach the notebook to the cluster you created. Click the cluster selector in the notebook toolbar and select your cluster from the dropdown menu. If you don’t see your cluster, click More… and select the cluster from the dropdown menu in the dialog.', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='Step 3: Create a table \\nCreate a table using data from a sample CSV data file available in Sample datasets, a collection of datasets mounted to What is the Databricks File System (DBFS)?, a distributed file system installed on Databricks clusters. You have two options for creating the table.\\n\\nOption 1: Create a Spark table from the CSV data \\nUse this option if you want to get going quickly, and you only need standard levels of performance. Copy and paste this code snippet into a notebook cell:\\nDROP TABLE IF EXISTS diamonds;\\n\\nCREATE TABLE diamonds USING CSV OPTIONS (path \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header \"true\")\\n\\n\\n\\n\\nOption 2: Write the CSV data to Delta Lake format and create a Delta table \\nDelta Lake offers a powerful transactional storage layer that enables fast reads and other benefits. Delta Lake format consists of Parquet files plus a transaction log. Use this option to get the best performance on future operations on the table.', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='Read the CSV data into a DataFrame and write out in Delta Lake format. This command uses a Python language magic command, which allows you to interleave commands in languages other than the notebook default language (SQL). Copy and paste this code snippet into a notebook cell:\\n%python\\n\\ndiamonds = (spark.read\\n  .format(\"csv\")\\n  .option(\"header\", \"true\")\\n  .option(\"inferSchema\", \"true\")\\n  .load(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\\n)\\n\\ndiamonds.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/diamonds\")\\n\\n\\n\\nCreate a Delta table at the stored location. Copy and paste this code snippet into a notebook cell:\\nDROP TABLE IF EXISTS diamonds;\\n\\nCREATE TABLE diamonds USING DELTA LOCATION \\'/mnt/delta/diamonds/\\'\\n\\n\\n\\n\\nRun cells by pressing SHIFT + ENTER. The notebook automatically attaches to the cluster you created in Step 2 and runs the command in the cell.', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='Step 4: Query the table \\nRun a SQL statement to query the table for the average diamond price by color.\\n\\nTo add a cell to the notebook, mouse over the cell bottom and click the  icon.\\n\\n\\n\\n\\nCopy this snippet and paste it in the cell.\\nSELECT color, avg(price) AS price FROM diamonds GROUP BY color ORDER BY COLOR\\n\\n\\n\\nPress SHIFT + ENTER. The notebook displays a table of diamond color and average price.\\n\\n\\n\\n\\n\\n\\n\\nStep 5: Display the data \\nDisplay a chart of the average diamond price by color.\\n\\nNext to the Table tab, click + and then click Visualization.\\nThe visualization editor displays.\\n\\nIn the Visualization Type drop-down, verify that Bar is selected.\\nClear the Horizontal chart checkbox.\\nChange the aggregation type for the y columns from Sum to Average.\\nClick Save.\\n\\n\\n\\n\\n\\n\\n\\nNext steps \\nTo learn more about the primary tools you use and tasks you can perform with Databricks Data Science & Engineering workspace, see:', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='What is Databricks?\\nNavigate the workspace\\nIntroduction to Databricks notebooks and Visualizations in Databricks notebooks\\nLibraries\\nCompute and Introduction to Databricks Workflows\\nLoad data using the add data UI and Create or modify a table using file upload\\nWhat is Catalog Explorer?\\nDeveloper tools and guidance\\nTechnology partners\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/getting-started/quick-start.html', 'title': 'Tutorial: Query data with notebooks | Databricks on AWS', 'description': 'Learn data science basics on Databricks. Create a cluster, run a notebook, create a table, and query and display data.', 'language': 'en-US'}),\n",
              " Document(page_content='Run your first ETL workload on Databricks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nRun your first ETL workload on Databricks\\n\\n\\n\\n\\n\\n\\n\\nRun your first ETL workload on Databricks \\nLearn how to use production-ready tools from Databricks to develop and deploy your first extract, transform, and load (ETL) pipelines for data orchestration.\\nBy the end of this article, you will feel comfortable:', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='Launching a Databricks all-purpose compute cluster.\\nCreating a Databricks notebook.\\nConfiguring incremental data ingestion to Delta Lake with Auto Loader.\\nExecuting notebook cells to process, query, and preview data.\\nScheduling a notebook as a Databricks job.\\n\\nThis tutorial uses interactive notebooks to complete common ETL tasks in Python or Scala.\\nYou can also use Delta Live Tables to build ETL pipelines. Databricks created Delta Live Tables to reduce the complexity of building, deploying, and maintaining production ETL pipelines. See Tutorial: Declare a data pipeline with SQL in Delta Live Tables.\\nYou can also use the Databricks Terraform provider to create this article’s resources. See Create clusters, notebooks, and jobs with Terraform.\\n\\nRequirements \\n\\nYou are logged into a Databricks workspace.\\nYou have permission to create a cluster.', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='Note\\nIf you do not have cluster control privileges, you can still complete most of the steps below as long as you have access to a cluster.\\nIf you only have access to the Databricks SQL workspace, see Set up your workspace to use Databricks SQL.\\n\\n\\n\\nStep 1: Create a cluster \\nTo do exploratory data analysis and data engineering, create a cluster to provide the compute resources needed to execute commands.\\n\\nClick  Compute in the sidebar.\\nOn the Compute page, click Create Cluster. This opens the New Cluster page.\\nSpecify a unique name for the cluster, leave the remaining values in their default state, and click Create Cluster.\\n\\nTo learn more about Databricks clusters, see Compute.\\n\\n\\nStep 2: Create a Databricks notebook \\nTo get started writing and executing interactive code on Databricks, create a notebook.\\n\\nClick  New in the sidebar, then click Notebook.\\nOn the Create Notebook page:', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='Specify a unique name for your notebook.\\nMake sure the default language is set to Python or Scala.\\nSelect the cluster you created in step 1 from the Cluster dropdown.\\nClick Create.\\n\\n\\n\\nA notebook opens with an empty cell at the top.\\nTo learn more about creating and managing notebooks, see Manage notebooks.\\n\\n\\nStep 3: Configure Auto Loader to ingest data to Delta Lake \\nDatabricks recommends using Auto Loader for incremental data ingestion. Auto Loader automatically detects and processes new files as they arrive in cloud object storage.\\nDatabricks recommends storing data with Delta Lake. Delta Lake is an open source storage layer that provides ACID transactions and enables the data lakehouse. Delta Lake is the default format for tables created in Databricks.\\nTo configure Auto Loader to ingest data to a Delta Lake table, copy and paste the following code into the empty cell in your notebook:\\n\\n# Import functions\\nfrom pyspark.sql.functions import col, current_timestamp', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='# Define variables used in code below\\nfile_path = \"/databricks-datasets/structured-streaming/events\"\\nusername = spark.sql(\"SELECT regexp_replace(current_user(), \\'[^a-zA-Z0-9]\\', \\'_\\')\").first()[0]\\ntable_name = f\"{username}_etl_quickstart\"\\ncheckpoint_path = f\"/tmp/{username}/_checkpoint/etl_quickstart\"\\n\\n# Clear out data from previous demo execution\\nspark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\\ndbutils.fs.rm(checkpoint_path, True)\\n\\n# Configure Auto Loader to ingest JSON data to a Delta table\\n(spark.readStream\\n  .format(\"cloudFiles\")\\n  .option(\"cloudFiles.format\", \"json\")\\n  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\\n  .load(file_path)\\n  .select(\"*\", col(\"_metadata.file_path\").alias(\"source_file\"), current_timestamp().alias(\"processing_time\"))\\n  .writeStream\\n  .option(\"checkpointLocation\", checkpoint_path)\\n  .trigger(availableNow=True)\\n  .toTable(table_name))', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='// Imports\\nimport org.apache.spark.sql.functions.current_timestamp\\nimport org.apache.spark.sql.streaming.Trigger\\nimport spark.implicits._\\n\\n// Define variables used in code below\\nval file_path = \"/databricks-datasets/structured-streaming/events\"\\nval username = spark.sql(\"SELECT regexp_replace(current_user(), \\'[^a-zA-Z0-9]\\', \\'_\\')\").first.get(0)\\nval table_name = s\"${username}_etl_quickstart\"\\nval checkpoint_path = s\"/tmp/${username}/_checkpoint\"\\n\\n// Clear out data from previous demo execution\\nspark.sql(s\"DROP TABLE IF EXISTS ${table_name}\")\\ndbutils.fs.rm(checkpoint_path, true)', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='// Configure Auto Loader to ingest JSON data to a Delta table\\nspark.readStream\\n  .format(\"cloudFiles\")\\n  .option(\"cloudFiles.format\", \"json\")\\n  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\\n  .load(file_path)\\n  .select($\"*\", $\"_metadata.file_path\".as(\"source_file\"), current_timestamp.as(\"processing_time\"))\\n  .writeStream\\n  .option(\"checkpointLocation\", checkpoint_path)\\n  .trigger(Trigger.AvailableNow)\\n  .toTable(table_name)\\n\\n\\n\\n\\nNote\\nThe variables defined in this code should allow you to safely execute it without risk of conflicting with existing workspace assets or other users. Restricted network or storage permissions will raise errors when executing this code; contact your workspace administrator to troubleshoot these restrictions.\\n\\nTo learn more about Auto Loader, see What is Auto Loader?.\\n\\n\\nStep 4: Process and interact with data \\nNotebooks execute logic cell-by-cell. To execute the logic in your cell:', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='To run the cell you completed in the previous step, select the cell and press SHIFT+ENTER.\\nTo query the table you’ve just created, copy and paste the following code into an empty cell, then press SHIFT+ENTER to run the cell.\\n\\ndf = spark.read.table(table_name)\\n\\n\\nval df = spark.read.table(table_name)\\n\\n\\n\\n\\nTo preview the data in your DataFrame, copy and paste the following code into an empty cell, then press SHIFT+ENTER to run the cell.\\n\\ndisplay(df)\\n\\n\\ndisplay(df)\\n\\n\\n\\n\\n\\nTo learn more about interactive options for visualizing data, see Visualizations in Databricks notebooks.\\n\\n\\nStep 5: Schedule a job \\nYou can run Databricks notebooks as production scripts by adding them as a task in a Databricks job. In this step, you will create a new job that you can trigger manually.\\nTo schedule your notebook as a task:', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='Click Schedule on the right side of the header bar.\\nEnter a unique name for the Job name.\\nClick Manual.\\nIn the Cluster drop-down, select the cluster you created in step 1.\\nClick Create.\\nIn the window that appears, click Run now.\\nTo see the job run results, click the  icon next to the Last run timestamp.\\n\\nFor more information on jobs, see What is Databricks Jobs?.\\n\\n\\nAdditional Integrations \\nLearn more about integrations and tools for data engineering with Databricks:\\n\\nConnect your favorite IDE\\nUse dbt with Databricks\\nLearn about the Databricks Command Line Interface (CLI)\\nLearn about the Databricks Terraform Provider\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/getting-started/etl-quick-start.html', 'title': 'Run your first ETL workload on Databricks | Databricks on AWS', 'description': 'Learn how to use Databricks to quickly develop and deploy your first ETL pipeline for data orchestration.', 'language': 'en-US'}),\n",
              " Document(page_content='Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nTutorial: Run an end-to-end lakehouse analytics pipeline\\n\\n\\n\\n\\n\\n\\n\\nTutorial: Run an end-to-end lakehouse analytics pipeline \\nThis tutorial shows you how to set up an end-to-end analytics pipeline for a Databricks lakehouse.\\n\\nImportant\\nThis tutorial uses interactive notebooks to complete common ETL tasks in Python on Unity Catalog enabled clusters. If you are not using Unity Catalog, see Run your first ETL workload on Databricks.\\n\\n\\nTasks in this tutorial \\nBy the end of this article, you will feel comfortable:', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Launching a Unity Catalog enabled compute cluster.\\nCreating a Databricks notebook.\\nWriting and reading data from a Unity Catalog external location.\\nConfiguring incremental data ingestion to a Unity Catalog table with Auto Loader.\\nExecuting notebook cells to process, query, and preview data.\\nScheduling a notebook as a Databricks job.\\nQuerying Unity Catalog tables from Databricks SQL\\n\\nDatabricks provides a suite of production-ready tools that allow data professionals to quickly develop and deploy extract, transform, and load (ETL) pipelines. Unity Catalog allows data stewards to configure and secure storage credentials, external locations, and database objects for users throughout an organization. Databricks SQL allows analysts to run SQL queries against the same tables used in production ETL workloads, allowing for real time business intelligence at scale.\\n\\n\\n\\nRequirements \\n\\nYou are logged into Databricks.', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Note\\nIf you do not have cluster control privileges, you can still complete most of the steps below as long as you have access to a cluster.\\nIf you only have access to the Databricks SQL workspace, see Set up your workspace to use Databricks SQL.\\n\\n\\n\\nStep 1: Create a cluster \\nTo do exploratory data analysis and data engineering, create a cluster to provide the compute resources needed to execute commands.\\n\\nClick  Compute in the sidebar.\\nClick  New in the sidebar, then select Cluster. This opens the New Cluster/Compute page.\\nSpecify a unique name for the cluster.\\nSelect the Single node radio button.\\nSelect Single User from the Access mode dropdown.\\nMake sure your email address is visible in the Single User field.\\nSelect the desired Databricks runtime version, 11.1 or above to use Unity Catalog.\\nClick Create compute to create the cluster.\\n\\nTo learn more about Databricks clusters, see Compute.', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Step 2: Create a Databricks notebook \\nTo get started writing and executing interactive code on Databricks, create a notebook.\\n\\nClick  New in the sidebar, then click Notebook.\\nOn the Create Notebook page:\\n\\nSpecify a unique name for your notebook.\\nMake sure the default language is set to Python.\\nUse the Connect dropdown menu to select the cluster you created in step 1 from the Cluster dropdown.\\n\\n\\n\\nThe notebook opens with one empty cell.\\nTo learn more about creating and managing notebooks, see Manage notebooks.', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Step 3: Write and read data from an external location managed by Unity Catalog \\nDatabricks recommends using Auto Loader for incremental data ingestion. Auto Loader automatically detects and processes new files as they arrive in cloud object storage.\\nUse Unity Catalog to manage secure access to external locations. Users or service principals with READ FILES permissions on an external location can use Auto Loader to ingest data.\\nNormally, data will arrive in an external location due to writes from other systems. In this demo, you can simulate data arrival by writing out JSON files to an external location.\\nCopy the code below into a notebook cell. Replace the string value for catalog with the name of a catalog with CREATE CATALOG and USE CATALOG permissions. Replace the string value for external_location with the path for an external location with READ FILES, WRITE FILES, and CREATE EXTERNAL TABLE permissions.', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='External locations can be defined as an entire storage container, but often point to a directory nested in a container.\\nThe correct format for an external location path is \"s3://bucket-name/path/to/external_location\".', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='external_location = \"<your-external-location>\"\\n catalog = \"<your-catalog>\"\\n\\n dbutils.fs.put(f\"{external_location}/filename.txt\", \"Hello world!\", True)\\n display(dbutils.fs.head(f\"{external_location}/filename.txt\"))\\n dbutils.fs.rm(f\"{external_location}/filename.txt\")\\n\\n display(spark.sql(f\"SHOW SCHEMAS IN {catalog}\"))', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Executing this cell should print a line that reads 12 bytes, print the string “Hello world!”, and display all the databases present in the catalog provided. If you are unable to get this cell to run, confirm that you are in a Unity Catalog enabled workspace and request proper permissions from your workspace administrator to complete this tutorial.\\nThe Python code below uses your email address to create a unique database in the catalog provided and a unique storage location in external location provided. Executing this cell will remove all data associated with this tutorial, allowing you to execute this example idempotently. A class is defined and instantiated that you will use to simulate batches of data arriving from a connected system to your source external location.\\nCopy this code to a new cell in your notebook and execute it to configure your environment.', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Note\\nThe variables defined in this code should allow you to safely execute it without risk of conflicting with existing workspace assets or other users. Restricted network or storage permissions will raise errors when executing this code; contact your workspace administrator to troubleshoot these restrictions.\\n\\n\\nfrom pyspark.sql.functions import col\\n\\n# Set parameters for isolation in workspace and reset demo\\nusername = spark.sql(\"SELECT regexp_replace(current_user(), \\'[^a-zA-Z0-9]\\', \\'_\\')\").first()[0]\\ndatabase = f\"{catalog}.e2e_lakehouse_{username}_db\"\\nsource = f\"{external_location}/e2e-lakehouse-source\"\\ntable = f\"{database}.target_table\"\\ncheckpoint_path = f\"{external_location}/_checkpoint/e2e-lakehouse-demo\"\\n\\nspark.sql(f\"SET c.username=\\'{username}\\'\")\\nspark.sql(f\"SET c.database={database}\")\\nspark.sql(f\"SET c.source=\\'{source}\\'\")\\n\\nspark.sql(\"DROP DATABASE IF EXISTS ${c.database} CASCADE\")\\nspark.sql(\"CREATE DATABASE ${c.database}\")\\nspark.sql(\"USE ${c.database}\")', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='# Clear out data from previous demo execution\\ndbutils.fs.rm(source, True)\\ndbutils.fs.rm(checkpoint_path, True)\\n\\n\\n# Define a class to load batches of data to source\\nclass LoadData:\\n\\n    def __init__(self, source):\\n        self.source = source\\n\\n    def get_date(self):\\n        try:\\n            df = spark.read.format(\"json\").load(source)\\n        except:\\n            return \"2016-01-01\"\\n        batch_date = df.selectExpr(\"max(distinct(date(tpep_pickup_datetime))) + 1 day\").first()[0]\\n        if batch_date.month == 3:\\n            raise Exception(\"Source data exhausted\")\\n        return batch_date\\n\\n    def get_batch(self, batch_date):\\n        return (\\n            spark.table(\"samples.nyctaxi.trips\")\\n            .filter(col(\"tpep_pickup_datetime\").cast(\"date\") == batch_date)\\n        )\\n\\n    def write_batch(self, batch):\\n        batch.write.format(\"json\").mode(\"append\").save(self.source)', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='def land_batch(self):\\n        batch_date = self.get_date()\\n        batch = self.get_batch(batch_date)\\n        self.write_batch(batch)\\n\\nRawData = LoadData(source)\\n\\n\\nYou can now land a batch of data by copying the following code into a cell and executing it. You can manually execute this cell up to 60 times to trigger new data arrival.\\nRawData.land_batch()\\n\\n\\n\\n\\nStep 4: Configure Auto Loader to ingest data to Unity Catalog \\nDatabricks recommends storing data with Delta Lake. Delta Lake is an open source storage layer that provides ACID transactions and enables the data lakehouse. Delta Lake is the default format for tables created in Databricks.\\nTo configure Auto Loader to ingest data to a Unity Catalog table, copy and paste the following code into an empty cell in your notebook:\\n# Import functions\\nfrom pyspark.sql.functions import col, current_timestamp', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='# Configure Auto Loader to ingest JSON data to a Delta table\\n(spark.readStream\\n  .format(\"cloudFiles\")\\n  .option(\"cloudFiles.format\", \"json\")\\n  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\\n  .load(file_path)\\n  .select(\"*\", col(\"_metadata.file_path\").alias(\"source_file\"), current_timestamp().alias(\"processing_time\"))\\n  .writeStream\\n  .option(\"checkpointLocation\", checkpoint_path)\\n  .trigger(availableNow=True)\\n  .option(\"mergeSchema\", \"true\")\\n  .toTable(table))\\n\\n\\nTo learn more about Auto Loader, see What is Auto Loader?.\\nTo learn more about Structured Streaming with Unity Catalog, see Using Unity Catalog with Structured Streaming.\\n\\n\\nStep 5: Process and interact with data \\nNotebooks execute logic cell-by-cell. Use these steps to execute the logic in your cell:', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='To run the cell you completed in the previous step, select the cell and press SHIFT+ENTER.\\nTo query the table you’ve just created, copy and paste the following code into an empty cell, then press SHIFT+ENTER to run the cell.\\ndf = spark.read.table(table_name)\\n\\n\\n\\nTo preview the data in your DataFrame, copy and paste the following code into an empty cell, then press SHIFT+ENTER to run the cell.\\ndisplay(df)\\n\\n\\n\\n\\nTo learn more about interactive options for visualizing data, see Visualizations in Databricks notebooks.\\n\\n\\nStep 6: Schedule a job \\nYou can run Databricks notebooks as production scripts by adding them as a task in a Databricks job. In this step, you will create a new job that you can trigger manually.\\nTo schedule your notebook as a task:', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Click Schedule on the right side of the header bar.\\nEnter a unique name for the Job name.\\nClick Manual.\\nIn the Cluster drop-down, select the cluster you created in step 1.\\nClick Create.\\nIn the window that appears, click Run now.\\nTo see the job run results, click the  icon next to the Last run timestamp.\\n\\nFor more information on jobs, see What is Databricks Jobs?.\\n\\n\\nStep 7: Query table from Databricks SQL \\nAnyone with the USE CATALOG permission on the current catalog, the USE SCHEMA permission on the current schema, and SELECT permissions on the table can query the contents of the table from their preferred Databricks API.\\nYou need access to a running SQL warehouse to execute queries in Databricks SQL.\\nThe table you created earlier in this tutorial has the name target_table. You can query it using the catalog you provided in the first cell and the database with the patern e2e_lakehouse_<your-username>. You can use Catalog Explorer to find the data objects that you created.', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Additional Integrations \\nLearn more about integrations and tools for data engineering with Databricks:\\n\\nConnect your favorite IDE\\nUse dbt with Databricks\\nLearn about the Databricks Command Line Interface (CLI)\\nLearn about the Databricks Terraform Provider\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/getting-started/lakehouse-e2e.html', 'title': 'Tutorial: Run an end-to-end lakehouse analytics pipeline | Databricks on AWS', 'description': 'Run your first end-to-end analytics pipeline in a Databricks lakehouse.', 'language': 'en-US'}),\n",
              " Document(page_content='Get free Databricks training | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nSign up for a free trial\\nNavigate the workspace\\nQuery data from a notebook\\nBuild a basic ETL pipeline\\nBuild an end-to-end data pipeline\\nBuild a simple lakehouse analytics pipeline\\nBest practices\\nFree training\\n\\n\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data', metadata={'source': 'http://docs.databricks.com/getting-started/free-training.html', 'title': 'Get free Databricks training | Databricks on AWS', 'description': 'Access free training for Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Work with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nGet started: Account and workspace setup \\nGet free Databricks training\\n\\n\\n\\n\\n\\n\\n\\nGet free Databricks training \\nAs a customer, you have access to all Databricks free customer training offerings. These offerings include courses, recorded webinars, and quarterly product roadmap webinars. You can access the material from your Databricks Academy account. Follow these steps to get started:\\n\\nGo to Databricks Academy and click the red Academy login button in the top navigation.', metadata={'source': 'http://docs.databricks.com/getting-started/free-training.html', 'title': 'Get free Databricks training | Databricks on AWS', 'description': 'Access free training for Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='If you’ve logged into Databricks Academy before, use your existing credentials.\\nIf you’ve never logged into Databricks Academy, a customer account has been created for you, using your Databricks username, usually your work email address. You must reset your password. It may take up to 24 hours for the training pathway to appear in your account.\\n\\n\\nAfter you log into your Databricks Academy account, click  in the top left corner.\\n\\nClick Course Catalog.\\nThe catalogs available to you appear. Databricks Academy organizes groupings of learning content into catalogs, which include courses and learning paths.\\n\\n\\n\\nIf you’ve followed the steps above and do not see the pathways in your account, please file a training support ticket.\\nThe Databricks documentation also provides many tutorials and quickstarts that can help you get up to speed on the platform, both here in the Getting Started section and in other sections:', metadata={'source': 'http://docs.databricks.com/getting-started/free-training.html', 'title': 'Get free Databricks training | Databricks on AWS', 'description': 'Access free training for Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='Quickstart\\nApache Spark\\nLoad data into a Databricks lakehouse\\nSample datasets\\nDataFrames\\nDelta Lake\\nStructured Streaming\\nMachine Learning\\n\\nThe Knowledge Base provides troubleshooting tips and answers to frequently asked questions.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/getting-started/free-training.html', 'title': 'Get free Databricks training | Databricks on AWS', 'description': 'Access free training for Databricks.', 'language': 'en-US'}),\n",
              " Document(page_content='SQL language reference | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Reference\\nPython, SparkR & Scala intros\\nREST API reference\\nMLFlow API\\nFeature Store Python API\\nApache Spark API\\nDelta Lake API\\nDelta Live Tables API\\nSQL language reference\\n\"Applies to\" label\\nHow to read a syntax diagram\\nHow to add comments to SQL statements\\nConfiguration parameters\\nData types and literals\\nFunctions\\nSQL data type rules\\nDatetime patterns\\nH3 geospatial functions\\nLambda functions\\nWindow functions\\nIdentifiers\\nNames\\nIDENTIFIER clause\\nNULL semantics\\nExpressions\\nParameter markers\\nVariables\\nName resolution\\nJSON path expressions\\nPartitions\\nANSI compliance in Databricks Runtime\\nApache Hive compatibility\\nPrincipals\\nPrivileges and securable objects in Unity Catalog\\nPrivileges and securable objects in the Hive metastore\\nRefresh Unity Catalog metadata\\nExternal locations\\nExternal tables\\nStorage credentials\\nVolumes\\nDelta Sharing\\nFederated queries (Lakehouse Federation)\\nInformation schema\\nReserved words\\nALTER CATALOG\\nALTER CONNECTION\\nALTER CREDENTIAL\\nALTER DATABASE\\nALTER LOCATION', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='ALTER LOCATION\\nALTER PROVIDER\\nALTER RECIPIENT\\nALTER STREAMING TABLE\\nADD CONSTRAINT clause\\nDROP CONSTRAINT clause\\nALTER TABLE … COLUMN clause\\nALTER TABLE … PARTITION\\nCLUSTER BY clause (TABLE)\\nColumn mask clause\\nROW FILTER clause\\nALTER TABLE\\nALTER SCHEMA\\nALTER SHARE\\nALTER VIEW\\nALTER VOLUME\\nCOMMENT ON\\nCREATE BLOOMFILTER INDEX\\nCREATE CATALOG\\nCREATE CONNECTION\\nCREATE DATABASE\\nCREATE FUNCTION (SQL)\\nCREATE FUNCTION (External)\\nCREATE LOCATION\\nCREATE MATERIALIZED VIEW\\nCREATE RECIPIENT\\nCREATE SCHEMA\\nCREATE SERVER\\nCREATE SHARE\\nCREATE STREAMING TABLE\\nCREATE TABLE [USING]\\nCREATE TABLE LIKE\\nCONSTRAINT clause\\nCREATE TABLE CLONE\\nTable properties and table options\\nCREATE TABLE with Hive format\\nCREATE TABLE\\nCREATE VIEW\\nCREATE VOLUME\\nDECLARE VARIABLE\\nDROP BLOOMFILTER INDEX\\nDROP CATALOG\\nDROP CONNECTION\\nDROP DATABASE\\nDROP CREDENTIAL\\nDROP FUNCTION\\nDROP LOCATION\\nDROP PROVIDER\\nDROP RECIPIENT\\nDROP SCHEMA\\nDROP SHARE\\nDROP TABLE\\nDROP VARIABLE\\nDROP VIEW\\nDROP VOLUME\\nMSCK REPAIR TABLE', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='MSCK REPAIR TABLE\\nREFRESH FOREIGN (CATALOG, SCHEMA, or TABLE)\\nREFRESH (MATERIALIZED VIEW or STREAMING TABLE)\\nSYNC\\nTRUNCATE TABLE\\nUNDROP TABLE\\nCOPY INTO\\nDELETE FROM\\nINSERT INTO\\nINSERT OVERWRITE DIRECTORY\\nINSERT OVERWRITE DIRECTORY with Hive format\\nLOAD DATA\\nMERGE INTO\\nUPDATE\\nQuery\\nSELECT\\nVALUES\\nEXPLAIN\\nCACHE SELECT\\nCONVERT TO DELTA\\nDESCRIBE HISTORY\\nFSCK REPAIR TABLE\\nGENERATE\\nOPTIMIZE\\nREORG TABLE\\nRESTORE\\nVACUUM\\nANALYZE TABLE\\nCACHE TABLE\\nCLEAR CACHE\\nREFRESH CACHE\\nREFRESH FUNCTION\\nREFRESH TABLE\\nUNCACHE TABLE\\nDESCRIBE CATALOG\\nDESCRIBE CONNECTION\\nDESCRIBE CREDENTIAL\\nDESCRIBE DATABASE\\nDESCRIBE FUNCTION\\nDESCRIBE LOCATION\\nDESCRIBE PROVIDER\\nDESCRIBE QUERY\\nDESCRIBE RECIPIENT\\nDESCRIBE SCHEMA\\nDESCRIBE SHARE\\nDESCRIBE TABLE\\nDESCRIBE VOLUME\\nLIST\\nSHOW ALL IN SHARE\\nSHOW CATALOGS\\nSHOW COLUMNS\\nSHOW CONNECTIONS\\nSHOW CREATE TABLE\\nSHOW CREDENTIALS\\nSHOW DATABASES\\nSHOW FUNCTIONS\\nSHOW GROUPS\\nSHOW LOCATIONS\\nSHOW PARTITIONS\\nSHOW PROVIDERS\\nSHOW RECIPIENTS\\nSHOW SCHEMAS\\nSHOW SHARES\\nSHOW SHARES IN PROVIDER', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='SHOW TABLE\\nSHOW TABLES\\nSHOW TABLES DROPPED\\nSHOW TBLPROPERTIES\\nSHOW USERS\\nSHOW VIEWS\\nSHOW VOLUMES\\nEXECUTE IMMEDIATE\\nRESET\\nSET\\nSET TIMEZONE\\nSET VARIABLE\\nUSE CATALOG\\nUSE DATABASE\\nUSE SCHEMA\\nADD ARCHIVE\\nADD FILE\\nADD JAR\\nLIST ARCHIVE\\nLIST FILE\\nLIST JAR\\nALTER GROUP\\nCREATE GROUP\\nDENY\\nDROP GROUP\\nGRANT\\nGRANT SHARE\\nREPAIR PRIVILEGES\\nREVOKE\\nREVOKE SHARE\\nSHOW GRANTS\\nSHOW GRANTS ON SHARE\\nSHOW GRANTS TO RECIPIENT', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Resources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nDatabricks reference documentation \\nLanguage-specific introductions to Databricks \\nSQL language reference\\n\\n\\n\\n\\n\\n\\n\\nSQL language reference \\nThis is a SQL command reference for Databricks SQL and Databricks Runtime.\\nFor information about using SQL with Delta Live Tables, see Delta Live Tables SQL language reference.\\n\\nGeneral reference \\nThis general reference describes data types, functions, identifiers, literals, and semantics:', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='\"Applies to\" label\\nHow to read a syntax diagram\\nHow to add comments to SQL statements\\nConfiguration parameters\\nData types and literals\\nFunctions\\nSQL data type rules\\nDatetime patterns\\nH3 geospatial functions\\nLambda functions\\nWindow functions\\nIdentifiers\\nNames\\nIDENTIFIER clause\\nNULL semantics\\nExpressions\\nParameter markers\\nVariables\\nName resolution\\nJSON path expressions\\nPartitions\\nANSI compliance in Databricks Runtime\\nApache Hive compatibility\\nPrincipals\\nPrivileges and securable objects in Unity Catalog\\nPrivileges and securable objects in the Hive metastore\\nRefresh Unity Catalog metadata\\nExternal locations\\nExternal tables\\nStorage credentials\\nVolumes\\nDelta Sharing\\nFederated queries (Lakehouse Federation)\\nInformation schema\\nReserved words\\n\\n\\n\\n\\nDDL statements \\nYou use data definition statements to create or modify the structure of database objects in a database:', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='ALTER CATALOG\\nALTER CONNECTION\\nALTER CREDENTIAL\\nALTER DATABASE\\nALTER LOCATION\\nALTER PROVIDER\\nALTER RECIPIENT\\nALTER STREAMING TABLE\\nALTER TABLE\\nALTER SCHEMA\\nALTER SHARE\\nALTER VIEW\\nALTER VOLUME\\nCOMMENT ON\\nCREATE BLOOMFILTER INDEX\\nCREATE CATALOG\\nCREATE CONNECTION\\nCREATE DATABASE\\nCREATE FUNCTION (SQL)\\nCREATE FUNCTION (External)\\nCREATE LOCATION\\nCREATE MATERIALIZED VIEW\\nCREATE RECIPIENT\\nCREATE SCHEMA\\nCREATE SERVER\\nCREATE SHARE\\nCREATE STREAMING TABLE\\nCREATE TABLE\\nCREATE VIEW\\nCREATE VOLUME\\nDECLARE VARIABLE\\nDROP BLOOMFILTER INDEX\\nDROP CATALOG\\nDROP CONNECTION\\nDROP DATABASE\\nDROP CREDENTIAL\\nDROP FUNCTION\\nDROP LOCATION\\nDROP PROVIDER\\nDROP RECIPIENT\\nDROP SCHEMA\\nDROP SHARE\\nDROP TABLE\\nDROP VARIABLE\\nDROP VIEW\\nDROP VOLUME\\nMSCK REPAIR TABLE\\nREFRESH FOREIGN (CATALOG, SCHEMA, or TABLE)\\nREFRESH (MATERIALIZED VIEW or STREAMING TABLE)\\nSYNC\\nTRUNCATE TABLE\\nUNDROP TABLE\\n\\n\\n\\n\\nDML statements \\nYou use data manipulation statements to add, change, or delete data from a Delta Lake table:', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='COPY INTO\\nDELETE FROM\\nINSERT INTO\\nINSERT OVERWRITE DIRECTORY\\nINSERT OVERWRITE DIRECTORY with Hive format\\nLOAD DATA\\nMERGE INTO\\nUPDATE\\n\\n\\n\\n\\nData retrieval statements \\nYou use a query to retrieve rows from one or more tables according to the specified clauses. The full syntax\\nand brief description of supported clauses are explained in the Query article.\\nThe related SQL statements SELECT and VALUES are also included in this section.\\n\\n\\nQuery\\nSELECT\\nVALUES\\n\\n\\nDatabricks SQL also provides the ability to generate the logical and physical plan for a query using the EXPLAIN statement.\\n\\n\\nEXPLAIN\\n\\n\\n\\n\\nDelta Lake statements \\nYou use Delta Lake SQL statements to manage tables stored in Delta Lake format:\\n\\n\\nCACHE SELECT\\nCONVERT TO DELTA\\nDESCRIBE HISTORY\\nFSCK REPAIR TABLE\\nGENERATE\\nOPTIMIZE\\nREORG TABLE\\nRESTORE\\nVACUUM\\n\\n\\n\\nFor details on using Delta Lake statements, see What is Delta Lake?.', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Auxiliary statements \\nYou use auxiliary statements to collect statistics, manage caching,\\nexplore metadata, set configurations, and manage resources:\\n\\n\\nAnalyze statement\\nApache Spark Cache statements\\nDescribe statements\\nShow statements\\nConfiguration, variable management, and misc statements\\nResource management\\n\\n\\n\\nAnalyze statement \\n\\n\\nANALYZE TABLE\\n\\n\\n\\n\\nApache Spark Cache statements \\nApplies to:  Databricks Runtime\\n\\n\\nCACHE TABLE\\nCLEAR CACHE\\nREFRESH CACHE\\nREFRESH FUNCTION\\nREFRESH TABLE\\nUNCACHE TABLE\\n\\n\\n\\n\\nDescribe statements \\n\\n\\nDESCRIBE CATALOG\\nDESCRIBE CONNECTION\\nDESCRIBE CREDENTIAL\\nDESCRIBE DATABASE\\nDESCRIBE FUNCTION\\nDESCRIBE LOCATION\\nDESCRIBE PROVIDER\\nDESCRIBE QUERY\\nDESCRIBE RECIPIENT\\nDESCRIBE SCHEMA\\nDESCRIBE SHARE\\nDESCRIBE TABLE\\nDESCRIBE VOLUME\\n\\n\\n\\n\\nShow statements', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Show statements \\n\\n\\nLIST\\nSHOW ALL IN SHARE\\nSHOW CATALOGS\\nSHOW COLUMNS\\nSHOW CONNECTIONS\\nSHOW CREATE TABLE\\nSHOW CREDENTIALS\\nSHOW DATABASES\\nSHOW FUNCTIONS\\nSHOW GROUPS\\nSHOW LOCATIONS\\nSHOW PARTITIONS\\nSHOW PROVIDERS\\nSHOW RECIPIENTS\\nSHOW SCHEMAS\\nSHOW SHARES\\nSHOW SHARES IN PROVIDER\\nSHOW TABLE\\nSHOW TABLES\\nSHOW TABLES DROPPED\\nSHOW TBLPROPERTIES\\nSHOW USERS\\nSHOW VIEWS\\nSHOW VOLUMES\\n\\n\\n\\n\\nConfiguration, variable management, and misc statements \\n\\n\\nEXECUTE IMMEDIATE\\nRESET\\nSET\\nSET TIMEZONE\\nSET VARIABLE\\nUSE CATALOG\\nUSE DATABASE\\nUSE SCHEMA\\n\\n\\n\\n\\nResource management \\nApplies to:  Databricks Runtime\\n\\n\\nADD ARCHIVE\\nADD FILE\\nADD JAR\\nLIST ARCHIVE\\nLIST FILE\\nLIST JAR\\n\\n\\n\\n\\n\\nSecurity statements \\nYou use security SQL statements to manage access to data:\\n\\n\\nALTER GROUP\\nCREATE GROUP\\nDENY\\nDROP GROUP\\nGRANT\\nGRANT SHARE\\nREPAIR PRIVILEGES\\nREVOKE\\nREVOKE SHARE\\nSHOW GRANTS\\nSHOW GRANTS ON SHARE\\nSHOW GRANTS TO RECIPIENT\\n\\n\\nFor details about using these statements, see Hive metastore privileges and securable objects (legacy).', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/sql/language-manual/index.html', 'title': 'SQL language reference | Databricks on AWS', 'description': 'Learn about the SQL language constructs supported in Databricks SQL.', 'language': 'en-US'}),\n",
              " Document(page_content='Error handling in Databricks | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\nDocumentation\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n日本語\\nPortuguês (Brasil)\\n\\n\\n\\n\\n\\n        Amazon Web Services\\n    \\n\\n        Microsoft Azure\\n    \\n\\n        Google Cloud Platform\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nLoad data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDeveloper tools\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance\\nLakehouse architecture\\n\\nReference & resources', metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content='Reference\\nResources\\nStatus Page\\nLimits\\nDatabricks clouds and regions\\nSupported browsers\\nConfigure domain name firewall rules\\nPlatform release process\\nSubmit product feedback\\nSupport\\nError messages\\nSQLSTATE codes\\nError classes in Databricks\\n\\n\\n\\n\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated Jan 26, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nResources \\nError handling in Databricks\\n\\n\\n\\n\\n\\n\\n\\nError handling in Databricks \\nApplies to:  Databricks SQL  Databricks Runtime 12.2 and above\\n\\n\\n\\nError components \\nWhen Databricks raises an error it includes the following components:\\n\\nError Class\\nA descriptive, human-readable, string unique to the error condition.\\nSome error classes include sublasses.\\nFor example: ‘TABLE_OR_VIEW_NOT_FOUND’, and ‘INCOMPLETE_TYPE_DEFINITION.ARRAY’.\\nFor a list of all error classes see Error Classes.', metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content=\"SQLSTATE\\nA five character long string, grouping error classes into a standard format supported by many products and APIs.\\nFor example: '42P01'\\nFor a full list of all SQLSTATEs used by Databricks see SQLSTATEs.\\n\\nParameterized Message\\nThe error message with placeholders for the parameters.\\nFor example : ‘TABLE_OR_VIEW_NOT_FOUND’ includes the following message:\\nThe table or view <relationName> cannot be found.\\n\\n\\nYou can use the parameterized message to render an error message by mapping message parameter values to the parameter tags <parameter>.\\n\\nMessage Parameters\\nA map of parameters and values that provide additional information about the error.\\nFor example: 'relationName' -> 'main.default.tab1'.\", metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content=\"Message\\nThe completely rendered error message, including the error class and the SQLSTATE, with the parameters filled in.\\nFor example:\\n[TABLE_OR_VIEW_NOT_FOUND] The table or view `does_not_exist` cannot be found. Verify the spelling and correctness of the schema and catalog.\\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 14;\\n'Project [*]\\n+- 'UnresolvedRelation [does_not_exist], [], false\\n\\n\\n\\n\\n\\nWarning\\nMessage and Parameterized Message are not stable across releases.\\nThe message text may be changed or localized without notice.\\nTo programmatically handle an error condition, use the Error Class, SQLSTATE, and Message Parameters instead.\\n\\n\\n\\nHandling error conditions \\nApplies to:  Databricks SQL  Databricks Runtime 14.2 and above\\n\\nPreview\\nThis feature is in Public Preview.\", metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks provides language specific APIs to handle error conditions.\\n\\nPython \\nFor Python use pySparkException\\n\\nPySparkException.getErrorClass(): Returns the error class of the exception as a string.\\nPySparkException.getMessageParameters(): Returns the message parameters of the exception as a dictionary.\\nPySparkException.getSqlState(): Returns the SQLSTATE of the expression as a string.\\n\\n\\n\\nScala \\nFor Scala use SparkThrowable\\n\\ngetErrorClass(): Returns an error class as a string.\\ngetMessageParameters(): Returns a message parameters as a map.\\ngetSqlState(): Returns an SQLSTATE as a string.\\n\\n\\n\\nExamples \\n\\nCatch any exception and display error class, message parameters and SQLSTATE. Also display the default error message\\n\\nimport org.apache.spark.SparkThrowable', metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content='try {\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\n}\\ncatch {\\n  case ex: SparkThrowable =>\\n    println(\"Error Class       : \" + ex.getErrorClass)\\n    println(\"Message parameters: \" + ex.getMessageParameters())\\n    println(\"SQLSTATE          : \" + ex.getSqlState)\\n    println(ex)\\n}\\n\\n\\nfrom pyspark.errors import PySparkException\\n\\ntry:\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\nexcept PySparkException as ex:\\n  print(\"Error Class       : \" + ex.getErrorClass())\\n  print(\"Message parameters: \" + str(ex.getMessageParameters()))\\n  print(\"SQLSTATE          : \" + ex.getSqlState())\\n  print(ex)', metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content='Result\\n  Error Class       : TABLE_OR_VIEW_NOT_FOUND\\n  Message parameters: {\\'relationName\\': \\'`does_not_exist`\\'}\\n  SQLSTATE          : 42P01\\n  [TABLE_OR_VIEW_NOT_FOUND] The table or view `does_not_exist` cannot be found. Verify the spelling and correctness of the schema and catalog.\\n  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\\n  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 1 pos 14;\\n  \\'Project [*]\\n  +- \\'UnresolvedRelation [does_not_exist], [], false\\n\\n\\n\\nCatch the SQLSTATE 42P01 only and display a custom message:\\n\\nimport org.apache.spark.SparkThrowable\\n\\ntry {\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\n}\\ncatch {\\n  case ex: SparkThrowable if (ex.getSqlState == \"42P01\") =>\\n    println(\"I\\'m so sorry, but I cannot find: \" + ex.getMessageParameters().get(\"relationName\"))\\n}\\n\\n\\nfrom pyspark.errors import PySparkException', metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content='try:\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\nexcept PySparkException as ex:\\n  if (ex.getSqlState() == \"42P01\"):\\n    print(\"I\\'m so sorry, but I cannot find: \" + ex.getMessageParameters()[\\'relationName\\'])\\n  else:\\n    raise\\n\\n\\n\\nResult\\nI\\'m so sorry, but I cannot find: `does_not_exist`\\n\\n\\n\\nCatch the error class TABLE_OR_VIEW_NOT_FOUND only and display a custom message:\\n\\nimport org.apache.spark.SparkThrowable\\n\\ntry {\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\n}\\ncatch {\\n  case ex: SparkThrowable if (ex.getErrorClass == \"TABLE_OR_VIEW_NOT_FOUND\") =>\\n    println(\"I\\'m so sorry, but I cannot find: \" + ex.getMessageParameters().get(\"relationName\"))\\n}\\n\\n\\nfrom pyspark.errors import PySparkException\\n\\ntry:\\n  spark.sql(\"SELECT * FROM does_not_exist\").show()\\nexcept PySparkException as ex:\\n  if (ex.getErrorClass() == \"TABLE_OR_VIEW_NOT_FOUND\"):\\n    print(\"I\\'m so sorry, but I cannot find: \" + ex.getMessageParameters()[\\'relationName\\'])\\n  else:\\n    raise', metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content=\"Result\\nI'm so sorry, but I cannot find: `does_not_exist`\\n\\n\\n\\n\\n\\n\\n\\nUser raised exceptions \\nDatabricks provides the following functions to raise user defined errors:\\n\\nraise_error\\nRaises an exception with a custom error message.\\n\\nassert_true\\nRaises an error with an optional error message, if a condition is not met.\\n\\n\\nBoth functions return the error class ‘USER_RAISED_EXCEPTION’ and the SQLSTATE 'P0001' along with a user defined  message.\\n\\nExamples \\n> SELECT raise_error('This is a custom error message');\\n [USER_RAISED_EXCEPTION] This is a custom error message. SQLSTATE: P0001\\n\\n> SELECT assert_true(1 = 2, 'One is not two!');\\n [USER_RAISED_EXCEPTION] One is not two! SQLSTATE: P0001\\n\\n> SELECT assert_true(1 = 2);\\n [USER_RAISED_EXCEPTION] '(1 = 2)' is not true! SQLSTATE: P0001\\n\\n\\n\\n\\n\\nRelated \\n\\nassert_true function\\nraise_error function\", metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content='© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use', metadata={'source': 'http://docs.databricks.com/error-messages/index.html', 'title': 'Error handling in Databricks | Databricks on AWS', 'description': 'Error handling in Databricks', 'language': 'en-US'}),\n",
              " Document(page_content='Welcome to The Apache Software Foundation!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to Main Content\\n\\n\\n\\n\\n\\nToggle navigation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSponsor the ASF\\n\\n\\n\\nCommunity\\xa0\\n\\nContributor Getting Started\\nBecoming a Committer\\nCode of Conduct\\nCommunity Resources\\nCommunity Over Code\\nEvents\\nStore\\n\\n\\n\\nProjects\\xa0\\n\\nProjects\\nIncubator Projects\\nProjects Directory \\nMailing Lists \\nReport a Vulnerability\\n\\n\\n\\nDownloads\\xa0\\n\\nDistributions\\nReleases\\nInfrastructure Status\\nInfrastructure Statistics\\n\\n\\n\\nLearn\\xa0\\n\\nBlog\\nHow the ASF Works\\nThe Apache Way\\nLegal & Trademark\\nLicenses\\nGlossary\\nFAQ\\n\\n\\n\\nResources & Tools\\xa0\\n\\nDeveloper Information\\nWiki\\nIssues\\nSlack\\nSelf Serve Portal\\nInfrastructure\\nWhimsy\\nBrand Guidelines\\nProject Logos\\n\\n\\n\\nAbout\\xa0\\n\\nAbout\\nOur Sponsors\\nCorporate Sponsorship\\nIndividual Supporters\\nLeadership\\nMembers\\nDiversity & Inclusion\\nNewsroom\\nContact\\n\\n\\n\\nSearch', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Software for the Public Good¶\\nASF’s open source software is used ubiquitously around the world with more than 8,400 committers contributing to more than 320 active projects.\\nSee All ProjectsContribute\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"The most popular open source software is\\xa0Apache…\"\\nDZone, “What Open Source Software Do You\\xa0Use?”\\n\\n\\n\\n\\n\"Apache-style licensing may yield more adoption and\\xa0money.\"\\nMatt Asay, c|net\\n\\n\\n\\n\\n\"Apache’s \\'survival of the fittest\\' ethos breeds better\\xa0software\"\\nZDNet\\n\\n\\n\\n❮\\n❯\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLatest News¶\\nKeep up with the ASF\\'s news and announcements by subscribing to the\\n   \\t\\tApache Announcements List, as well as following the Foundation Blog,\\n\\t\\tApache Weekly News Round-Ups,\\n\\t\\t@TheASF on Twitter,\\n\\t\\tThe Apache Software Foundation on LinkedIn, on the ASF\\'s YouTube channel, and on Feathercast, the voice of the ASF.\\n\\n\\n\\n\\n\\nFoundation Blog\\nWhy Generative AI Guidance is Essential to Contributors of Open Source\\xa0\\n\\nContinue Reading\\xa0→', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Continue Reading\\xa0→\\n\\n\\nFoundation Blog\\nUpdate on EU Software Regulation: Lots of improvements &#038; good news\\n\\nContinue Reading\\xa0→\\n\\n\\nFoundation Blog\\nNew year, new goals!\\xa0\\n\\nContinue Reading\\xa0→\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReports¶\\nOfficial ASF reports and statements,\\n\\t\\t\\t\\t\\t\\t including Quarterly and Annual Reports, Vision Statement,\\n\\t\\t\\t\\t\\t\\t \"Apache is Open\", 5-Year Strategic Plan, and more.\\n\\n\\n\\n\\n\\n\\n\\nCommunity¶\\nGuidance and mentoring for those interested in participating in Apache projects and their communities.\\n\\t\\t\\t\\t\\t\\t\\tFrom Google Summer of Code to community events, get started here to learn how to become an\\n\\t\\t\\t\\t\\t\\t\\tApache contributor.\\n\\n\\n\\n\\n\\n\\n\\nThe Apache Way¶\\nOur consensus-driven, open development process was refined over the past 20 years and produced some of\\n\\t\\t\\t\\t\\t\\t\\tthe largest and longest-lived Open Source projects that have revolutionized the industry.', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Conferences¶\\n\"Tomorrow\\'s Technology Today\" since 1998. Intentionally intimate, offering unparalleled educational,\\n\\t\\t\\t\\t\\t\\t\\tnetworking, and collaboration opportunities.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout the Foundation¶\\nThe ASF is a 501(c)(3) charitable organization run almost exclusively by volunteers who oversee hundreds of projects. Our sponsors enable us to maintain the infrastructure needed to support them.\\n\\n\\nGovernanceSee our sponsors\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nASF Projects¶\\nThe ASF develops, stewards, and incubates hundreds of freely available, enterprise-grade projects that serve as the backbone for the most visible and widely used applications in computing today.\\n\\n\\nContribute¶\\nIf you wish to contribute to ASF projects, the Community Development site has tools, processes, and advice to help you get started.\\nGet Started\\n\\n\\nHost a Project¶\\nThe Apache Incubator provides a path for projects and their communities that want to enter the ASF.\\nLearn more\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFeatured Projects¶\\n\\n\\n\\n\\n\\nCXF\\n\\nYetus\\n\\nMINA', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Yetus\\n\\nMINA\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCXF\\nService Framework\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\nYetus\\nCollection of libraries and tools that enable contribution and release processes for software projects\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\nMINA\\nMultipurpose Infrastructure for Network Application\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIncubating Projects¶\\nThe Apache Incubator is the primary entry path into The Apache Software Foundation for projects and their communities wishing to become part of the Foundation’s efforts. \\n        All code donations from external organisations and existing external projects seeking to join the Apache community enter through the Incubator.\\n\\n\\n\\n\\n\\nMilagro\\n\\nBaremaps\\n\\nTraining\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMilagro\\nMilagro is core security infrastructure and crypto libraries for decentralized networks and distributed systems.\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\nBaremaps\\nApache Baremaps is a toolkit and a set of infrastructure components for creating, publishing, and operating online maps.\\nLearn More...', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Training\\nThe Training project aims to develop resources which can be used for training purposes in various media formats, languages and for various Apache and non-Apache target projects.\\nLearn More...\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nApache Project List¶\\n\\n\\n\\n\\n\\nOverview\\nAll Projects\\n\\nBy Category¶\\n\\nAttic\\nBig Data\\nBuild Management\\nCloud\\nContent\\nDatabases\\nFTP\\nGraphics\\nHTTP\\nHTTP-module\\nIncubating\\n\\nJavaEE\\nLibraries\\nMail\\nMobile\\nNetwork-client\\nNetwork-server\\nOSGi\\nRegExp\\nRetired\\nSearch\\nSecurity\\nSQL\\nTesting\\nVirtual-machine\\nWeb-framework\\nXML\\n\\n\\n\\n\\n\\n\\nBy Name¶\\n\\n\\n\\n\\nHTTP Server\\nA\\nAGE\\nAPISIX\\nAccumulo\\nActiveMQ\\nAiravata\\nAirflow\\nAllura\\nAmbari\\nAnt\\nArchiva\\nAries\\nArrow\\nAsterixDB\\nAtlas\\nAttic\\nAvro\\nAxis\\nB\\nBVal\\nBeam\\nBigtop\\nBloodhound\\nBookKeeper\\nBrooklyn\\nBuildStream\\nbRPC\\nC\\nCXF\\nCalcite\\nCamel\\nCarbonData\\nCassandra\\nCauseway\\nCayenne\\nCelix\\nCloudStack\\nCocoon', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Commons\\nCommunity Development\\nCordova\\nCouchDB\\nCreadur\\nCurator\\ncTAKES\\nD\\nDB\\nDaffodil\\nDataFu\\nDataSketches\\nDeltaSpike\\nDirectory\\nDolphinScheduler\\nDoris\\nDrill\\nDruid\\nDubbo\\nE\\nECharts\\nEmpire-db\\nEventMesh\\nF\\nFelix\\nFineract\\nFlagon\\nFlex\\nFlink\\nFlume\\nFluo\\nFreeMarker\\nG\\nGeode\\nGeronimo\\nGobblin\\nGora\\nGriffin\\nGroovy\\n\\n\\n\\n\\n\\n\\n\\n\\nGuacamole\\nGump\\nH\\nHAWQ\\nHBase\\nHadoop\\nHelix\\nHive\\nHop\\nHttpComponents\\nHudi\\nI\\nIceberg\\nIgnite\\nImpala\\nInLong\\nIncubator\\nIoTDB\\nJ\\nJMeter\\nJSPWiki\\nJackrabbit\\nJames\\nJena\\nJohnzon\\nJuneau\\njclouds\\nK\\nKafka\\nKaraf\\nKibble\\nKnox\\nKudu\\nKvrocks\\nKylin\\nKyuubi\\nL\\nLibcloud\\nLinkis\\n\\n\\n\\n\\nLogging Services\\nLucene\\nLucene.Net\\nM\\nMADlib\\nMINA\\nMahout\\nManifoldCF\\nMaven\\nMesos\\nMnemonic\\nMyFaces\\nMynewt\\nN\\nNetBeans\\nNiFi\\nNutch\\nNuttX\\nO\\nOFBiz\\nORC\\nOlingo\\nOozie\\nOpenDAL\\nOpenJPA\\nOpenMeetings\\nOpenNLP\\nOpenOffice\\nOpenWebBeans\\nOpenWhisk\\nOzone\\nP\\nPDFBox\\nPLC4X\\nPOI\\nParquet\\nPerl\\nPetri\\nPhoenix', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Pig\\nPinot\\nPivot\\nPortable Runtime (APR)\\nPortals\\nPulsar\\nQ\\nQpid\\nR\\nRanger\\nRatis\\nRocketMQ\\nRoller\\nRoyale\\nRya\\nS\\nSINGA\\nSIS\\nSamza\\nSantuario\\nSeaTunnel\\nSedona\\nSerf\\nServiceComb\\nServiceMix\\nShardingSphere\\nShenYu\\nShiro\\nSkyWalking\\nSling\\nSolr\\nSpamAssassin\\nSpark\\nSteve\\nStorm\\nStreamPipes\\nStreams\\nStruts\\nSubmarine\\n\\n\\n\\n\\nSubversion\\nSuperset\\nSynapse\\nSyncope\\nSystemDS\\nT\\nTVM\\nTapestry\\nTcl\\nTez\\nThrift\\nTika\\nTinkerPop\\nTomEE\\nTomcat\\nTraffic Control\\nTraffic Server\\nTsFile\\nTurbine\\nU\\nUIMA\\nUnomi\\nV\\nVCL\\nVelocity\\nW\\nWeb Services\\nWhimsy\\nWicket\\nX\\nXML Graphics\\nXalan\\nXerces\\nY\\nYetus\\nYuniKorn\\nZ\\nZeppelin\\nZooKeeper\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\n\\nContributor Getting Started\\nBecoming a Committer\\nCode of Conduct\\nCommunity Resources\\nCommunity Over Code\\nEvents\\nStore\\n\\n\\n\\nProjects\\n\\nProjects\\nIncubator Projects\\nProjects Directory \\nMailing Lists \\nReport a Vulnerability\\n\\n\\n\\nDownloads\\n\\nDistributions\\nReleases\\nInfrastructure Status\\nInfrastructure Statistics\\n\\n\\n\\nLearn\\n\\nBlog\\nHow the ASF Works\\nThe Apache Way\\nLegal & Trademark\\nLicenses\\nGlossary\\nFAQ', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Resources & Tools\\n\\nDeveloper Information\\nWiki\\nIssues\\nSlack\\nSelf Serve Portal\\nInfrastructure\\nWhimsy\\nBrand Guidelines\\nProject Logos\\n\\n\\n\\nAbout\\n\\nAbout\\nOur Sponsors\\nCorporate Sponsorship\\nIndividual Supporters\\nLeadership\\nMembers\\nDiversity & Inclusion\\nNewsroom\\nContact\\nPrivacy Policy\\n\\n\\n\\n\\n\\n\\nCopyright © 2024 The Apache Software Foundation, Licensed under the Apache License, Version 2.0.\\nApache and the Apache feather logo are trademarks of The Apache Software Foundation.', metadata={'source': 'http://www.apache.org/', 'title': 'Welcome to The Apache Software Foundation!', 'description': 'Home page of The Apache Software Foundation', 'language': 'en'}),\n",
              " Document(page_content='Privacy Notice | DatabricksSkip to main contentWhy Databricks DiscoverFor ExecutivesFor Startups Lakehouse Architecture CustomersFeatured StoriesSee All CustomersPartnersCloud ProvidersDatabricks on AWS, Azure, and GCPConsulting & System IntegratorsExperts to build, deploy and migrate to DatabricksTechnology PartnersConnect your existing tools to your LakehouseC&SI Partner ProgramBuild, deploy or migrate to the LakehouseData PartnersAccess the ecosystem of data consumersPartner SolutionsFind custom industry and migration solutionsBuilt on DatabricksBuild, market and grow your businessProduct Databricks PlatformPlatform OverviewA unified platform for data, analytics and AIData ManagementData reliability, security and performanceSharingAn open, secure, zero-copy sharing for all dataData WarehousingETL and orchestration for batch and streaming dataGovernanceUnified governance for all data, analytics and AI assetsReal-Time AnalyticsReal-time analytics, AI and applications made', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='applications made simpleArtificial IntelligenceA data-centric approach to AIData EngineeringETL and orchestration for batch and streaming dataMosaicMLTrain and deploy secure generative AI modelsData ScienceCollaborative data science at scaleIntegrations and DataMarketplaceOpen marketplace for data, analytics and AIIDE IntegrationsBuild on the Lakehouse in your favorite IDEPartner ConnectDiscover and integrate with the Databricks ecosystemPricingDatabricks PricingExplore product pricing, DBUs and moreCost CalculatorEstimate your compute costs on any cloudOpen SourceOpen Source TechnologiesLearn more about the innovations behind the platformSolutions Databricks for IndustriesFinancial ServicesRetailHealthcare & Life SciencesMedia and EntertainmentManufacturingCommunicationsPublic SectorSee All IndustriesCross Industry SolutionsConsumer Data Platform Cyber SecurityMigration & DeploymentData MigrationProfessional ServicesSolution AcceleratorsExplore AcceleratorsMove faster toward outcomes', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='toward outcomes that matterResources Training and CertificationLearning OverviewHub for training, certification, events and moreTraining OverviewDiscover curriculum tailored to your needsLearning PathsGuided learning by role and career pathCertificationGain recognition and differentiationUniversity AllianceWant to teach Databricks? See how.EventsData + AI SummitData + AI World TourEvent CalendarBlog and PodcastsDatabricks BlogExplore news, product announcements, and moreDatabricks Mosaic AI Research BlogDiscover the latest in our Gen AI researchData Brew PodcastLet’s talk data!Champions of Data + AI PodcastInsights from data leaders powering innovationGet HelpCustomer SupportDocumentationCommunityDive DeepResource CenterDemo CenterAbout CompanyWho We AreOur TeamDatabricks VenturesContact UsCareersWorking at DatabricksOpen JobsPressAwards and RecognitionNewsroomSecurity and TrustSecurity and TrustReady to get started?Get a DemoLoginContact UsTry DatabricksSkip to main', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='to main contentOverviewTermsMaster Cloud Services AgreementAcceptable Use PolicyExternal User TermsUS Public Sector ServicesCommunity Edition Terms of ServicePartner Terms and ConditionsWebsite Terms of UsePrivacyData Processing AddendumAmendment to Data Processing AddendumPrivacy NoticeInternational Data Transfers FAQDatabricks SubprocessorsCookie NoticeApplicant Privacy NoticeSecurityDatabricks SecuritySecurity AddendumTrust CenterCompliance and EthicsCode of ConductThird Party Code of ConductModern Slavery StatementPay Equity Report (France)Subscribe to UpdatesPrivacy NoticeThis Privacy Notice explains how Databricks, Inc. and its affiliates (“Databricks”, “we”, “our”, and “us”) collects, uses, shares and otherwise processes your personal information (also known as personal data) in connection with the use of Databricks websites and applications that link to this Privacy Notice (the “Sites”), our data processing platform products and services (the “Platform Services”) and in the', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='and in the usual course of business, such as in connection with our events, sales, and marketing activities (collectively, “Databricks Services”). It also contains information about your choices and privacy rights.Our ServicesWe provide the Platform Services to our customers and users (collectively, “Customers”) under an agreement with them and solely for their benefit and the benefit of personnel authorized to use the Platform Services (“Authorized Users”). Our processing of such data is governed by our agreement with the relevant Customer. This Privacy Notice does not apply to (i) the data that our Customers upload, submit or otherwise make available to the Platform Services and other data that we process on their behalf, as defined in our agreement with the Customer; (ii) any products, services, websites, or content that are offered by third parties or that have their own privacy notice; or (iii) personal information that we collect and process in connection with our recruitment', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='our recruitment activities, which is covered under our Applicant Privacy Notice.We recommend that you read this Privacy Notice in full to ensure that you are informed. However, if you only want to access a particular section of this Privacy Notice, you can click on the link below to go to that section.Information We Collect About YouHow We Use Your InformationHow We Share Your InformationInternational TransfersYour Choices and RightsAdditional Information for Certain JurisdictionsOther Important InformationChanges to this NoticeHow to Contact UsInformation We Collect About YouInformation that we collect from or about you includes information you provide, information we collect automatically, and information we receive from other sources.Information you provideWhen using our Databricks Services, we may collect certain information, such as your name, email address, phone number, postal address, job title, and company name. We may also collect other information that you provide through', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='you provide through your interactions with us, for example if you request information about our Platform Services, interact with our sales team or contact customer support, complete a survey, provide feedback or post comments, register for an event, or take part in marketing activities.\\xa0We may keep a record of your communications with us and other information you share during the course of the communications.When you create an account, for example, through our Sites or register to use our Platform Services, we may collect your personal information, such as your name and contact information. We may also collect credit card information if chosen by you as a payment method, which may be shared with our third party service providers, including for payment and billing purposes.\\xa0Information we collect automatically\\xa0We use standard automated data collection tools, such as cookies, web beacons, tracking pixels, tags, and similar tools, to collect information about how people use our Sites and', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='use our Sites and interact with our emails.For example, when you visit our Sites we (or an authorized third party) may collect certain information from you or your device. This may include information about your computer or device (such as operating system, device identifier, browser language, and Internet Protocol (IP) address), and information about your activities on our Sites (such as how you came to our Sites, access times, the links you click on, and other statistical information).\\xa0For example, your IP address may be used to derive general location information. We use this information to help us understand how you are using our Sites and how to better provide the Sites to you. We may also use web beacons and pixels in our emails. For example, we may place a pixel in our emails that notifies us when you click on a link in the email. We use these technologies to improve our communications.\\xa0The types of data collection tools we use may change over time as technology evolves. You', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='evolves. You can learn more about our use of cookies and similar tools, as well as how to opt out of certain data collection, by visiting our\\xa0Cookie Notice.\\xa0When you use our Platform Services, we automatically collect information about how you are using the Platform Services (“Usage Data”). While most Usage Data is not personal information, it may include information about your account (such as User ID, email address, or Internet Protocol (IP) address) and information about your computer or device (such as browser type and operating system). It may also include information about your activities within the Platform Services, such as the pages or features you access or use, the time spent on those pages or features, search terms entered, commands executed, information about the types and size of files analyzed via the Platform Services, and other statistical information relating to your use of the Platform Services. We collect Usage Data to provide, support and operate the Platform', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='the Platform Services, for network and information security, and to better understand how our Authorized Users and Customers are using the Platform Services to improve our products and services.\\xa0We may also use the information we collect automatically (for example, IP address, and unique device identifiers) to identify the same unique person across Databricks Services to provide a more seamless and personalized experience to you.\\xa0Information we receive from other sourcesWe may obtain information about you from third party sources, including resellers, distributors, business partners, event sponsors, security and fraud detection services, social media platforms, and publicly available sources. Examples of information that we receive from third parties include marketing and sales information (such as name, email address, phone number and similar contact information), and purchase, support and other information about your interactions with our Sites and Platform Services. We may combine', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='We may combine such information with the information we receive and collect from you.How We Use Your InformationWe use your personal information to provide, maintain, improve and update our Databricks Services. Our purposes for collecting your personal information include:to provide, maintain, deliver and update the Databricks Services;to create and maintain your Databricks account;to measure your use and improve Databricks Services, and to develop new products and services;for billing, payment, or account management; for example, to identify your account and correctly identify your usage of our products and services;to provide you with customer service and support;to register and provide you with training and certification programs;to investigate security issues, prevent fraud, or combat the illegal or controlled uses of our products and services;for sales phone calls for training and coaching purposes, quality assurance and administration (in accordance with applicable laws),', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='applicable laws), including to analyze sales calls using analytics tools to gain better insights into our interactions with customers;\\xa0to send you notifications about the Databricks Services, including technical notices, updates, security alerts, administrative messages and invoices;to respond to your questions, comments, and requests, including to keep in contact with you regarding the products and services you use;to tailor and send you newsletters, emails and other content to promote our products and services (you can always unsubscribe from our marketing emails by clicking\\xa0here) and to allow third party partners (like our event sponsors) to send you marketing communications about their services, in accordance with your preferences;to personalize your experience when using our Sites and Platform Services;for advertising purposes; for example, to display and measure advertising on third party websites;to contact you to conduct surveys and for market research purposes;to generate and', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='generate and analyze statistical information about how our Sites and Platform Services are used in the aggregate;for other legitimate interests or lawful business purposes; for example, customer surveys, collecting feedback, and conducting audits;to comply with our obligations under applicable law, legal process, or government regulation; andfor other purposes, where you have given consent.How We Share Your InformationWe may share your personal information with third parties as follows:with our affiliates and subsidiaries for the purposes described in this Privacy Notice;with our service providers who assist us in providing the Databricks Services, such as billing, payment card processing, customer support, sales and marketing, and data analysis, subject to confidentiality obligations and the requirement that those service providers do not sell your personal information;with our service providers who assist us with detecting and preventing fraud, security threats or other illegal or', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='or other illegal or malicious behavior, for example Sift who provides fraud detection services where your personal information is processed by Sift in accordance with its Privacy Notice available at https://sift.com/service-privacy;with third party business partners, such as resellers, distributors, and/or referral partners, who are involved in providing content, products or services to our prospects or Customers. We may also engage with third party partners who are working with us to organize or sponsor an event to which you have registered to enable them to contact you about the event or their services (but only where we have a lawful basis to do so, such as your consent where required by applicable law);with marketing partners, such as advertising providers that tailor online ads to your interests based on information they collect about your online activity (known as interest-based advertising);with the organization that is sponsoring your training or certification program, for', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='program, for example to notify them of your registration and completion of the course;when authorized by law or we deem necessary to comply with a legal process;when required to protect and defend the rights or property of Databricks or our Customers, including the security of our Sites, products and services (including the Platform Services);when necessary to protect the personal safety, property or other rights of the public, Databricks or our Customers;where it has been de-identified, including through aggregation or anonymization;when you instruct us to do so;where you have consented to the sharing of your information with third parties; orin connection with a merger, sale, financing or reorganization of all or part of our business.International TransfersDatabricks may transfer your personal information to countries other than your country of residence. In particular, we may transfer your personal information to the United States and other countries where our affiliates, business', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='business partners and services providers are located. These countries may not have equivalent data protection laws to the country where you reside.\\xa0Wherever we process your personal information, we take appropriate steps to ensure it is protected in accordance with this Privacy Notice and applicable data protection laws. These safeguards include implementing the European Commission’s Standard Contractual Clauses for transfers of personal information from the EEA or Switzerland between us and our business partners and service providers, and equivalent measures for transfers of personal information from the United Kingdom. Databricks also offers our Customers the ability to enter into a data processing addendum (DPA) that contains the Standard Contractual Clauses, for transfers between us and our Customers. We also make use of supplementary measures to ensure your information is adequately protected.\\xa0Data Privacy Framework NoticeDatabricks complies with the EU-U.S. Data Privacy', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='Data Privacy Framework (EU-U.S. DPF), the UK Extension to the EU-U.S. DPF, and the Swiss-U.S. Data Privacy Framework (Swiss-U.S. DPF) as set forth by the U.S. Department of Commerce.\\xa0 Databricks has certified to the U.S. Department of Commerce that it adheres to the EU-U.S. Data Privacy Framework Principles (EU-U.S. DPF Principles) with regard to the processing of personal data received from the European Union in reliance on the EU-U.S. DPF and from the United Kingdom (and Gibraltar) in reliance on the UK Extension to the EU-U.S. DPF.\\xa0 Databricks has certified to the U.S. Department of Commerce that it adheres to the Swiss-U.S. Data Privacy Framework Principles (Swiss-U.S. DPF Principles) with regard to the processing of personal data received from Switzerland in reliance on the Swiss-U.S. DPF.\\xa0 If there is any conflict between the terms in this privacy policy and the EU-U.S. DPF Principles and/or the Swiss-U.S. DPF Principles, the Principles shall govern.\\xa0 To learn more about the', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='more about the Data Privacy Framework (DPF) program, and to view our certification, please visit https://www.dataprivacyframework.gov/.\\xa0To learn more, visit our Data Privacy Framework Notice\\xa0here.Your Choices and RightsWe offer you choices regarding the collection, use and sharing of your personal information and we will respect the choices you make in accordance with applicable law. Please note that if you decide not to provide us with certain personal information, you may not be able to access certain features of the Sites or use the Platform Services.Account informationIf you want to correct, update or delete your account information, please log on to your Databricks account and update your profile.Opt out of marketingWe may periodically send you marketing communications that promote our products and services consistent with your choices. You may opt out from receiving such communications, either by following the unsubscribe instructions in the communication you receive or by', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='you receive or by clicking\\xa0here. Please note that we may still send you important service-related communications regarding our products or services, such as communications about your subscription or account, service announcements or security information.Your privacy rightsDepending upon your place of residence, you may have rights in relation to your personal information. Please review the jurisdiction specific sections below, including the disclosures for California residents. Depending on applicable data protection laws, those rights may include asking us to provide certain information about our collection and processing of your personal information, or requesting access, correction or deletion of your personal information. You also have the right to withdraw your consent, to the extent we rely on consent to process your personal information.\\xa0If you wish to exercise any of your rights under applicable data protection laws, submit a request online by completing the request form here', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='request form here or emailing us at [email\\xa0protected]. We will respond to requests that we receive in accordance with applicable laws. Databricks may take certain steps to verify your request using information available to us, such as your email address or other information associated with your Databricks account, and if needed we may ask you to provide additional information for the purposes of verifying your request. Any information you provide to us for verification purposes will only be used to process and maintain a record of your request.As described above, we may also process personal information that has been submitted by a Customer to our Platform Services. If your personal information has been submitted to the Platform Services by or on behalf of a Databricks Customer and you wish to exercise your privacy rights, please direct your request to the relevant Customer. For other inquiries, please contact us at [email\\xa0protected].Additional Information for Certain', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='for Certain JurisdictionsThis section provides additional information about our privacy practices for certain jurisdictions.CaliforniaIf you are a California resident, the California Consumer Privacy Act (“CCPA”) requires us to provide you with additional information regarding your rights with respect to your “personal information. This information is described in our Supplemental Privacy Notice to California Residents.\\xa0\\xa0Other US StatesDepending on applicable laws in your state of residence, you may request to: (1) confirm whether or not we process your personal information; (2) access, correct, or delete personal information we maintain about you; (3) receive a portable copy of such personal information; and/or (4) restrict or opt out of certain processing of your personal information, such as targeted advertising, or profiling in furtherance of decisions that produce legal or similarly significant effects. If we refuse to take action on a request, we will provide instructions on how', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='instructions on how you may appeal the decision. We will respond to requests consistent with applicable law.European Economic Area, UK and SwitzerlandIf you are located in the European Economic Area, United Kingdom or Switzerland, the controller of your personal information is Databricks, Inc., 160 Spear Street, Suite 1300, San Francisco, CA 94105, United States.\\xa0We only collect your personal information if we have a legal basis for doing so. The legal basis that we rely on depends on the personal information concerned and the specific context in which we collect it. Generally, we collect and process your personal information where:We need it to enter into or perform a contract with you, such as to provide you with the Platform Services, respond to your request, or provide you with customer support;We need to process your personal information to comply with a legal obligation (such as to comply with applicable legal, tax and accounting requirements) or to protect the vital interests', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='the vital interests of you or other individuals;You give us your consent, such as to receive certain marketing communications; orWhere we have a legitimate interest, such as to respond to your requests and inquiries, to ensure the security of the Sites and Platform Services, to detect and prevent fraud, to maintain, customize and improve the Sites and Platform Services, to promote Databricks and our Platform Services, and to defend our interests and rights.If you have consented to our use of your personal information for a specific purpose, you have the right to change your mind at any time but this will not affect our processing of your information that has already taken place.\\xa0You also have the following rights with respect to your personal information:The right to access, correct, update, or request deletion of your personal information;The right to object to the processing of your personal information or ask that we restrict the processing of your personal information;The right to', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='right to request portability of your personal information;The right to withdraw your personal information at any time, if we collected and processed your personal information with your consent; andThe right to lodge a complaint with your national data protection authority or equivalent regulatory body.If you wish to exercise any of your rights under data protection laws, please contact us as described under “Your Choices and Rights”.Other Important InformationNotice to Authorized UsersOur Platform Services are intended to be used by organizations. Where the Platform Services are made available to you through an organization (e.g., your employer), that organization is the administrator of the Platform Services and responsible for the accounts and/or services over which it has control. For example, administrators can access and change information in your account or restrict and terminate your access to the Platform Services. We are not responsible for the privacy or security practices', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content=\"security practices of an administrator's organization, which may be different from this Privacy Notice. Please contact your organization or refer to your organization's policies for more information.Data RetentionDatabricks retains the personal information described in this Privacy Notice for as long as you use our Databricks Services, as may be required by law (for example, to comply with applicable legal tax or accounting requirements), as necessary for other legitimate business or commercial purposes described in this Privacy Notice (for example, to resolve disputes or enforce our agreements), or as otherwise communicated to you.SecurityWe are committed to protecting your information. We use a variety of technical, physical, and organizational security measures designed to protect against unauthorized access, alteration, disclosure, or destruction of information. However, no security measures are perfect or impenetrable. As such, we cannot guarantee the security of your\", metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content=\"security of your information.Third Party ServicesOur Databricks Services may contain links to third party websites, applications, services, or social networks (including co-branded websites or products that are maintained by one of our business partners). We may also make available certain features that allow you to sign into our Sites using third party login credentials (such as LinkedIn, Facebook, Twitter and Google+) or access third party services from our Platform Services (such as Github). Any information that you choose to submit to third party services is not covered by this Privacy Notice. We encourage you to read the terms of use and privacy notices of use of such third party services before sharing your information with them to understand how your information may be collected and used.Children's DataThe Sites and Platform Services are not directed to children under 18 years of age and Databricks does not knowingly collect personal information from children under 18. If we\", metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='under 18. If we learn that we have collected any personal information from children under 18, we will promptly take steps to delete such information. If you are aware that a child has submitted us such information, please contact us using the details provided below.Changes to this NoticeDatabricks may change this Privacy Notice from time to time. We will post any changes on this page and, if we make material changes, provide a more prominent notice (for example, by adding a statement to the website landing page, providing notice through the Platform Services login screen, or by emailing you). You can see the date on which the latest version of this Privacy Notice was posted below. If you disagree with any changes to this Privacy Notice, you should stop using the Databricks Services and deactivate your Databricks account.\\xa0How to Contact UsPlease contact us at\\xa0[email\\xa0protected]\\xa0if you have any questions about our privacy practices or this Privacy Notice. You can also write to us at', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='also write to us at Databricks Inc., 160 Spear Street, Suite 1300, San Francisco, CA 94105 Attn: Privacy.If you interact with Databricks through or on behalf of your organization, then your personal information may also be subject to your organization’s privacy practices and you should direct any questions to that organization.Last updated: August 15th, 2023Why DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsWhy DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsResourcesDocumentationCustomer', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustDatabricks Inc.', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='160 Spear Street, 13th Floor\\nSan Francisco, CA 94105\\n1-866-330-0121See Careers\\nat Databricks© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the\\xa0Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights', metadata={'source': 'https://databricks.com/privacy-policy', 'title': 'Privacy Notice | Databricks', 'description': 'This Privacy Notice explains how Databricks, Inc.', 'language': 'en-US'}),\n",
              " Document(page_content='Terms of Use | DatabricksSkip to main contentWhy Databricks DiscoverFor ExecutivesFor Startups Lakehouse Architecture CustomersFeatured StoriesSee All CustomersPartnersCloud ProvidersDatabricks on AWS, Azure, and GCPConsulting & System IntegratorsExperts to build, deploy and migrate to DatabricksTechnology PartnersConnect your existing tools to your LakehouseC&SI Partner ProgramBuild, deploy or migrate to the LakehouseData PartnersAccess the ecosystem of data consumersPartner SolutionsFind custom industry and migration solutionsBuilt on DatabricksBuild, market and grow your businessProduct Databricks PlatformPlatform OverviewA unified platform for data, analytics and AIData ManagementData reliability, security and performanceSharingAn open, secure, zero-copy sharing for all dataData WarehousingETL and orchestration for batch and streaming dataGovernanceUnified governance for all data, analytics and AI assetsReal-Time AnalyticsReal-time analytics, AI and applications made', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='applications made simpleArtificial IntelligenceA data-centric approach to AIData EngineeringETL and orchestration for batch and streaming dataMosaicMLTrain and deploy secure generative AI modelsData ScienceCollaborative data science at scaleIntegrations and DataMarketplaceOpen marketplace for data, analytics and AIIDE IntegrationsBuild on the Lakehouse in your favorite IDEPartner ConnectDiscover and integrate with the Databricks ecosystemPricingDatabricks PricingExplore product pricing, DBUs and moreCost CalculatorEstimate your compute costs on any cloudOpen SourceOpen Source TechnologiesLearn more about the innovations behind the platformSolutions Databricks for IndustriesFinancial ServicesRetailHealthcare & Life SciencesMedia and EntertainmentManufacturingCommunicationsPublic SectorSee All IndustriesCross Industry SolutionsConsumer Data Platform Cyber SecurityMigration & DeploymentData MigrationProfessional ServicesSolution AcceleratorsExplore AcceleratorsMove faster toward outcomes', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='toward outcomes that matterResources Training and CertificationLearning OverviewHub for training, certification, events and moreTraining OverviewDiscover curriculum tailored to your needsLearning PathsGuided learning by role and career pathCertificationGain recognition and differentiationUniversity AllianceWant to teach Databricks? See how.EventsData + AI SummitData + AI World TourEvent CalendarBlog and PodcastsDatabricks BlogExplore news, product announcements, and moreDatabricks Mosaic AI Research BlogDiscover the latest in our Gen AI researchData Brew PodcastLet’s talk data!Champions of Data + AI PodcastInsights from data leaders powering innovationGet HelpCustomer SupportDocumentationCommunityDive DeepResource CenterDemo CenterAbout CompanyWho We AreOur TeamDatabricks VenturesContact UsCareersWorking at DatabricksOpen JobsPressAwards and RecognitionNewsroomSecurity and TrustSecurity and TrustReady to get started?Get a DemoLoginContact UsTry DatabricksSkip to main', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='to main contentOverviewTermsMaster Cloud Services AgreementAcceptable Use PolicyExternal User TermsUS Public Sector ServicesCommunity Edition Terms of ServicePartner Terms and ConditionsWebsite Terms of UsePrivacyData Processing AddendumAmendment to Data Processing AddendumPrivacy NoticeInternational Data Transfers FAQDatabricks SubprocessorsCookie NoticeApplicant Privacy NoticeSecurityDatabricks SecuritySecurity AddendumTrust CenterCompliance and EthicsCode of ConductThird Party Code of ConductModern Slavery StatementPay Equity Report (France)Subscribe to UpdatesTerms of UseWebsite Terms of UseThese terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”). These Terms expressly do not govern your access to or', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='your access to or use of the Databricks Platform Services (known as Databricks and Databricks Community Edition, each located at *.cloud.databricks.com, or the related website at help.databricks.com and platform support services, together the “Platform Services”), which are subject to the\\xa0Databricks Terms of Service\\xa0(or with respect to the Community Edition, the\\xa0Community Edition Terms of Service) or other written agreement in place between Databricks, Inc. (“Databricks”) and our subscribers (“Subscribers”) (any such agreement, a “Services Agreement”).PLEASE READ CAREFULLY THESE TERMS AND THE\\xa0DATABRICKS PRIVACY POLICY\\xa0(“PRIVACY POLICY”) WHICH IS INCORPORATED BY REFERENCE INTO THESE TERMS. BY ACCESSING OR USING ANY OF THE SITES, YOU REPRESENT THAT YOU ARE AT LEAST 18 YEARS OLD, YOU ACKNOWLEDGE AND AGREE THAT YOU HAVE READ AND UNDERSTOOD THESE TERMS, AND YOU AGREE TO BE LEGALLY BOUND BY ALL OF THESE TERMS. IF YOU DO NOT AGREE TO ALL OF THESE TERMS, DO NOT ACCESS OR USE THE SITES. WE', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='USE THE SITES. WE SUGGEST YOU PRINT A COPY OF THESE TERMS FOR YOUR RECORDS.Throughout the Terms, “we,” “us,” “our” and “ours” refer to Databricks, and “you,” “your” or “yours” refer to you personally (i.e., the individual who reads and agrees to be bound by these Terms) and, if you access the Sites on behalf of a legal entity, to that entity. If you are using the Sites on behalf of any entity you represent and warrant that you are authorized to accept these Terms on such entity’s behalf and, by accepting these Terms, you are hereby binding such entity to the Terms.Subject to your compliance with these Terms, solely for so long as you are permitted by Databricks to access and use the Sites, and provided that you keep intact all copyright and other proprietary notices, you may view Content and you may download and print the materials that Databricks specifically makes available for downloading from the Sites (such as white papers or user documentation), in each case solely for', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='case solely for informational purposes and solely for personal or internal business use.ACCEPTANCE OF TERMS', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks provides the Sites to you conditioned upon your accepting all of the Terms, without modification. Your use of the Sites constitutes your agreement with such Terms. We reserve the right to change, modify, add to, or remove portions of these Terms in our sole discretion at any time and we will, at our sole discretion, either post the modification on\\xa0https://www.databricks.com/terms-of-use\\xa0or provide you with email notice of the modification. You should check these Terms periodically for changes and you can determine when these Terms were last revised by referring to the “Last Updated” reference at the top of these Terms. Any modification shall be effective immediately upon the uploading of modified Terms. You indicate your agreement to comply with, and be bound by, any such modification by continuing to use or access the Sites after modified Terms are posted. If the modified Terms are not acceptable to you, your sole recourse is to discontinue your use of the Sites.', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='If you have registered for and opened an account through the Sites (an “Account”), you are entirely responsible for maintaining the confidentiality of your Account information, including your password, and for any and all activity that occurs under your Account. You agree to notify Databricks immediately of any unauthorized use of your Account or password, or any other breach of security. However, you will remain responsible for losses incurred by Databricks or by any other party due to your knowingly or inadvertently permitting unauthorized use of your Account or your Account information. You may not use anyone else’s ID, password or account at any time unless we expressly pre-approve such use, or unless expressly permitted under a Services Agreement. Databricks cannot and will not be liable for any loss or damage arising from your failure to comply with these obligations. Registration for any account is void where the user lacks the requisite eligibility for registration or if', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='registration or if such registration is otherwise prohibited.Software (“Software”) or the Platform Services may be made available to you through the Sites. Your rights to access and use the Platform Services, including any Software will be subject to your agreement to the applicable Services Agreement governing your use of the Platform Services and to any terms and conditions of any applicable third party software license agreement (“Software License”) identified in the Software or on the web page providing access to the Software. You may not use any Software unless you agree to be bound by all terms and conditions of any applicable Software License. If there is a conflict between any Services Agreement and any Software License, the conflicting term of the Software License shall control but only to the extent necessary to eliminate the conflict.LICENSE GRANT AND PROPRIETARY RIGHTS', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='Provided that you fully comply at all times with these Terms and any other policies or restrictions posted on or transmitted through the Sites, Databricks grants you a limited, non-exclusive, non-transferable, revocable license to access and use the Sites. Except as otherwise specifically noted in these Terms or on the Sites, the Software, Submissions (as later defined), and all other information, content, user interfaces, graphics, registered or unregistered trademarks, logos, images, artwork, videos, and documents, and the design, structure, selection, coordination, expression, “look and feel” and arrangement of such materials, made available through the Sites (collectively, the “Content”), regardless of its source or creation, is owned, controlled or licensed by or to Databricks, and is protected by trade dress, copyright, patent and trademark laws, and various other intellectual property rights and unfair competition laws, and Databricks reserves and retains all rights in and to', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='rights in and to such Content. Any reproduction, redistribution or other use or exploitation of Software in violation of any applicable Software License or in violation of any license granted under these Terms or, if applicable, under a Services Agreement, is expressly prohibited by law, and may result in civil and criminal penalties.', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='“Apache” and “Spark” are trademarks of the Apache Software Foundation. Any other third party trademarks, service marks, logos, trade names or other proprietary designations, that are or may become present within the Sites, including within any Content, are the registered or unregistered trademarks of the respective parties.Except solely as necessary for you to access the Sites for the intended purpose pursuant to these Terms, you may not copy, collect, modify, create derivative works or uses of, translate, distribute, transmit, publish, re-publish, perform, display, post, download, upload, sublicense, transfer, dispose of, resell or sell the Content or any other part of the Services. Except as expressly set forth in these Terms, these Terms do not grant to you any license to any intellectual property rights or other proprietary rights, including any implied licenses or licenses granted by estoppel or otherwise.INFORMATION SUBMITTED THROUGH OR TO OUR SITES', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='At our sole discretion, you may be permitted to provide Submissions (as defined in the next sentence) to the Sites (e.g., through our forums). “Submissions” are defined to include: any messages, emails, text, graphics, code, questions, suggestions, comments, feedback, ideas, plans, notes, drawings, sample data, sound, images, video, original or creative materials, and other items or materials that you may provide to discussion forums, blogs, or other interactive features or areas of the Services where you or other users can create, post, transmit or store Content. Unless otherwise specifically agreed to by you and Databricks, by uploading, e-mailing, posting, publishing or otherwise transmitting any Submission, you hereby acknowledge that such Submission is non-confidential and you automatically grant (or warrant that the owner of such rights has expressly granted) to Databricks a perpetual, irrevocable, worldwide, non-exclusive, sub-licensable, fully paid-up and royalty-free license', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='license to use, make, have made, offer for sale, sell, copy, distribute, perform, display (whether publicly or otherwise), modify, adapt, publish, transmit and otherwise exploit such Submission, by means of any form, medium, or technology now known or later developed, and to grant to others rights to do any of the foregoing. In addition, you warrant that all so-called moral rights in such Submission have been waived.', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='For each Submission, you represent and warrant that you have all rights necessary for you to grant the license granted in the prior paragraph, and that such Submission, and your provision thereof to and through the Sites, does not violate any privacy, publicity, contractual, intellectual property, or other right or rights of any person or entity or otherwise violate any applicable laws, rules or regulations. You acknowledge that Databricks may have ideas or materials already under consideration or development that are or may be similar to your Submissions and that you are not entitled to any form of compensation or reimbursement from Databricks in connection with your Submissions. You agree to be fully responsible for, and to pay any and all royalties, fees, damages, and any other monies owing any person or entity by reason of, any Submission you provide to the Sites. We reserve the right to terminate access to all or any part of the Sites for anyone we suspect to be an infringer of', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='be an infringer of our or any third party’s intellectual property rights of any kind whatsoever.You agree that you will not, and will not allow or authorize any third party to, post Submissions containing:Anything that is or may be (a) threatening, harassing, degrading, abusive or hateful; (b) an incitement to violence, terrorism or other wrongdoing; (c) defamatory or libelous; (d) invasive of privacy rights; (e) fraudulent, deceptive, impersonating of any person or entity, or misrepresentative of your affiliation with any person or entity; (f) obscene, pornographic, indecent, grotesque or otherwise objectionable; or (g) protected by copyright, trademark, confidentiality obligations, or other proprietary or privacy right without the express prior written consent of the owner of such right.Any material, the posting or usage of which would give rise to criminal or civil liability, or cause violation of any rules or regulations, or that encourages conduct that constitutes a criminal', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='a criminal offense.Any virus, worm, Trojan horse or other computer code, file, data or program that is harmful, disruptive, corrupted, or invasive, or may be or is intended to damage or hijack the operation of any hardware or software.Any information identifiable to a particular individual, including but not limited to addresses, phone numbers, email addresses, birthdates, Social Security numbers and other government-issued identification numbers, payment card and other financial account numbers or login credentials, and health information.Any unsolicited or unauthorized advertising, promotional materials, junk mail, spam, chain letter, pyramid scheme, political campaign message, offering of an investment opportunity, or any other form of solicitation.Any material with respect to which you do not have all rights, power and authority necessary for its collection, use and processing, or where your use and provision to the Sites would breach any agreement between you and any third', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='you and any third party.Databricks generally does not pre-screen or monitor Submissions (but reserves the right to do so) and does not control Submissions. Therefore, Databricks does not guarantee the accuracy, quality or appropriateness of Submissions and disclaims any responsibility for Submissions, including any liability for errors or omissions, or for any loss or damage of any kind incurred as a result of their use. However, Databricks reserves the right at its sole discretion to refuse, delete, screen or edit Submissions, provided that even if we do remove or alter any Submission, we shall have no obligation to stop our other uses of such Submission or any other Submission as permitted above. We have no obligation to store any of your Submissions. We have no responsibility or liability for the deletion or failure to store, transmit or receive your Submissions, nor do we have any responsibility for the security, privacy, storage or transmission of other communications originating', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='originating with or involving your use of the Sites, except as may be expressly stated in these Terms or in the Privacy Policy. You are solely responsible for creating backup copies of and replacing any Submissions at your sole cost and expense. Our Privacy Policy governs your Submissions.By accepting these Terms, you agree to our collection, use, and disclosure of your information as described in the Privacy Policy. No one under age 18 may register for an Account or provide any personal information to Databricks or to the Sites. If we learn that we have collected personal information from or about anyone under age 18, we will delete that information as quickly as possible. If you believe that we might have any information from or about a child under 18, please contact us at\\xa0[email\\xa0protected]\\xa0with the subject “Child Data“.Databricks reserves the right to disclose any Submissions, and the circumstances surrounding their transmission, to any third party to operate the Sites, to protect', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='Sites, to protect Databricks or its suppliers or representatives, to protect users of the Sites, to comply with legal or regulatory obligations, to enforce these Terms, or for any other reason. Databricks is not responsible or liable for the conduct of, or your interactions with, any other users of the Sites (whether online or offline), or for any associated loss, damage, injury or harm. By using the Site, you may be exposed to Submissions that are offensive, indecent or objectionable and you agree that Databricks bears no liability for such exposure.REQUIRED CONDUCT WHILE USING OUR SITES', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='While using the Sites you will comply with all applicable laws, rules and regulations. In addition, Databricks expects users of the Sites to respect the rights and dignity of others. Your use of the Sites is conditioned on your compliance with the rules of conduct set forth in this Section; any failure to comply may also result in termination of your access to the Sites pursuant to Section 9 (Suspension or Termination of Access to Our Sites). In using the Sites, you agree that you will not, and will not allow or authorize any third party to:', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='Use the Sites or any Content for any purpose that is illegal, fraudulent, deceptive or unauthorized by these Terms, or would give rise to civil liability, or to solicit the performance of any illegal activity or other activity which infringes the rights of Databricks or others, or to encourage or promote any such activity.Engage in or promote any conduct that is offensive, harassing, predatory, stalking, violent, threatening, discriminatory, racist, hateful, or otherwise harmful, against any individual or group.Harvest or collect information about any third parties, including their email addresses or other personally identifiable information.Send, by email or other means, any unsolicited or unauthorized advertising, promotional materials, junk mail, spam, chain letter, pyramid scheme, political campaign message, offering of an investment opportunity, or any other form of solicitation, or conceal or forge headers of emails or other messages, or otherwise misrepresent the identity of', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='the identity of senders, for the purpose of sending spam or other unsolicited messages.Impersonate or post on behalf of, or express or imply the endorsement of, any individual or entity, including Databricks or any of its representatives, or otherwise misrepresent your affiliation with a person or entity.Use the Sites in any manner, whether deliberate or otherwise, including without limitation a denial of service attack, that could in any way (a) interfere with, damage, disable, overburden or impair the functioning of the Sites, or Databricks’ systems or networks, or any systems or networks connected to the Sites, or (b) violate any requirements, procedures, policies or regulations of such systems or networks.Operate non-permissioned network services, including open proxies, mail relays or recursive domain name servers, or use any means to bypass user limitations relating to the Sites.Use any robot, spider, crawler, scraper, deep-link, page-scrape, site search/retrieval application or', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='application or other manual or automated device, program, algorithm or methodology or interface not provided by us to access, acquire, copy, retrieve, index, scrape, data mine, in any way reproduce or circumvent the navigational structure or presentation of the Sites or monitor any portion of the Sites or to extract data, or to sell, resell, frame, mirror or otherwise exploit for any commercial purpose, any portion of, use of, or access to the Sites (including any Content, Software and other materials available through the Sites), or attempt to circumvent any content filtering techniques we may employ.Remove any copyright, trademark or other proprietary rights notice from the Sites or from Content or other materials contained on or originating from the Sites.Create a database of any type by systematically downloading and storing any Content unless expressly permitted by Databricks to do so.Attempt to gain unauthorized access to any portion or feature of the Sites, or any other systems', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='any other systems or networks connected to the Sites or to any Databricks server, or to any of the services offered on or through the Sites, by hacking, password mining or any other illegitimate means.Use or attempt to use any account you are not authorized to use.Probe, scan, monitor or test the vulnerability of the Sites or any network connected to the Sites, or breach the security or authentication measures on the Sites or any network connected to the Sites.Modify, adapt, create derivative works of, translate, reverse engineer, decompile or disassemble any portion of the Sites (including any Content or other materials available through the Sites), or do anything that might discover source code or bypass or circumvent measures employed to prevent or limit access to any area, Content or code within the Sites except as, and solely to the extent, expressly authorized under applicable law overriding any of these restrictions.Develop any third-party applications that interact with the', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='interact with the Sites or Content without our prior written consent.Use or apply the Sites in any manner directly or indirectly competitive with any business of Databricks.\\xa0LINKS', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='We may from time-to-time at our discretion host or provide links to services, products, web pages, websites or other content of third parties (“Third-Party Content”). The inclusion of any link to, or the hosting of, any Third Party Content is provided solely as a convenience to our users, including you, and does not imply affiliation, endorsement, approval, control or adoption by us of the Third-Party Content. We make no claims or representations regarding, and accept no responsibility or liability for, Third-Party Content including without limitation its quality, accuracy, nature, ownership or reliability. Your use of Third-Party Content is at your own risk. When you leave the Sites to access Third Party Content via a link, you should be aware that our policies, including the Privacy Policy, no longer govern. You should review the applicable terms and policies, including privacy and data gathering policies, of any website to which you navigate from the Sites.DISCLAIMER OF WARRANTIES', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='YOU EXPRESSLY AGREE THAT YOUR USE OF THE SITES, INCLUDING ANY CONTENT, IS AT YOUR SOLE RISK. ALL OF THE SITES AND CONTENT ARE PROVIDED TO YOU ON AN “AS IS” AND “AS AVAILABLE” BASIS, AND DATABRICKS MAKES NO RELATED REPRESENTATIONS, AND DISCLAIMS ALL POSSIBLE WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. WE DO NOT WARRANT THAT THE SITES OR CONTENT ARE ACCURATE, CONTINUOUSLY AVAILABLE, COMPLETE, RELIABLE, SECURE, CURRENT, ERROR-FREE, OR FREE OF VIRUSES OR OTHER HARMFUL COMPONENTS. DATABRICKS CANNOT AND DOES NOT GUARANTEE THAT ANY DEFECTS, ERRORS OR OMISSIONS WILL BE CORRECTED, REGARDLESS OF WHETHER DATABRICKS IS AWARE OF SUCH DEFECTS, ERRORS OR OMISSIONS.', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='TO THE EXTENT APPLICABLE STATE LAW DOES NOT ALLOW THE EXCLUSIONS AND DISCLAIMERS OF WARRANTIES AS SET FORTH IN THIS SECTION 6, SOME OR ALL OF THE ABOVE EXCLUSIONS AND DISCLAIMERS MAY NOT APPLY TO YOU, IN WHICH CASE SUCH EXCLUSIONS AND DISCLAIMERS WILL APPLY TO THE FULLEST EXTENT PERMITTED BY APPLICABLE LAW. YOU ACKNOWLEDGE THAT THE DISCLAIMERS, LIMITATIONS, AND WAIVERS OF LIABILITY SET FORTH IN THIS SECTION 6 SHALL SURVIVE ANY EXPIRATION OR TERMINATION OF THESE TERMS OR YOUR USE OF THE SITES.LIMITATION OF LIABILITY', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='YOU ACKNOWLEDGE AND AGREE THAT, TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE ENTIRE RISK ARISING OUT OF YOUR ACCESS TO AND USE OF THE SITES AND CONTENT REMAINS WITH YOU. IN NO EVENT WILL DATABRICKS OR ANY OF ITS DIRECTORS, EMPLOYEES, AGENTS OR SUPPLIERS BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL, EXEMPLARY, CONSEQUENTIAL OR PUNITIVE DAMAGES OF ANY KIND (INCLUDING, BUT NOT LIMITED TO, LOSS OF USE, LOSS OF BUSINESS, LOSS OF PROFITS, LOSS OF DATA, LOSS OF GOODWILL, SERVICE INTERRUPTION, COMPUTER DAMAGE, SYSTEM FAILURE OR THE COST OF SUBSTITUTE PRODUCTS OR SERVICES) ARISING OUT OF OR IN CONNECTION WITH THE SITES, AND ANY CONTENT, SERVICES OR PRODUCTS INCLUDED ON OR OTHERWISE MADE AVAILABLE THROUGH THE SITES, REGARDLESS OF THE FORM OF ACTION (WHETHER IN CONTRACT, TORT, STRICT LIABILITY, EQUITY OR OTHERWISE) AND EVEN IF WE ARE AWARE OF OR HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='IN NO EVENT WILL OUR TOTAL CUMULATIVE LIABILITY TO YOU ARISING OUT OF OR IN CONNECTION WITH THESE TERMS, OR FROM THE USE OF OR INABILITY TO USE THE SITES, INCLUDING ANY CONTENT, OR FROM THE USE OF OR EXPOSURE TO ANY SUBMISSIONS, EXCEED ONE HUNDRED DOLLARS ($100.00). MULTIPLE CLAIMS WILL NOT EXPAND THIS LIMITATION.THE FOREGOING LIMITATIONS AND EXCLUSIONS SHALL NOT APPLY WITH RESPECT TO ANY LIABILITY ARISING UNDER FRAUD, FRAUDULENT MISREPRESENTATION, GROSS NEGLIGENCE, OR ANY OTHER LIABILITY THAT CANNOT BE LIMITED OR EXCLUDED BY LAW. ADDITIONALLY, TO THE EXTENT APPLICABLE STATE OR OTHER LAW DOES NOT ALLOW THE EXCLUSIONS AND LIMITATIONS OF DAMAGES AS SET FORTH IN THIS SECTION 7, SOME OR ALL OF THE ABOVE EXCLUSIONS AND LIMITATIONS MAY NOT APPLY TO YOU, IN WHICH CASE DATABRICKS’ LIABILITY TO YOU WILL BE LIMITED BY THIS SECTION TO THE FULLEST EXTENT PERMITTED BY APPLICABLE LAW.THIS SECTION WILL BE GIVEN FULL EFFECT EVEN IF ANY REMEDY SPECIFIED IN THESE TERMS IS DEEMED TO HAVE FAILED OF ITS', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='HAVE FAILED OF ITS ESSENTIAL PURPOSE. THESE LIMITATIONS OF LIABILITY FORM AN ESSENTIAL BASIS OF THE BARGAIN BETWEEN THE PARTIES. YOU ACKNOWLEDGE THAT THE LIMITATIONS OF LIABILITY SET FORTH IN THIS SECTION 7 SHALL SURVIVE ANY TERMINATION OR EXPIRATION OF THESE TERMS OR YOUR USE OF THE SITES.INDEMNIFICATION', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='To the fullest extent permitted by law, you agree to indemnify, defend and hold harmless Databricks, its officers, directors, shareholders, successors in interest, employees, agents, subsidiaries and affiliates, from and against any and all actual or threatened third party claims (groundless or otherwise), demands, losses, damages, costs and liability, proceedings (at law or in equity) and expenses (including reasonable attorneys’ and expert fees and costs of investigation) arising out of or in connection with (a) your use of the Sites, including without limitation any of your Submissions, (b) your breach of these Terms, including your breach of any covenant, representation, warranty, term, or condition set forth herein, including, without limitation, the obligations set forth in Section 3 (Information Submitted Through Our Sites) and Section 4 (Required Conduct While Using Our Sites), (c) your violation of any law or regulation or of any third party rights, including infringement,', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='infringement, libel, misappropriation, or other violation of any third party’s intellectual property or other legal rights or (d) the disclosure, solicitation or use of any personal information by you, whether with or without your knowledge or consent. Databricks reserves the right, however, to assume the exclusive defense and control of any matter otherwise subject to indemnification by you and, in such case, you agree to cooperate with Databricks’ defense of such claim, and in no event may you agree to any settlement affecting Databricks without Databricks’ prior written consent.SUSPENSION OR TERMINATION OF ACCESS TO OUR SITES', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='Notwithstanding any provision to the contrary in these Terms, you agree that Databricks may, in its sole discretion and with or without prior notice, for any or no reason, suspend or terminate your access to any or all of the Sites and/or block your future access to any or all of the Sites, including without limitation for any of the following reasons: (a) if we determine that you have violated any provision, or the spirit, of these Terms, (b) in response to a request by a law enforcement or other government agency, (c) due to discontinuance or material modification of any of the Sites, or (d) due to unexpected technical issues or problems. Databricks shall not be liable to you or any third party for any termination of your access to any part of the Sites. The rights and obligations of these Terms which by their nature should survive, shall so survive any termination of your use of the Sites.CONTACT', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='Questions or comments about the Terms or about the Sites may be directed to Databricks at the email address [email\\xa0protected]. You may also email us at that address if you would like to report what you believe to be a violation of these Terms. However, please note that we do not accept any responsibility to maintain the confidentiality of any report of a violation you may submit to us, including your identity, nor do we commit to providing a personal reply to any report you submit, nor are we obligated to take action in response to your report.CLAIMS OF COPYRIGHT INFRINGEMENT', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='Databricks respects the intellectual property rights of others and we request that the people who use the Sites do the same. The Digital Millennium Copyright Act of 1998 (the “DMCA”) provides recourse for copyright owners who believe that material appearing on the Internet infringes their rights under U.S. copyright law. If you believe in good faith that materials available on the Sites infringe your copyright, you (or your agent) may send Databricks a notice requesting that we remove the material or block access to it. If you believe in good faith that someone has wrongly filed a notice of copyright infringement against you, you may send a counter-notice to Databricks under applicable provisions of the DMCA. Please note that substantial penalties under U.S. copyright law may be levied against any filer of a false counter-notice. Notices and counter-notices must meet the then-current statutory requirements imposed by the DMCA. See 17 U.S.C. § 512(c)(3), available at', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='available at https://www.copyright.gov/title17/92chap5.html for details. Notices and counter-notices should be sent to:', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='Attn: Legal Department/DMCA Copyright Agent\\n\\tDatabricks, Inc.\\n\\t160 Spear Street, Suite 1300\\n\\tSan Francisco, CA 94105\\n[email\\xa0protected]\\n\\t(866) 330-0121You should note that if you knowingly misrepresent in your notification that the material or activity is infringing, you will be liable for any damages, including costs and attorneys’ fees, incurred by us or the alleged infringer as the result of our relying upon such misrepresentation in removing or disabling access to the material or activity claimed to be infringing. We encourage you to consult your legal advisor before filing a notice or counter-notice.In accordance with the DMCA and other applicable law, Databricks may at our discretion limit access to the Sites and/or terminate the accounts of any users who infringe any intellectual property rights of others, whether or not there is any repeat infringement.GENERAL', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='The Terms and the relationship between each user and Databricks shall be governed by the laws of the State of California without regard to its conflict of law provisions and each party shall submit to the personal and exclusive jurisdiction of the courts located in San Francisco, California. The application of the United Nations Convention on Contracts for the International Sale of Goods is expressly excluded. Except to the extent a Services Agreement applies, these Terms, along with the Privacy Policy, constitute the entire agreement between you and Databricks with respect to your use of the Sites and supersede all prior or contemporaneous communications and proposals, whether electronic, oral or written, between you and Databricks with respect to the Sites. If any provision of the Terms is found by a court of competent jurisdiction to be invalid, the parties nevertheless agree that the court should endeavor to give effect to the parties’ intentions as reflected in the provision,', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='in the provision, and the other provisions of the Terms remain in full force and effect. A party may only waive its rights under these Terms by a written document executed by both parties. Databricks’ failure to insist on or enforce strict performance of these Terms shall not be construed as a waiver by Databricks of any provision or any right it has to enforce these Terms, nor shall any course of conduct between Databricks and you or any other party be deemed to modify any provision of these Terms. The headings of the sections of these Terms are for convenience of reference only and are not intended to restrict, affect or be of any weight in the interpretation or construction of the provisions of such sections.', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='None of your rights or duties under these Terms may be transferred, assigned or delegated by you without our prior written consent, and any attempted transfer, assignment or delegation without such consent will be void and without effect. We may freely transfer, assign or delegate any of our rights or duties under these Terms. Subject to the foregoing, these Terms will be binding upon and will inure to the benefit of the parties and their respective representatives, heirs, administrators, successors and permitted assigns. No provision of these Terms is intended for the benefit of any third party, and the parties do not intend that any provision should be enforceable by a third party. Our relationship is an independent contractor relationship, and neither these Terms nor any actions by either party may be interpreted as creating an agency or partnership relationship. Nothing in these Terms shall be construed to obligate Databricks to enter into or engage with you on any commercial', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='on any commercial transaction.If you are provided access to any Software, you acknowledge that such Software may be subject to regulation by local laws and United States government agencies which prohibit export or diversion of certain products or information about products to certain countries and certain persons. You represent and warrant that you will not export or re-export such Software in violation of these regulations.You acknowledge that your breach of any of the provisions of these Terms may cause immediate and irreparable harm to Databricks for which we may not have an adequate remedy in money damages. We will therefore be entitled to obtain an injunction against such breach from any court of competent jurisdiction immediately upon request and will be entitled to recover from you the costs incurred in seeking such an injunction. The availability or exercise of our right to obtain injunctive relief will not limit our right to seek or obtain any other remedy.You agree that we', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='agree that we will not be liable for delays, failures, or inadequate performance of the Sites resulting from conditions outside of our reasonable control, including but not limited to natural disasters or other acts of God, failure of telecommunications networks or any other network or utility, threatened or actual acts of terrorism or war, riots, labor strikes, or governmental acts or orders.Last Updated May 25, 2018Why DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsWhy DatabricksDiscoverFor ExecutivesFor StartupsLakehouse ArchitectureCustomersFeaturedSee AllPartnersCloud ProvidersTechnology PartnersData PartnersBuilt on DatabricksConsulting & System IntegratorsC&SI Partner ProgramPartner SolutionsProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectProductDatabricks PlatformPlatform OverviewSharingGovernanceArtificial IntelligenceMosaicMLData ManagementData WarehousingReal-Time AnalyticsData EngineeringData SciencePricingPricing OverviewPricing CalculatorOpen SourceIntegrations and DataMarketplaceIDE IntegrationsPartner ConnectSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional ServicesSolution AcceleratorsSolutionsDatabricks For IndustriesFinancial ServicesHealthcare and Life SciencesManufacturingPublic SectorRetailMedia and EntertainmentCommunicationsView AllCross Industry SolutionsConsumer Data PlatformCyber SecurityData MigrationProfessional', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='ServicesSolution AcceleratorsResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastResourcesDocumentationCustomer SupportCommunityTraining and CertificationLearning OverviewTraining OverviewCertificationUniversity AllianceDatabricks Academy LoginEventsData + AI SummitData + AI World TourFull CalendarBlog and PodcastsDatabricks BlogDatabricks Mosaic AI Research BlogData Brew PodcastChampions of Data & AI PodcastAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustAboutCompanyWho We AreOur TeamDatabricks VenturesCareersOpen JobsWorking at DatabricksPressAwards and RecognitionNewsroomSecurity and TrustDatabricks Inc.', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'}),\n",
              " Document(page_content='160 Spear Street, 13th Floor\\nSan Francisco, CA 94105\\n1-866-330-0121See Careers\\nat Databricks© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the\\xa0Apache Software Foundation.Privacy Notice|Terms of Use|Your Privacy Choices|Your California Privacy Rights', metadata={'source': 'https://databricks.com/terms-of-use', 'title': 'Terms of Use | Databricks', 'description': 'Website Terms of Use These terms of use (“Terms”) govern your access to and use of all Databricks-branded publicly available websites, including sites located on databricks.com (other than *.cloud.databricks.com and help.databricks.com), as well as spark-summit.org, and spark-packages.org and any other pages that link to these Terms (collectively, the “Sites”).', 'language': 'en-US'})]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "# storing embeddings in the vector store\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589,
          "referenced_widgets": [
            "3940e60d84604f81b0a3ff08f7fd27eb",
            "74b3b3ecc3ed4bfda6863e10af796e84",
            "5387bb6e809c4dd197aa965bcd462d03",
            "4ca2fcb48fd344e993d45864be2fe020",
            "41f1a9a06a1940df83219fe194b356b0",
            "70eb7afe93bc4fdda4ff366699a5ddd6",
            "ede9cc898a794c2f92638c5ade954a9e",
            "ea1d1288ed4e4490bf8034c788d763df",
            "9c1e114173cb4d7091d53f57ca11212c",
            "39edd6c1c8e749a7825e95b281af744b",
            "4ef9cd961cff4723baae26a2917883ed",
            "fc97e281032c48de8bb820cee493d58d",
            "de5bd1d68188407299271352d49286ea",
            "ab8205b0cb444eb584bfca29c34a1295",
            "3710b33d408e4d8e8649617eadffe78e",
            "e6a48933067b43f99f22d92645028383",
            "44750abc65f14e8e87319c4f32f1f937",
            "29fadb181bc54b798143ed4203025d4f",
            "e42dd3936d3c478fbb08c17c66f447ac",
            "3023f1860ffd49ebbc4577367ffa28c4",
            "5fd408c74740452297e0a47bff2e5c8c",
            "c7d68f997cc943cb80f8eb299f71488e",
            "b988304270d248348a6538499ef672fb",
            "1c56e3c5129d4afabff11fb65a6da9c2",
            "88ac9a71e8bc4fcd9b63ec6c163fb943",
            "10be0c0cc9094f7490a97a5258e698e7",
            "0ecc522377634d71bad238a7194dfbba",
            "471ed1877060453281c78720730581b2",
            "835b2891f21e4db0b4593c367182c798",
            "c8f6a6a104f044d6831af5aa662ac8b8",
            "929dd3f4fa534170b42f7a193168a81e",
            "0a1ed4af3fad4574885fd86bd2e2afba",
            "707565c3baf644c7a8204a1e1ecf3022",
            "44968b681d804ae69ada7ed0bf7dc08e",
            "1ac19c54b521497aac2ea67798f5ae0f",
            "90ee7182fbb347dda8cf73a3078944a4",
            "7cb83642c37a4c36a31ee149d14ded08",
            "2f685916ddf04a56b9d8a65a7fc50267",
            "8c656a269ecc40bda945fd96b9c117c9",
            "f2c7df60dfa54034939e272184bf909d",
            "02f2956fb0e146218439db5f8633278d",
            "06f0e0b411964ff1a3d2d32a0b5841b5",
            "3b93c08ffeb24cc693d6c9f35e64ba79",
            "f29f31d77a0347cc9d06f390e8a3b5ec",
            "b0f9adfac683408097a9a029d8c51463",
            "9724bfc63f29423eb3df74dcbbc4ae95",
            "c4c82b6527264cc29b2b705d3419eabe",
            "cc15b55b5c5c47e69a5ab4a439f2633a",
            "6414f1acf09a4cbd80cfb84c4a457f22",
            "23588760bc8b4aaa84d66c86c1fb2884",
            "654ab5b7a378460d9d65256b0b102138",
            "98046f6755ab42ba80aa38e042ae540f",
            "1bf0aca459e743eeb5ec61c46802fc5b",
            "eb15bea4696e4ceba186d901cb5582e7",
            "4ec96a8f14dc4e3bb97772cf5a9cdc28",
            "e19b707f4ddd44e4b2199ada2e4b059f",
            "534d2b5878bf4d9ab3d697d537565261",
            "f94b318cea3e4103b538131f40b4ca57",
            "970f23e1cae5423e8aec0ea40750fd7b",
            "bb0ae8cc6b314178aa715f564d44c435",
            "f56561007e3b42e38f847046c187f0ed",
            "ac1901be8e674f84a037bc1afa6336af",
            "68573a32b70147dd998965664fb3d158",
            "b9da9846627f4a39ab4f9bd94e8c4ade",
            "ee3e2b867dc449b196a41bf16209afbc",
            "526facf324bf4c0989c76cb92b974d85",
            "d08eaffb93ff497facb64a48062cdf84",
            "3e21f63df376479793a8d13de228a090",
            "8a16649f7a5b4fbe9203910bcc6f22ff",
            "7341203ecd0f45a499d6b5dbe46ecdcf",
            "2e189a3f56a44ee4b262f37249797037",
            "ac3d2a9a41364907afc5afee63afb785",
            "db094e7c86984c918c5a9782be1b0683",
            "b4688fe76e164cecb1f1373e2da5f384",
            "743c34fb18df4958b1fced6611e6e96a",
            "5d17f3b6544040809287593ff225e125",
            "4ea57d97a14543baa05812769ccadd92",
            "7e867b48c5e2453881c77f32cf99f159",
            "aab574933c9c4970b577e3db1a3a0dcf",
            "e27796658c14456b9233caa853064098",
            "af59357edf75402abb87e6c89875ddff",
            "ffce8211101643c4b22f2a71d6c4b84d",
            "367e5cc0df704ec8b9ef4789dff0902f",
            "74c5262c07c440bd92b34f59164821f9",
            "62e772d30b7546b4b786d4e26750083b",
            "4111e7a5907b412ab47a0848ebb99019",
            "30e5170f98224b2a8e2e402828f516d3",
            "6b8a3fc23d61451a8cc7ca8449a56c46",
            "277a0d89d8c444b182a8d66c8503d91f",
            "005c61a2029442dbad2c4d876f304f23",
            "513fd1f899d448c5a7b277486d0bee4c",
            "271e69a082ff4d93b051644211e7c22b",
            "eac7a3b210654134a9500836c1ce16fa",
            "980d76c95a914c0c89699e8c73c51174",
            "566810cc036d4cb9986cafea56e238e2",
            "1bb0b690ae194809933c6a93b347fe37",
            "d9a2c49f97df4289bb52657d48c0f867",
            "dac984c9ba3a4deab3d4b85f7f072996",
            "3e7db0554e0f4cf6817a10fa9bd87401",
            "b18a4ae52485472682abea96a9d52ed6",
            "37fbf7377a004e0a90c644107c13eecf",
            "e42e04c3ca354553b7976ae3395e8861",
            "6e998624454e4d578f878e1883555e59",
            "8f71285c8b0d49e6937185e75983108f",
            "d427bb63e49c460ab4fc7c2aef1d511b",
            "c5e70b74da634c9297a73488e3d8e177",
            "be3bdab71eaa4d53a930d483e6f2ff3e",
            "5284432f4a8d49c783365f76e0fdc645",
            "3bc2663932524576a908949f7666a071",
            "f1530d38d2cb428098a9637ccb4d3270",
            "ae8a0a38919a44c4b5afb5d6c0389d08",
            "70127e26514d4b299254d8108e46e222",
            "bfea34e7f2ea4b56bc84164ffe76e59f",
            "51d0c7898f3e4faca78fd5836537e5e3",
            "79c757d47e854639a0b2098d0522ea0b",
            "67531bf406474d818273595a77d7e2cb",
            "996006c0102d450188fa01d3f618d6c3",
            "c789cca8eff1443a82b5b1d74aeff84f",
            "08518cc3729049f9acbe7ba0d47022ec",
            "77b2c87709d34a8eb936a5b517bfc5b0",
            "a99873e16f9a4f45b1c35015306c9cab",
            "56821ea9bd144e17ae8147d09a01b46e",
            "c07959e48d92419faa36153bb83e29d3",
            "23bda95754a94295875309dbc956c77b",
            "6f561a764f504fef9d05d1074878eb07",
            "b514bffe5a7e4b2e85318508533e5477",
            "e11ce430d52b4833b8f811bc373fe110",
            "76b7f652f03f49c48ec36095d2e774ec",
            "55d41486719a48bfa87512efb1b50524",
            "a9a730cd29874d548e130f59bfeecc98",
            "2de9435f9cf249faa8a535204fd1e9fd",
            "1639331afe29477282cf3bbac80119fc",
            "a857565e57a445d7908d59f78fac3de2",
            "5e5a7d05fee2448391dde6cf67073163",
            "9798639f72f3430b9037fcdb544943ef",
            "ad73898660f54b83b26f49e3cb6712e3",
            "e16252b112824e72ba52705c8cdd5141",
            "e48f4eae296a4b5d8c799a12e6cda6bc",
            "5a0cb9513cf6485baa73b7c3c1a22ff7",
            "5c1e24bd56a14db0b26d01fbffcf471a",
            "a8a2f8165b7849e4ab0be17edf1a83c9",
            "d2f619ee59b548408d64233e3d2a0b30",
            "a2382b7a6d2f4e4ca17e6c5ae44e00f2",
            "336a39cc159343a49f43d5ca1b0a5089",
            "ce512c4d6fe243a3977f35c7ec6cba14",
            "791cf9a8db9c4522af564bf231247f84",
            "bc81d1ca229845699d457e0a9bb1ade3",
            "5619f57069cd445297482315e63ec74e",
            "67a419257cc84e7f90820f50e64fa32f",
            "3e7df6fa548e4e388c610b15d7e59be5",
            "e3549a69f5774bbe889dad5f690a6914",
            "835b489759e848ce95a3c52826575ad3",
            "8edf842f8e0349eda4f32615424bf166",
            "e43a536c9b8e425bbbd2f5c7d141aa6a"
          ]
        },
        "id": "3QxbP35qgpKc",
        "outputId": "97bca75e-b155-4a62-f09c-d3437086dd39"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3940e60d84604f81b0a3ff08f7fd27eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc97e281032c48de8bb820cee493d58d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b988304270d248348a6538499ef672fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44968b681d804ae69ada7ed0bf7dc08e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0f9adfac683408097a9a029d8c51463"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e19b707f4ddd44e4b2199ada2e4b059f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d08eaffb93ff497facb64a48062cdf84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e867b48c5e2453881c77f32cf99f159"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "277a0d89d8c444b182a8d66c8503d91f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b18a4ae52485472682abea96a9d52ed6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae8a0a38919a44c4b5afb5d6c0389d08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56821ea9bd144e17ae8147d09a01b46e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a857565e57a445d7908d59f78fac3de2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "336a39cc159343a49f43d5ca1b0a5089"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing RAG with conversation buffer memory."
      ],
      "metadata": {
        "id": "jXbSWLZIhK5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
      ],
      "metadata": {
        "id": "o2k9MY74g66n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "query = \"What is Data lakehouse architecture in Databricks?\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n2B74pOg7Mz",
        "outputId": "c2a2610a-0647-4aec-a0b3-cb4d4707c3d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In Databricks, a data lakehouse architecture refers to a scalable storage and processing system that supports multiple workloads, such as machine learning and business intelligence. It typically consists of several layers, including a bronze layer for raw data, a silver layer for transformed data, and a gold layer for highly refined and curated data. Each layer can contain one or more tables, and the data warehouse is typically modeled at the silver layer and feeds specialized data marts in the gold layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [(query, result[\"answer\"])]\n",
        "\n",
        "query = \"What are Data Governance and Interoperability in it?\"\n",
        "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Eg_UQEIg-Ny",
        "outputId": "89f62e66-1027-462f-d8da-6e1624725879"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In a data lakehouse architecture, Data Governance refers to the policies and procedures implemented to manage data assets within an organization. It differs from traditional data governance practices as it focuses on managing data across various systems and platforms, including data lakes, data warehouses, and other cloud-based storage solutions. Additionally, Data Governance in a data lakehouse architecture emphasizes interoperability and usability, ensuring that data can be easily accessed and used by various stakeholders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fun(query):\n",
        "  chat_history = []\n",
        "  result = chain({\"question\":query, \"chat_history\": chat_history})\n",
        "  chat_history = [(query, result[\"answer\"])]\n",
        "  return result['answer']"
      ],
      "metadata": {
        "id": "9fBrer2cr_vo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evidence of answer from chunks of documents"
      ],
      "metadata": {
        "id": "uM21QDRYhXtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "for i,evidence in enumerate(result['source_documents'],start=1):\n",
        "  print(\"#\"*20,i)\n",
        "  print(evidence.page_content,\"\\n\")\n",
        "  print(json.dumps(evidence.metadata,indent=2),\"\\n\")\n",
        "  print(\"\\n\"*2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MBrwrc3pWtQ",
        "outputId": "5ab0ccba-5ace-4a8d-a08b-7ed32b04bca3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#################### 1\n",
            "Data Governance and Interoperability & Usability in lakehouse architectures \n",
            "The pillars “Data Governance” and “Interoperability and Usability” cover concerns specific for the lakehouse.\n",
            "Data governance encapsulates the policies and practices implemented to securely manage the data assets within an organization. One of the fundamental aspects of a lakehouse is centralized data governance: The lakehouse unifies data warehousing and AI uses cases on a single platform. This simplifies the modern data stack by eliminating the data silos that traditionally separate and complicate data engineering, analytics, BI, data science, and machine learning. To simplify data governance, the lakehouse offers a unified governance solution for data, analytics and AI. By minimizing the copies of your data and moving to a single data processing layer where all your data governance controls can run together, you improve your chances of staying in compliance and detecting a data breach. \n",
            "\n",
            "{\n",
            "  \"source\": \"http://docs.databricks.com/lakehouse-architecture/index.html\",\n",
            "  \"title\": \"Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS\",\n",
            "  \"description\": \"Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.\",\n",
            "  \"language\": \"en-US\"\n",
            "} \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#################### 2\n",
            "Administration\n",
            "\n",
            "Account and workspace administration\n",
            "Security and compliance\n",
            "Data governance\n",
            "Lakehouse architecture\n",
            "Data governance\n",
            "Interoperability & usability\n",
            "Operational excellence\n",
            "Security, compliance & privacy\n",
            "Reliability\n",
            "Performance efficiency\n",
            "Cost optimization\n",
            "\n",
            "\n",
            "\n",
            "Reference & resources\n",
            "\n",
            "Reference\n",
            "Resources\n",
            "What’s coming?\n",
            "Documentation archive\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    Updated Jan 26, 2024\n",
            "  \n",
            "\n",
            "\n",
            "Send us feedback\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Documentation \n",
            "Data lakehouse architecture: Databricks well-architected framework\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Data lakehouse architecture: Databricks well-architected framework \n",
            "This set of data lakehouse architecture articles provides principles and best practices for the implementation and operation of a lakehouse using Databricks.\n",
            "\n",
            "Databricks well-architected framework for the lakehouse \n",
            "\n",
            "\n",
            "\n",
            "The well-architected lakehouse consists of 7 pillars which describe different areas of concern for the implementation of a data lakehouse in the cloud: \n",
            "\n",
            "{\n",
            "  \"source\": \"http://docs.databricks.com/lakehouse-architecture/index.html\",\n",
            "  \"title\": \"Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS\",\n",
            "  \"description\": \"Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.\",\n",
            "  \"language\": \"en-US\"\n",
            "} \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#################### 3\n",
            "Data governance\n",
            "The oversight to ensure that data brings value and supports your business strategy.\n",
            "\n",
            "Interoperability and usability\n",
            "The ability of the lakehouse to interact with users and other systems.\n",
            "\n",
            "Operational excellence\n",
            "All operations processes that keep the lakehouse running in production.\n",
            "\n",
            "Security, privacy, compliance\n",
            "Protect the Databricks application, customer workloads and customer data from threats.\n",
            "\n",
            "Reliability\n",
            "The ability of a system to recover from failures and continue to function.\n",
            "\n",
            "Performance efficiency\n",
            "The ability of a system to adapt to changes in load.\n",
            "\n",
            "Cost optimization\n",
            "Managing costs to maximize the value delivered. \n",
            "\n",
            "{\n",
            "  \"source\": \"http://docs.databricks.com/lakehouse-architecture/index.html\",\n",
            "  \"title\": \"Data lakehouse architecture: Databricks well-architected framework | Databricks on AWS\",\n",
            "  \"description\": \"Introduction to a set of architecture articles providing principles and best practices for the implementation and operation of the Databricks lakehouse.\",\n",
            "  \"language\": \"en-US\"\n",
            "} \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "#################### 4\n",
            "Administration\n",
            "\n",
            "Account and workspace administration\n",
            "Security and compliance\n",
            "Data governance\n",
            "Lakehouse architecture\n",
            "\n",
            "Reference & resources\n",
            "\n",
            "Reference\n",
            "Resources\n",
            "What’s coming?\n",
            "Documentation archive\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    Updated Jan 26, 2024\n",
            "  \n",
            "\n",
            "\n",
            "Send us feedback\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Documentation \n",
            "What is data warehousing on Databricks?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "What is data warehousing on Databricks? \n",
            "Data warehousing refers to collecting and storing data from multiple sources so it can be quickly accessed for business insights and reporting. This article contains key concepts for building a data warehouse in your data lakehouse.\n",
            "\n",
            "Data warehousing in your lakehouse \n",
            "The lakehouse architecture and Databricks SQL bring cloud data warehousing capabilities to your data lakes. Using familiar data structures, relations, and management tools, you can model a highly-performant, cost-effective data warehouse that runs directly on your data lake. For more information, see What is a data lakehouse? \n",
            "\n",
            "{\n",
            "  \"source\": \"http://docs.databricks.com/sql/index.html\",\n",
            "  \"title\": \"What is data warehousing on Databricks? | Databricks on AWS\",\n",
            "  \"description\": \"Learn about building a data warehousing solution on the Databricks Platform using Databricks SQL.\",\n",
            "  \"language\": \"en-US\"\n",
            "} \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Fastapi server"
      ],
      "metadata": {
        "id": "l61QJfu64G5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir fastapi_server"
      ],
      "metadata": {
        "id": "UvOJjXmz4dvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi_server import RAGFastAPI"
      ],
      "metadata": {
        "id": "2isxZv7C4NTI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uvicorn fastapi_server.RAGFastAPI.py:app --proxy-headers --reload --host 0.0.0.0 --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM4-kKn14Nio",
        "outputId": "4dfd4cc4-f5a3-4e9b-ae1a-45232680cc3f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Will watch for changes in these directories: ['/content']\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Started reloader process [\u001b[36m\u001b[1m8843\u001b[0m] using \u001b[36m\u001b[1mStatReload\u001b[0m\n",
            "\u001b[31mERROR\u001b[0m:    Error loading ASGI app. Could not import module \"fastapi_server.RAGFastAPI.py\".\n",
            "\u001b[32mINFO\u001b[0m:     Stopping reloader process [\u001b[36m\u001b[1m8843\u001b[0m]\n"
          ]
        }
      ]
    }
  ]
}